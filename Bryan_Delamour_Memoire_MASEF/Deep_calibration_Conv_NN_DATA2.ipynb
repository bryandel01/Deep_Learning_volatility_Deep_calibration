{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cmath import * \n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "from time import time \n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from termcolor import colored\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "strikes = np.arange(0.5,1.6,0.1)\n",
    "maturities = np.array([0.1,0.3,0.6,0.9,1.2,1.5,1.8,2])\n",
    "S0=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected Neural Network with:\n",
    "\n",
    "    # Elu activation (alpha=?)\n",
    "    # 3 hidden Layers\n",
    "    # 30 nodes\n",
    "\n",
    "hidden_dim = 30\n",
    "input_dim = 4 # xi, nu, rho, H \n",
    "output_dim = 11*8 # 11x8 IV surface\n",
    "learning_rate=0.005\n",
    "\n",
    "class NN_CAL(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(NN_CAL, self).__init__()\n",
    "        \n",
    "        self.conv2D_1 = nn.Conv2d(1, 16, 3)\n",
    "        self.Maxpool_1 = nn.MaxPool2d(2, 2)\n",
    "        self.L1 = nn.Linear(16 * 4 * 3, 50)\n",
    "        self.L2 = nn.Linear(50, 4)\n",
    "    \n",
    "    \n",
    "\n",
    "        self.elu = nn.ELU(alpha=1, inplace=False)\n",
    "        \n",
    "        for name, param in self.conv2D_1.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "        for name, param in self.Maxpool_1.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)  \n",
    "        for name, param in self.L1.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param) \n",
    "        for name, param in self.L2.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param) \n",
    "        \n",
    "        \n",
    "    def forward(self, iv_surface): # [ Batch_size , 1, 11, 8] \n",
    "                              \n",
    "        conv = self.conv2D_1(iv_surface) #[ Batch_size , 16, 9, 6]\n",
    "\n",
    "        activation1 = self.elu(conv) \n",
    "\n",
    "        maxpool = self.Maxpool_1(activation1)   #[Batch_size, 16, 4, 3]\n",
    "                \n",
    "        maxpool = maxpool.view(-1, 16 * 4 * 3) # [Batch_size, 16 * 4 * 3]\n",
    "        \n",
    "        lin1 = self.L1(maxpool)    # [Batch_size, 50]\n",
    "\n",
    "        activation2 = self.elu(lin1)\n",
    "\n",
    "        output = self.L2(activation2) # [Batch_size,  4 ]\n",
    "\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN_CAL(\n",
      "  (conv2D_1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (Maxpool_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (L1): Linear(in_features=192, out_features=50, bias=True)\n",
      "  (L2): Linear(in_features=50, out_features=4, bias=True)\n",
      "  (elu): ELU(alpha=1)\n",
      ")\n",
      "Number of parameters 10014\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Model summary\n",
    "NN_cal = NN_CAL()\n",
    "print(NN_cal)\n",
    "print(\"Number of parameters\", count_parameters(NN_CAL()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data importation\n",
    "\n",
    "import gzip\n",
    "f = gzip.GzipFile('rBergomiTrainSet.txt.gz', \"r\")\n",
    "dat=np.load(f)\n",
    "\n",
    "model_parameters=dat[:,:4]\n",
    "implied_vols=dat[:,4:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "mean_data = np.mean(implied_vols, axis = 0)\n",
    "std_data = np.std(implied_vols, axis = 0)\n",
    "\n",
    "implied_vols_N = (implied_vols - mean_data)/std_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and test\n",
    "\n",
    "implied_vols_training =  torch.Tensor(implied_vols_N[:33792,:])\n",
    "model_parameters_training =  torch.Tensor(model_parameters[:33792,:])\n",
    "\n",
    "implied_vols_test =  torch.Tensor(implied_vols_N[34000:,:])\n",
    "model_parameters_test =  torch.Tensor(model_parameters[34000:,:])\n",
    "\n",
    "XI_test = model_parameters[34000:,0]\n",
    "NU_test = model_parameters[34000:,1]\n",
    "RHO_test = model_parameters[34000:,2]\n",
    "H_test = model_parameters[34000:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(in_sample, out_sample, batch_size):\n",
    "    rnd_idx = np.random.permutation(in_sample.shape[0])\n",
    "    in_sample_batch = []\n",
    "    out_sample_batch = []\n",
    "    n_batches = in_sample.shape[0] // batch_size\n",
    "    \n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        in_sample_batch.append(in_sample[batch_idx,:])\n",
    "        out_sample_batch.append(out_sample[batch_idx,:])\n",
    "        \n",
    "    return ( (in_sample_batch, out_sample_batch) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setting \n",
    "\n",
    "learning_rate = 0.005\n",
    "batch_size = 128\n",
    "epochs = 400\n",
    "nb_batch = model_parameters_training.shape[0]//batch_size #nb of patch / set\n",
    "optimizer = torch.optim.Adam(NN_cal.parameters(), lr=learning_rate)\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = 10\n",
    "LOSS_B = []\n",
    "LOSS_test = []\n",
    "training_rates = [learning_rate]\n",
    "NN_cal_model_File = \"NN_cal_TEST_DATA_model.pkl\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rate decay\n",
    "tau = epochs\n",
    "min_rate = 0.01*learning_rate #Cf deep learning Chap 8 p310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector(matrix):\n",
    "    x = matrix[:,0,:]\n",
    "    for i in range(1,matrix.shape[1]):\n",
    "        x = torch.cat((x,matrix[:,i,:]), dim=1)\n",
    "    return(x)\n",
    "\n",
    "def matrix(vector,rows,colums):\n",
    "    x = torch.zeros((vector.shape[0],rows,colums))\n",
    "    for i in range(vector.shape[0]):\n",
    "        for j in range(rows):\n",
    "            x[i,j,:] = vector[i,j*colums:(j+1)*colums]\n",
    "    return(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "implied_vols_test = torch.reshape(matrix(implied_vols_test, 8, 11), (implied_vols_test.shape[0],11,8) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.005\n",
      "\u001b[31mEpoch:\u001b[0m 0 loss: \u001b[31m0.035727888345718384\u001b[0m\n",
      "Best Loss 0.035727888345718384\n",
      "Best Batch Loss tensor(0.0046, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0063, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.006260181311517954)\u001b[0m\n",
      "Learning Rate 0.005\n",
      "\u001b[31mEpoch:\u001b[0m 1 loss: \u001b[31m0.005326916463673115\u001b[0m\n",
      "Best Loss 0.005326916463673115\n",
      "Best Batch Loss tensor(0.0032, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0051, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.005072787404060364)\u001b[0m\n",
      "Learning Rate 0.004987625000000001\n",
      "\u001b[31mEpoch:\u001b[0m 2 loss: \u001b[31m0.003115632338449359\u001b[0m\n",
      "Best Loss 0.003115632338449359\n",
      "Best Batch Loss tensor(0.0018, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.002155159367248416)\u001b[0m\n",
      "Learning Rate 0.0049752500000000005\n",
      "\u001b[31mEpoch:\u001b[0m 3 loss: \u001b[31m0.0020549206528812647\u001b[0m\n",
      "Best Loss 0.0020549206528812647\n",
      "Best Batch Loss tensor(0.0012, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0015035312389954925)\u001b[0m\n",
      "Learning Rate 0.004962875000000001\n",
      "\u001b[31mEpoch:\u001b[0m 4 loss: \u001b[31m0.001751800300553441\u001b[0m\n",
      "Best Loss 0.001751800300553441\n",
      "Best Batch Loss tensor(0.0011, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0013977832859382033)\u001b[0m\n",
      "Learning Rate 0.0049505\n",
      "\u001b[31mEpoch:\u001b[0m 5 loss: \u001b[31m0.0014350945129990578\u001b[0m\n",
      "Best Loss 0.0014350945129990578\n",
      "Best Batch Loss tensor(0.0009, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.001192297087982297)\u001b[0m\n",
      "Learning Rate 0.004938125\n",
      "\u001b[31mEpoch:\u001b[0m 6 loss: \u001b[31m0.0013635596260428429\u001b[0m\n",
      "Best Loss 0.0013635596260428429\n",
      "Best Batch Loss tensor(0.0008, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00121243332978338)\u001b[0m\n",
      "Learning Rate 0.00492575\n",
      "\u001b[31mEpoch:\u001b[0m 7 loss: \u001b[31m0.0011994625674560666\u001b[0m\n",
      "Best Loss 0.0011994625674560666\n",
      "Best Batch Loss tensor(0.0007, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.001106954994611442)\u001b[0m\n",
      "Learning Rate 0.004913375\n",
      "\u001b[31mEpoch:\u001b[0m 8 loss: \u001b[31m0.001289490726776421\u001b[0m\n",
      "Best Loss 0.0011994625674560666\n",
      "Best Batch Loss tensor(0.0006, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0009247622219845653)\u001b[0m\n",
      "Learning Rate 0.004901\n",
      "\u001b[31mEpoch:\u001b[0m 9 loss: \u001b[31m0.0009751327452249825\u001b[0m\n",
      "Best Loss 0.0009751327452249825\n",
      "Best Batch Loss tensor(0.0005, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0008959667757153511)\u001b[0m\n",
      "Learning Rate 0.004888625000000001\n",
      "\u001b[31mEpoch:\u001b[0m 10 loss: \u001b[31m0.0009673391468822956\u001b[0m\n",
      "Best Loss 0.0009673391468822956\n",
      "Best Batch Loss tensor(0.0005, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0009665073594078422)\u001b[0m\n",
      "Learning Rate 0.00487625\n",
      "\u001b[31mEpoch:\u001b[0m 11 loss: \u001b[31m0.0008228190708905458\u001b[0m\n",
      "Best Loss 0.0008228190708905458\n",
      "Best Batch Loss tensor(0.0004, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0006988461827859282)\u001b[0m\n",
      "Learning Rate 0.004863875\n",
      "\u001b[31mEpoch:\u001b[0m 12 loss: \u001b[31m0.0007627038867212832\u001b[0m\n",
      "Best Loss 0.0007627038867212832\n",
      "Best Batch Loss tensor(0.0004, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0007103631505742669)\u001b[0m\n",
      "Learning Rate 0.0048515\n",
      "\u001b[31mEpoch:\u001b[0m 13 loss: \u001b[31m0.0008058638195507228\u001b[0m\n",
      "Best Loss 0.0007627038867212832\n",
      "Best Batch Loss tensor(0.0004, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.000670608424115926)\u001b[0m\n",
      "Learning Rate 0.0048391250000000005\n",
      "\u001b[31mEpoch:\u001b[0m 14 loss: \u001b[31m0.0006943087209947407\u001b[0m\n",
      "Best Loss 0.0006943087209947407\n",
      "Best Batch Loss tensor(0.0003, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0006131839472800493)\u001b[0m\n",
      "Learning Rate 0.00482675\n",
      "\u001b[31mEpoch:\u001b[0m 15 loss: \u001b[31m0.0007450968259945512\u001b[0m\n",
      "Best Loss 0.0006943087209947407\n",
      "Best Batch Loss tensor(0.0003, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0004924095701426268)\u001b[0m\n",
      "Learning Rate 0.004814375\n",
      "\u001b[31mEpoch:\u001b[0m 16 loss: \u001b[31m0.000660059682559222\u001b[0m\n",
      "Best Loss 0.000660059682559222\n",
      "Best Batch Loss tensor(0.0003, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0004484333621803671)\u001b[0m\n",
      "Learning Rate 0.004802\n",
      "\u001b[31mEpoch:\u001b[0m 17 loss: \u001b[31m0.0005379567737691104\u001b[0m\n",
      "Best Loss 0.0005379567737691104\n",
      "Best Batch Loss tensor(0.0003, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00040797359542921185)\u001b[0m\n",
      "Learning Rate 0.0047896250000000005\n",
      "\u001b[31mEpoch:\u001b[0m 18 loss: \u001b[31m0.0005790423601865768\u001b[0m\n",
      "Best Loss 0.0005379567737691104\n",
      "Best Batch Loss tensor(0.0003, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0004383155610412359)\u001b[0m\n",
      "Learning Rate 0.004777249999999999\n",
      "\u001b[31mEpoch:\u001b[0m 19 loss: \u001b[31m0.0005124270683154464\u001b[0m\n",
      "Best Loss 0.0005124270683154464\n",
      "Best Batch Loss tensor(0.0003, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.000558601226657629)\u001b[0m\n",
      "Learning Rate 0.004764875\n",
      "\u001b[31mEpoch:\u001b[0m 20 loss: \u001b[31m0.0004785455821547657\u001b[0m\n",
      "Best Loss 0.0004785455821547657\n",
      "Best Batch Loss tensor(0.0003, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0004129436856601387)\u001b[0m\n",
      "Learning Rate 0.0047525\n",
      "\u001b[31mEpoch:\u001b[0m 21 loss: \u001b[31m0.0005550425266847014\u001b[0m\n",
      "Best Loss 0.0004785455821547657\n",
      "Best Batch Loss tensor(0.0002, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0004578089574351907)\u001b[0m\n",
      "Learning Rate 0.004740125\n",
      "\u001b[31mEpoch:\u001b[0m 22 loss: \u001b[31m0.00044156835065223277\u001b[0m\n",
      "Best Loss 0.00044156835065223277\n",
      "Best Batch Loss tensor(0.0002, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0005350844003260136)\u001b[0m\n",
      "Learning Rate 0.00472775\n",
      "\u001b[31mEpoch:\u001b[0m 23 loss: \u001b[31m0.00038513794424943626\u001b[0m\n",
      "Best Loss 0.00038513794424943626\n",
      "Best Batch Loss tensor(0.0002, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00048512592911720276)\u001b[0m\n",
      "Learning Rate 0.004715375000000001\n",
      "\u001b[31mEpoch:\u001b[0m 24 loss: \u001b[31m0.0004158451047260314\u001b[0m\n",
      "Best Loss 0.00038513794424943626\n",
      "Best Batch Loss tensor(0.0002, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0007504638633690774)\u001b[0m\n",
      "Learning Rate 0.004703000000000001\n",
      "\u001b[31mEpoch:\u001b[0m 25 loss: \u001b[31m0.0004569203592836857\u001b[0m\n",
      "Best Loss 0.00038513794424943626\n",
      "Best Batch Loss tensor(0.0002, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00028891602414660156)\u001b[0m\n",
      "Learning Rate 0.0046906249999999995\n",
      "\u001b[31mEpoch:\u001b[0m 26 loss: \u001b[31m0.0003944328927900642\u001b[0m\n",
      "Best Loss 0.00038513794424943626\n",
      "Best Batch Loss tensor(0.0002, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00031976550235413015)\u001b[0m\n",
      "Learning Rate 0.00467825\n",
      "\u001b[31mEpoch:\u001b[0m 27 loss: \u001b[31m0.0003613426524680108\u001b[0m\n",
      "Best Loss 0.0003613426524680108\n",
      "Best Batch Loss tensor(0.0002, grad_fn=<MseLossBackward>) \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0002808709105011076)\u001b[0m\n",
      "Learning Rate 0.004665875\n",
      "\u001b[31mEpoch:\u001b[0m 28 loss: \u001b[31m0.00035905567347072065\u001b[0m\n",
      "Best Loss 0.00035905567347072065\n",
      "Best Batch Loss tensor(0.0001, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0006729636806994677)\u001b[0m\n",
      "Learning Rate 0.0046535\n",
      "\u001b[31mEpoch:\u001b[0m 29 loss: \u001b[31m0.0003806190798059106\u001b[0m\n",
      "Best Loss 0.00035905567347072065\n",
      "Best Batch Loss tensor(0.0001, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0002381554659223184)\u001b[0m\n",
      "Learning Rate 0.004641125\n",
      "\u001b[31mEpoch:\u001b[0m 30 loss: \u001b[31m0.00036935193929821253\u001b[0m\n",
      "Best Loss 0.00035905567347072065\n",
      "Best Batch Loss tensor(0.0001, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00029967320733703673)\u001b[0m\n",
      "Learning Rate 0.004628750000000001\n",
      "\u001b[31mEpoch:\u001b[0m 31 loss: \u001b[31m0.0003266408748459071\u001b[0m\n",
      "Best Loss 0.0003266408748459071\n",
      "Best Batch Loss tensor(0.0001, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0002446425787638873)\u001b[0m\n",
      "Learning Rate 0.004616375000000001\n",
      "\u001b[31mEpoch:\u001b[0m 32 loss: \u001b[31m0.0003789094334933907\u001b[0m\n",
      "Best Loss 0.0003266408748459071\n",
      "Best Batch Loss tensor(0.0001, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00029726687353104353)\u001b[0m\n",
      "Learning Rate 0.004604\n",
      "\u001b[31mEpoch:\u001b[0m 33 loss: \u001b[31m0.00027148533263243735\u001b[0m\n",
      "Best Loss 0.00027148533263243735\n",
      "Best Batch Loss tensor(0.0001, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0002781499351840466)\u001b[0m\n",
      "Learning Rate 0.004591625\n",
      "\u001b[31mEpoch:\u001b[0m 34 loss: \u001b[31m0.00035729858791455626\u001b[0m\n",
      "Best Loss 0.00027148533263243735\n",
      "Best Batch Loss tensor(0.0001, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0002587819762993604)\u001b[0m\n",
      "Learning Rate 0.00457925\n",
      "\u001b[31mEpoch:\u001b[0m 35 loss: \u001b[31m0.0002831463352777064\u001b[0m\n",
      "Best Loss 0.00027148533263243735\n",
      "Best Batch Loss tensor(0.0001, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0001979779190151021)\u001b[0m\n",
      "Learning Rate 0.004566875\n",
      "\u001b[31mEpoch:\u001b[0m 36 loss: \u001b[31m0.00026984926080331206\u001b[0m\n",
      "Best Loss 0.00026984926080331206\n",
      "Best Batch Loss tensor(0.0001, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0002736220776569098)\u001b[0m\n",
      "Learning Rate 0.0045545\n",
      "\u001b[31mEpoch:\u001b[0m 37 loss: \u001b[31m0.0004156585200689733\u001b[0m\n",
      "Best Loss 0.00026984926080331206\n",
      "Best Batch Loss tensor(0.0001, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0002436754439258948)\u001b[0m\n",
      "Learning Rate 0.004542125\n",
      "\u001b[31mEpoch:\u001b[0m 38 loss: \u001b[31m0.00026923284167423844\u001b[0m\n",
      "Best Loss 0.00026923284167423844\n",
      "Best Batch Loss tensor(0.0001, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.000318677572067827)\u001b[0m\n",
      "Learning Rate 0.004529750000000001\n",
      "\u001b[31mEpoch:\u001b[0m 39 loss: \u001b[31m0.00022986080148257315\u001b[0m\n",
      "Best Loss 0.00022986080148257315\n",
      "Best Batch Loss tensor(8.4348e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0002877798688132316)\u001b[0m\n",
      "Learning Rate 0.004517375\n",
      "\u001b[31mEpoch:\u001b[0m 40 loss: \u001b[31m0.0002932082861661911\u001b[0m\n",
      "Best Loss 0.00022986080148257315\n",
      "Best Batch Loss tensor(8.4348e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0010888678953051567)\u001b[0m\n",
      "Learning Rate 0.004505\n",
      "\u001b[31mEpoch:\u001b[0m 41 loss: \u001b[31m0.00026188683114014566\u001b[0m\n",
      "Best Loss 0.00022986080148257315\n",
      "Best Batch Loss tensor(8.4348e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0002373532042838633)\u001b[0m\n",
      "Learning Rate 0.004492625\n",
      "\u001b[31mEpoch:\u001b[0m 42 loss: \u001b[31m0.00021346738503780216\u001b[0m\n",
      "Best Loss 0.00021346738503780216\n",
      "Best Batch Loss tensor(8.4348e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0002743510704021901)\u001b[0m\n",
      "Learning Rate 0.00448025\n",
      "\u001b[31mEpoch:\u001b[0m 43 loss: \u001b[31m0.00023963258718140423\u001b[0m\n",
      "Best Loss 0.00021346738503780216\n",
      "Best Batch Loss tensor(8.4348e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00018622099014464766)\u001b[0m\n",
      "Learning Rate 0.0044678750000000005\n",
      "\u001b[31mEpoch:\u001b[0m 44 loss: \u001b[31m0.00022344170429278165\u001b[0m\n",
      "Best Loss 0.00021346738503780216\n",
      "Best Batch Loss tensor(7.7124e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00021088600624352694)\u001b[0m\n",
      "Learning Rate 0.0044555\n",
      "\u001b[31mEpoch:\u001b[0m 45 loss: \u001b[31m0.00021355091303121299\u001b[0m\n",
      "Best Loss 0.00021346738503780216\n",
      "Best Batch Loss tensor(7.7124e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00019844996859319508)\u001b[0m\n",
      "Learning Rate 0.004443125\n",
      "\u001b[31mEpoch:\u001b[0m 46 loss: \u001b[31m0.00023791751300450414\u001b[0m\n",
      "Best Loss 0.00021346738503780216\n",
      "Best Batch Loss tensor(7.7124e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00012963646440766752)\u001b[0m\n",
      "Learning Rate 0.00443075\n",
      "\u001b[31mEpoch:\u001b[0m 47 loss: \u001b[31m0.00020471110474318266\u001b[0m\n",
      "Best Loss 0.00020471110474318266\n",
      "Best Batch Loss tensor(7.7124e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00015777972294017673)\u001b[0m\n",
      "Learning Rate 0.004418375\n",
      "\u001b[31mEpoch:\u001b[0m 48 loss: \u001b[31m0.0001860085758380592\u001b[0m\n",
      "Best Loss 0.0001860085758380592\n",
      "Best Batch Loss tensor(7.7124e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00015745327982585877)\u001b[0m\n",
      "Learning Rate 0.004406\n",
      "\u001b[31mEpoch:\u001b[0m 49 loss: \u001b[31m0.00021920162544120103\u001b[0m\n",
      "Best Loss 0.0001860085758380592\n",
      "Best Batch Loss tensor(7.7124e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00022500407067127526)\u001b[0m\n",
      "Learning Rate 0.004393625\n",
      "\u001b[31mEpoch:\u001b[0m 50 loss: \u001b[31m0.00021982623729854822\u001b[0m\n",
      "Best Loss 0.0001860085758380592\n",
      "Best Batch Loss tensor(6.7687e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00014175788965076208)\u001b[0m\n",
      "Learning Rate 0.004381250000000001\n",
      "\u001b[31mEpoch:\u001b[0m 51 loss: \u001b[31m0.0001973737234948203\u001b[0m\n",
      "Best Loss 0.0001860085758380592\n",
      "Best Batch Loss tensor(6.7687e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0001225926971528679)\u001b[0m\n",
      "Learning Rate 0.004368875\n",
      "\u001b[31mEpoch:\u001b[0m 52 loss: \u001b[31m0.00016434404824394733\u001b[0m\n",
      "Best Loss 0.00016434404824394733\n",
      "Best Batch Loss tensor(6.7687e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00010147441207664087)\u001b[0m\n",
      "Learning Rate 0.0043565\n",
      "\u001b[31mEpoch:\u001b[0m 53 loss: \u001b[31m0.00019705590966623276\u001b[0m\n",
      "Best Loss 0.00016434404824394733\n",
      "Best Batch Loss tensor(6.7687e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00011872685718117282)\u001b[0m\n",
      "Learning Rate 0.004344125\n",
      "\u001b[31mEpoch:\u001b[0m 54 loss: \u001b[31m0.00017323937208857387\u001b[0m\n",
      "Best Loss 0.00016434404824394733\n",
      "Best Batch Loss tensor(6.7687e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00016157959180418402)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.00433175\n",
      "\u001b[31mEpoch:\u001b[0m 55 loss: \u001b[31m0.0002598788123577833\u001b[0m\n",
      "Best Loss 0.00016434404824394733\n",
      "Best Batch Loss tensor(6.7687e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00013394170673564076)\u001b[0m\n",
      "Learning Rate 0.004319375\n",
      "\u001b[31mEpoch:\u001b[0m 56 loss: \u001b[31m0.00018641566566657275\u001b[0m\n",
      "Best Loss 0.00016434404824394733\n",
      "Best Batch Loss tensor(6.7687e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00011906294821528718)\u001b[0m\n",
      "Learning Rate 0.004307\n",
      "\u001b[31mEpoch:\u001b[0m 57 loss: \u001b[31m0.00014570419443771243\u001b[0m\n",
      "Best Loss 0.00014570419443771243\n",
      "Best Batch Loss tensor(6.7687e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00012503910693340003)\u001b[0m\n",
      "Learning Rate 0.004294625000000001\n",
      "\u001b[31mEpoch:\u001b[0m 58 loss: \u001b[31m0.00019407054060138762\u001b[0m\n",
      "Best Loss 0.00014570419443771243\n",
      "Best Batch Loss tensor(6.4744e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(8.3040e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 8.303965296363458e-05)\u001b[0m\n",
      "Learning Rate 0.0042822500000000005\n",
      "\u001b[31mEpoch:\u001b[0m 59 loss: \u001b[31m0.00012787232117261738\u001b[0m\n",
      "Best Loss 0.00012787232117261738\n",
      "Best Batch Loss tensor(5.0726e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(8.3040e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0002740882628131658)\u001b[0m\n",
      "Learning Rate 0.004269875\n",
      "\u001b[31mEpoch:\u001b[0m 60 loss: \u001b[31m0.0002045677974820137\u001b[0m\n",
      "Best Loss 0.00012787232117261738\n",
      "Best Batch Loss tensor(5.0726e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(8.3040e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 9.118028538068756e-05)\u001b[0m\n",
      "Learning Rate 0.0042575\n",
      "\u001b[31mEpoch:\u001b[0m 61 loss: \u001b[31m0.00017429274157620966\u001b[0m\n",
      "Best Loss 0.00012787232117261738\n",
      "Best Batch Loss tensor(5.0726e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(8.3040e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00030553474789485335)\u001b[0m\n",
      "Learning Rate 0.004245125\n",
      "\u001b[31mEpoch:\u001b[0m 62 loss: \u001b[31m0.00016998150385916233\u001b[0m\n",
      "Best Loss 0.00012787232117261738\n",
      "Best Batch Loss tensor(5.0726e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(8.3040e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00013143385876901448)\u001b[0m\n",
      "Learning Rate 0.0042327499999999995\n",
      "\u001b[31mEpoch:\u001b[0m 63 loss: \u001b[31m0.00018829299369826913\u001b[0m\n",
      "Best Loss 0.00012787232117261738\n",
      "Best Batch Loss tensor(5.0726e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(8.3040e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0001774999254848808)\u001b[0m\n",
      "Learning Rate 0.004220375\n",
      "\u001b[31mEpoch:\u001b[0m 64 loss: \u001b[31m0.00016755049000494182\u001b[0m\n",
      "Best Loss 0.00012787232117261738\n",
      "Best Batch Loss tensor(5.0726e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(8.3040e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0001359473099000752)\u001b[0m\n",
      "Learning Rate 0.004208\n",
      "\u001b[31mEpoch:\u001b[0m 65 loss: \u001b[31m0.0002154391841031611\u001b[0m\n",
      "Best Loss 0.00012787232117261738\n",
      "Best Batch Loss tensor(5.0726e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(8.3040e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00011862380051752552)\u001b[0m\n",
      "Learning Rate 0.004195625000000001\n",
      "\u001b[31mEpoch:\u001b[0m 66 loss: \u001b[31m0.00016753921227063984\u001b[0m\n",
      "Best Loss 0.00012787232117261738\n",
      "Best Batch Loss tensor(5.0726e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(8.3040e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00010569712321739644)\u001b[0m\n",
      "Learning Rate 0.0041832499999999995\n",
      "\u001b[31mEpoch:\u001b[0m 67 loss: \u001b[31m0.00012893964594695717\u001b[0m\n",
      "Best Loss 0.00012787232117261738\n",
      "Best Batch Loss tensor(5.0726e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(8.3040e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 9.774207865120843e-05)\u001b[0m\n",
      "Learning Rate 0.004170875\n",
      "\u001b[31mEpoch:\u001b[0m 68 loss: \u001b[31m0.0001710853393888101\u001b[0m\n",
      "Best Loss 0.00012787232117261738\n",
      "Best Batch Loss tensor(5.0726e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(8.3040e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00022456658189184964)\u001b[0m\n",
      "Learning Rate 0.0041585\n",
      "\u001b[31mEpoch:\u001b[0m 69 loss: \u001b[31m0.00013502595538739115\u001b[0m\n",
      "Best Loss 0.00012787232117261738\n",
      "Best Batch Loss tensor(5.0726e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(8.3040e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00014300763723440468)\u001b[0m\n",
      "Learning Rate 0.0041461250000000005\n",
      "\u001b[31mEpoch:\u001b[0m 70 loss: \u001b[31m0.0001596818765392527\u001b[0m\n",
      "Best Loss 0.00012787232117261738\n",
      "Best Batch Loss tensor(5.0726e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(8.3040e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0001234392257174477)\u001b[0m\n",
      "Learning Rate 0.00413375\n",
      "\u001b[31mEpoch:\u001b[0m 71 loss: \u001b[31m0.00012700387742370367\u001b[0m\n",
      "Best Loss 0.00012700387742370367\n",
      "Best Batch Loss tensor(5.0726e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(8.3040e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0002022229164140299)\u001b[0m\n",
      "Learning Rate 0.004121375\n",
      "\u001b[31mEpoch:\u001b[0m 72 loss: \u001b[31m0.00015341625839937478\u001b[0m\n",
      "Best Loss 0.00012700387742370367\n",
      "Best Batch Loss tensor(4.9976e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(8.3040e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0001648244506213814)\u001b[0m\n",
      "Learning Rate 0.004109000000000001\n",
      "\u001b[31mEpoch:\u001b[0m 73 loss: \u001b[31m0.00012076192069798708\u001b[0m\n",
      "Best Loss 0.00012076192069798708\n",
      "Best Batch Loss tensor(4.9976e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(8.3040e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 9.76057635853067e-05)\u001b[0m\n",
      "Learning Rate 0.004096625\n",
      "\u001b[31mEpoch:\u001b[0m 74 loss: \u001b[31m0.00015147267549764365\u001b[0m\n",
      "Best Loss 0.00012076192069798708\n",
      "Best Batch Loss tensor(4.9976e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(8.3040e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00016706337919458747)\u001b[0m\n",
      "Learning Rate 0.004084249999999999\n",
      "\u001b[31mEpoch:\u001b[0m 75 loss: \u001b[31m0.00015515743871219456\u001b[0m\n",
      "Best Loss 0.00012076192069798708\n",
      "Best Batch Loss tensor(4.9976e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(7.7564e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 7.7564109233208e-05)\u001b[0m\n",
      "Learning Rate 0.004071875\n",
      "\u001b[31mEpoch:\u001b[0m 76 loss: \u001b[31m0.00011264814384048805\u001b[0m\n",
      "Best Loss 0.00011264814384048805\n",
      "Best Batch Loss tensor(4.4668e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(7.7564e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00012595695443451405)\u001b[0m\n",
      "Learning Rate 0.004059500000000001\n",
      "\u001b[31mEpoch:\u001b[0m 77 loss: \u001b[31m0.00013813780969940126\u001b[0m\n",
      "Best Loss 0.00011264814384048805\n",
      "Best Batch Loss tensor(4.4668e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(7.7564e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00010832908446900547)\u001b[0m\n",
      "Learning Rate 0.004047125\n",
      "\u001b[31mEpoch:\u001b[0m 78 loss: \u001b[31m0.00012885493924841285\u001b[0m\n",
      "Best Loss 0.00011264814384048805\n",
      "Best Batch Loss tensor(4.4668e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(7.7564e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 9.473328100284562e-05)\u001b[0m\n",
      "Learning Rate 0.00403475\n",
      "\u001b[31mEpoch:\u001b[0m 79 loss: \u001b[31m0.00014904765703249723\u001b[0m\n",
      "Best Loss 0.00011264814384048805\n",
      "Best Batch Loss tensor(4.4668e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(7.7564e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00012568774400278926)\u001b[0m\n",
      "Learning Rate 0.004022375000000001\n",
      "\u001b[31mEpoch:\u001b[0m 80 loss: \u001b[31m0.00011875185009557754\u001b[0m\n",
      "Best Loss 0.00011264814384048805\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(7.7564e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00017331588605884463)\u001b[0m\n",
      "Learning Rate 0.00401\n",
      "\u001b[31mEpoch:\u001b[0m 81 loss: \u001b[31m0.0001356658322038129\u001b[0m\n",
      "Best Loss 0.00011264814384048805\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(7.7564e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00010537469643168151)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0039976249999999994\n",
      "\u001b[31mEpoch:\u001b[0m 82 loss: \u001b[31m0.00012479910219553858\u001b[0m\n",
      "Best Loss 0.00011264814384048805\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(7.7564e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 8.864723349688575e-05)\u001b[0m\n",
      "Learning Rate 0.00398525\n",
      "\u001b[31mEpoch:\u001b[0m 83 loss: \u001b[31m0.00018073210958391428\u001b[0m\n",
      "Best Loss 0.00011264814384048805\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(7.7564e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00015516896382905543)\u001b[0m\n",
      "Learning Rate 0.003972875\n",
      "\u001b[31mEpoch:\u001b[0m 84 loss: \u001b[31m0.00011033200280508026\u001b[0m\n",
      "Best Loss 0.00011033200280508026\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(7.6320e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 7.631996413692832e-05)\u001b[0m\n",
      "Learning Rate 0.0039605000000000005\n",
      "\u001b[31mEpoch:\u001b[0m 85 loss: \u001b[31m0.00014255598944146186\u001b[0m\n",
      "Best Loss 0.00011033200280508026\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(7.6320e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 8.209970110328868e-05)\u001b[0m\n",
      "Learning Rate 0.003948125\n",
      "\u001b[31mEpoch:\u001b[0m 86 loss: \u001b[31m0.00013304479944054037\u001b[0m\n",
      "Best Loss 0.00011033200280508026\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(7.6320e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 7.782600005157292e-05)\u001b[0m\n",
      "Learning Rate 0.003935750000000001\n",
      "\u001b[31mEpoch:\u001b[0m 87 loss: \u001b[31m0.00010007178934756666\u001b[0m\n",
      "Best Loss 0.00010007178934756666\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(7.6320e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 8.863205584930256e-05)\u001b[0m\n",
      "Learning Rate 0.003923375\n",
      "\u001b[31mEpoch:\u001b[0m 88 loss: \u001b[31m0.0001011202548397705\u001b[0m\n",
      "Best Loss 0.00010007178934756666\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(7.3474e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 7.347382052103058e-05)\u001b[0m\n",
      "Learning Rate 0.003911\n",
      "\u001b[31mEpoch:\u001b[0m 89 loss: \u001b[31m0.00017784611554816365\u001b[0m\n",
      "Best Loss 0.00010007178934756666\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(7.3474e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0003448595234658569)\u001b[0m\n",
      "Learning Rate 0.0038986249999999997\n",
      "\u001b[31mEpoch:\u001b[0m 90 loss: \u001b[31m9.093886910704896e-05\u001b[0m\n",
      "Best Loss 9.093886910704896e-05\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(7.3474e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 7.664029544685036e-05)\u001b[0m\n",
      "Learning Rate 0.0038862500000000004\n",
      "\u001b[31mEpoch:\u001b[0m 91 loss: \u001b[31m9.631059947423637e-05\u001b[0m\n",
      "Best Loss 9.093886910704896e-05\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(7.3474e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00026309964596293867)\u001b[0m\n",
      "Learning Rate 0.003873875\n",
      "\u001b[31mEpoch:\u001b[0m 92 loss: \u001b[31m0.00014949619071558118\u001b[0m\n",
      "Best Loss 9.093886910704896e-05\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(7.3474e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 7.48431557440199e-05)\u001b[0m\n",
      "Learning Rate 0.0038615\n",
      "\u001b[31mEpoch:\u001b[0m 93 loss: \u001b[31m8.937099482864141e-05\u001b[0m\n",
      "Best Loss 8.937099482864141e-05\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(6.0009e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 6.000895155011676e-05)\u001b[0m\n",
      "Learning Rate 0.0038491249999999997\n",
      "\u001b[31mEpoch:\u001b[0m 94 loss: \u001b[31m9.074802801478654e-05\u001b[0m\n",
      "Best Loss 8.937099482864141e-05\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(5.7475e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.747539398726076e-05)\u001b[0m\n",
      "Learning Rate 0.0038367500000000003\n",
      "\u001b[31mEpoch:\u001b[0m 95 loss: \u001b[31m0.00010869642574107274\u001b[0m\n",
      "Best Loss 8.937099482864141e-05\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(5.7475e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 9.368150494992733e-05)\u001b[0m\n",
      "Learning Rate 0.003824375\n",
      "\u001b[31mEpoch:\u001b[0m 96 loss: \u001b[31m0.00013269958435557783\u001b[0m\n",
      "Best Loss 8.937099482864141e-05\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(5.7475e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00012375932419672608)\u001b[0m\n",
      "Learning Rate 0.003812\n",
      "\u001b[31mEpoch:\u001b[0m 97 loss: \u001b[31m0.00010065577953355387\u001b[0m\n",
      "Best Loss 8.937099482864141e-05\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(5.7475e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 7.995306805241853e-05)\u001b[0m\n",
      "Learning Rate 0.0037996250000000005\n",
      "\u001b[31mEpoch:\u001b[0m 98 loss: \u001b[31m0.00013446742377709597\u001b[0m\n",
      "Best Loss 8.937099482864141e-05\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(5.7475e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 8.250596874859184e-05)\u001b[0m\n",
      "Learning Rate 0.0037872500000000003\n",
      "\u001b[31mEpoch:\u001b[0m 99 loss: \u001b[31m9.334865171695128e-05\u001b[0m\n",
      "Best Loss 8.937099482864141e-05\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(5.7475e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 9.376584785059094e-05)\u001b[0m\n",
      "Learning Rate 0.0037748749999999996\n",
      "\u001b[31mEpoch:\u001b[0m 100 loss: \u001b[31m0.0001047421756084077\u001b[0m\n",
      "Best Loss 8.937099482864141e-05\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(5.7475e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 8.886555588105693e-05)\u001b[0m\n",
      "Learning Rate 0.0037625\n",
      "\u001b[31mEpoch:\u001b[0m 101 loss: \u001b[31m0.00010620168905006722\u001b[0m\n",
      "Best Loss 8.937099482864141e-05\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(5.7475e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00014981359709054232)\u001b[0m\n",
      "Learning Rate 0.0037501250000000004\n",
      "\u001b[31mEpoch:\u001b[0m 102 loss: \u001b[31m0.00013645499711856246\u001b[0m\n",
      "Best Loss 8.937099482864141e-05\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(5.7475e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00010240801202598959)\u001b[0m\n",
      "Learning Rate 0.00373775\n",
      "\u001b[31mEpoch:\u001b[0m 103 loss: \u001b[31m0.00010269583435729146\u001b[0m\n",
      "Best Loss 8.937099482864141e-05\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(5.7475e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00015681491640862077)\u001b[0m\n",
      "Learning Rate 0.0037253749999999995\n",
      "\u001b[31mEpoch:\u001b[0m 104 loss: \u001b[31m9.48326924117282e-05\u001b[0m\n",
      "Best Loss 8.937099482864141e-05\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(5.7475e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00013728602789342403)\u001b[0m\n",
      "Learning Rate 0.003713\n",
      "\u001b[31mEpoch:\u001b[0m 105 loss: \u001b[31m0.0001452486903872341\u001b[0m\n",
      "Best Loss 8.937099482864141e-05\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(5.7475e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0004163356206845492)\u001b[0m\n",
      "Learning Rate 0.0037006250000000004\n",
      "\u001b[31mEpoch:\u001b[0m 106 loss: \u001b[31m0.00010603012196952477\u001b[0m\n",
      "Best Loss 8.937099482864141e-05\n",
      "Best Batch Loss tensor(3.0889e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(5.7475e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 8.321868517668918e-05)\u001b[0m\n",
      "Learning Rate 0.0036882499999999997\n",
      "\u001b[31mEpoch:\u001b[0m 107 loss: \u001b[31m7.44407152524218e-05\u001b[0m\n",
      "Best Loss 7.44407152524218e-05\n",
      "Best Batch Loss tensor(3.0084e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(5.7475e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 6.464331090683118e-05)\u001b[0m\n",
      "Learning Rate 0.0036758749999999995\n",
      "\u001b[31mEpoch:\u001b[0m 108 loss: \u001b[31m0.0001172355332528241\u001b[0m\n",
      "Best Loss 7.44407152524218e-05\n",
      "Best Batch Loss tensor(3.0084e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(5.3475e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.3475105232791975e-05)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0036635\n",
      "\u001b[31mEpoch:\u001b[0m 109 loss: \u001b[31m8.744148362893611e-05\u001b[0m\n",
      "Best Loss 7.44407152524218e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(5.3475e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.0001406745723215863)\u001b[0m\n",
      "Learning Rate 0.0036511250000000003\n",
      "\u001b[31mEpoch:\u001b[0m 110 loss: \u001b[31m0.00012791242625098675\u001b[0m\n",
      "Best Loss 7.44407152524218e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(5.3475e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00010598496737657115)\u001b[0m\n",
      "Learning Rate 0.0036387499999999996\n",
      "\u001b[31mEpoch:\u001b[0m 111 loss: \u001b[31m7.715748506598175e-05\u001b[0m\n",
      "Best Loss 7.44407152524218e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(5.3475e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00012649102427531034)\u001b[0m\n",
      "Learning Rate 0.003626375\n",
      "\u001b[31mEpoch:\u001b[0m 112 loss: \u001b[31m8.759176853345707e-05\u001b[0m\n",
      "Best Loss 7.44407152524218e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(5.3475e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00010024014045484364)\u001b[0m\n",
      "Learning Rate 0.003614\n",
      "\u001b[31mEpoch:\u001b[0m 113 loss: \u001b[31m0.00012490228982642293\u001b[0m\n",
      "Best Loss 7.44407152524218e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(5.3475e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.937425885349512e-05)\u001b[0m\n",
      "Learning Rate 0.0036016250000000002\n",
      "\u001b[31mEpoch:\u001b[0m 114 loss: \u001b[31m0.00010397406731499359\u001b[0m\n",
      "Best Loss 7.44407152524218e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(5.3475e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 6.42500162939541e-05)\u001b[0m\n",
      "Learning Rate 0.0035892500000000004\n",
      "\u001b[31mEpoch:\u001b[0m 115 loss: \u001b[31m8.060333493631333e-05\u001b[0m\n",
      "Best Loss 7.44407152524218e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(5.3475e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00012175576557638124)\u001b[0m\n",
      "Learning Rate 0.003576875\n",
      "\u001b[31mEpoch:\u001b[0m 116 loss: \u001b[31m0.00014532412751577795\u001b[0m\n",
      "Best Loss 7.44407152524218e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(5.3475e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00013182705151848495)\u001b[0m\n",
      "Learning Rate 0.0035645\n",
      "\u001b[31mEpoch:\u001b[0m 117 loss: \u001b[31m8.571097714593634e-05\u001b[0m\n",
      "Best Loss 7.44407152524218e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(5.3475e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.796114055556245e-05)\u001b[0m\n",
      "Learning Rate 0.003552125\n",
      "\u001b[31mEpoch:\u001b[0m 118 loss: \u001b[31m7.814844138920307e-05\u001b[0m\n",
      "Best Loss 7.44407152524218e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(5.3475e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.42273155588191e-05)\u001b[0m\n",
      "Learning Rate 0.0035397500000000004\n",
      "\u001b[31mEpoch:\u001b[0m 119 loss: \u001b[31m7.772868411848322e-05\u001b[0m\n",
      "Best Loss 7.44407152524218e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.6638e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.6637756895506755e-05)\u001b[0m\n",
      "Learning Rate 0.003527375\n",
      "\u001b[31mEpoch:\u001b[0m 120 loss: \u001b[31m8.270431135315448e-05\u001b[0m\n",
      "Best Loss 7.44407152524218e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.6638e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 8.702938066562638e-05)\u001b[0m\n",
      "Learning Rate 0.0035149999999999995\n",
      "\u001b[31mEpoch:\u001b[0m 121 loss: \u001b[31m8.363131928490475e-05\u001b[0m\n",
      "Best Loss 7.44407152524218e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.6638e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 6.805854354752228e-05)\u001b[0m\n",
      "Learning Rate 0.003502625\n",
      "\u001b[31mEpoch:\u001b[0m 122 loss: \u001b[31m0.00015021087892819196\u001b[0m\n",
      "Best Loss 7.44407152524218e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.6638e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 7.342746539507061e-05)\u001b[0m\n",
      "Learning Rate 0.0034902500000000003\n",
      "\u001b[31mEpoch:\u001b[0m 123 loss: \u001b[31m5.6724133173702285e-05\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.1077739297179505e-05)\u001b[0m\n",
      "Learning Rate 0.0034778750000000005\n",
      "\u001b[31mEpoch:\u001b[0m 124 loss: \u001b[31m0.00010138229845324531\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 7.208440365502611e-05)\u001b[0m\n",
      "Learning Rate 0.0034655\n",
      "\u001b[31mEpoch:\u001b[0m 125 loss: \u001b[31m7.962049858178943e-05\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.530176713364199e-05)\u001b[0m\n",
      "Learning Rate 0.003453125\n",
      "\u001b[31mEpoch:\u001b[0m 126 loss: \u001b[31m9.018793207360432e-05\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.650080154533498e-05)\u001b[0m\n",
      "Learning Rate 0.0034407500000000007\n",
      "\u001b[31mEpoch:\u001b[0m 127 loss: \u001b[31m8.876245556166396e-05\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 7.475837628589943e-05)\u001b[0m\n",
      "Learning Rate 0.003428375\n",
      "\u001b[31mEpoch:\u001b[0m 128 loss: \u001b[31m8.340074418811128e-05\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 8.57113191159442e-05)\u001b[0m\n",
      "Learning Rate 0.0034159999999999998\n",
      "\u001b[31mEpoch:\u001b[0m 129 loss: \u001b[31m7.779398583807051e-05\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.260936450213194e-05)\u001b[0m\n",
      "Learning Rate 0.003403625\n",
      "\u001b[31mEpoch:\u001b[0m 130 loss: \u001b[31m8.046959555940703e-05\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 8.557904948247597e-05)\u001b[0m\n",
      "Learning Rate 0.0033912500000000006\n",
      "\u001b[31mEpoch:\u001b[0m 131 loss: \u001b[31m9.3452850705944e-05\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 6.110100366640836e-05)\u001b[0m\n",
      "Learning Rate 0.003378875\n",
      "\u001b[31mEpoch:\u001b[0m 132 loss: \u001b[31m8.491204062011093e-05\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 6.589887198060751e-05)\u001b[0m\n",
      "Learning Rate 0.0033664999999999997\n",
      "\u001b[31mEpoch:\u001b[0m 133 loss: \u001b[31m6.699525692965835e-05\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 7.9555124102626e-05)\u001b[0m\n",
      "Learning Rate 0.0033541250000000003\n",
      "\u001b[31mEpoch:\u001b[0m 134 loss: \u001b[31m9.666467667557299e-05\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.984614861314185e-05)\u001b[0m\n",
      "Learning Rate 0.00334175\n",
      "\u001b[31mEpoch:\u001b[0m 135 loss: \u001b[31m8.776091272011399e-05\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 9.490158117841929e-05)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.003329375\n",
      "\u001b[31mEpoch:\u001b[0m 136 loss: \u001b[31m6.36617187410593e-05\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.7980e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 6.952546391403303e-05)\u001b[0m\n",
      "Learning Rate 0.0033169999999999996\n",
      "\u001b[31mEpoch:\u001b[0m 137 loss: \u001b[31m7.648918108316138e-05\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.3053e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 6.868276977911592e-05)\u001b[0m\n",
      "Learning Rate 0.0033046250000000003\n",
      "\u001b[31mEpoch:\u001b[0m 138 loss: \u001b[31m7.543487299699336e-05\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.3053e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 9.607349056750536e-05)\u001b[0m\n",
      "Learning Rate 0.00329225\n",
      "\u001b[31mEpoch:\u001b[0m 139 loss: \u001b[31m6.716189091093838e-05\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.3053e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 7.414965511998162e-05)\u001b[0m\n",
      "Learning Rate 0.0032798750000000007\n",
      "\u001b[31mEpoch:\u001b[0m 140 loss: \u001b[31m0.0001065643664333038\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.3053e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.878478623344563e-05)\u001b[0m\n",
      "Learning Rate 0.0032675000000000004\n",
      "\u001b[31mEpoch:\u001b[0m 141 loss: \u001b[31m7.662391726626083e-05\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.3053e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 7.501384243369102e-05)\u001b[0m\n",
      "Learning Rate 0.0032551249999999998\n",
      "\u001b[31mEpoch:\u001b[0m 142 loss: \u001b[31m6.706759450025856e-05\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.3053e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 6.028809730196372e-05)\u001b[0m\n",
      "Learning Rate 0.00324275\n",
      "\u001b[31mEpoch:\u001b[0m 143 loss: \u001b[31m6.358033715514466e-05\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.3053e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 6.503124313894659e-05)\u001b[0m\n",
      "Learning Rate 0.0032303750000000006\n",
      "\u001b[31mEpoch:\u001b[0m 144 loss: \u001b[31m7.190355245256796e-05\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.3053e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 8.614911348558962e-05)\u001b[0m\n",
      "Learning Rate 0.003218\n",
      "\u001b[31mEpoch:\u001b[0m 145 loss: \u001b[31m7.711358921369538e-05\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.3053e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00010713290248531848)\u001b[0m\n",
      "Learning Rate 0.0032056249999999997\n",
      "\u001b[31mEpoch:\u001b[0m 146 loss: \u001b[31m9.399890404893085e-05\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.3053e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.586986324284226e-05)\u001b[0m\n",
      "Learning Rate 0.0031932500000000003\n",
      "\u001b[31mEpoch:\u001b[0m 147 loss: \u001b[31m6.901951564941555e-05\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.3053e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 8.835778862703592e-05)\u001b[0m\n",
      "Learning Rate 0.0031808750000000005\n",
      "\u001b[31mEpoch:\u001b[0m 148 loss: \u001b[31m9.454870451008901e-05\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.3053e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 8.389492722926661e-05)\u001b[0m\n",
      "Learning Rate 0.0031685\n",
      "\u001b[31mEpoch:\u001b[0m 149 loss: \u001b[31m7.368528167717159e-05\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.3053e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.0811122491722926e-05)\u001b[0m\n",
      "Learning Rate 0.0031561249999999996\n",
      "\u001b[31mEpoch:\u001b[0m 150 loss: \u001b[31m6.804563599871472e-05\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.3053e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 9.260185470338911e-05)\u001b[0m\n",
      "Learning Rate 0.0031437500000000003\n",
      "\u001b[31mEpoch:\u001b[0m 151 loss: \u001b[31m0.00010592136823106557\u001b[0m\n",
      "Best Loss 5.6724133173702285e-05\n",
      "Best Batch Loss tensor(2.3053e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 8.18642001831904e-05)\u001b[0m\n",
      "Learning Rate 0.003131375\n",
      "\u001b[31mEpoch:\u001b[0m 152 loss: \u001b[31m5.553392111323774e-05\u001b[0m\n",
      "Best Loss 5.553392111323774e-05\n",
      "Best Batch Loss tensor(2.3053e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.337307811714709e-05)\u001b[0m\n",
      "Learning Rate 0.003119\n",
      "\u001b[31mEpoch:\u001b[0m 153 loss: \u001b[31m6.742453842889518e-05\u001b[0m\n",
      "Best Loss 5.553392111323774e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00015122038894332945)\u001b[0m\n",
      "Learning Rate 0.003106625\n",
      "\u001b[31mEpoch:\u001b[0m 154 loss: \u001b[31m9.247117122868076e-05\u001b[0m\n",
      "Best Loss 5.553392111323774e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.581423490890302e-05)\u001b[0m\n",
      "Learning Rate 0.00309425\n",
      "\u001b[31mEpoch:\u001b[0m 155 loss: \u001b[31m6.115363794378936e-05\u001b[0m\n",
      "Best Loss 5.553392111323774e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.316398528520949e-05)\u001b[0m\n",
      "Learning Rate 0.003081875\n",
      "\u001b[31mEpoch:\u001b[0m 156 loss: \u001b[31m7.721454312559217e-05\u001b[0m\n",
      "Best Loss 5.553392111323774e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.233459160081111e-05)\u001b[0m\n",
      "Learning Rate 0.0030695\n",
      "\u001b[31mEpoch:\u001b[0m 157 loss: \u001b[31m7.853782881284133e-05\u001b[0m\n",
      "Best Loss 5.553392111323774e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 6.399473932106048e-05)\u001b[0m\n",
      "Learning Rate 0.003057125\n",
      "\u001b[31mEpoch:\u001b[0m 158 loss: \u001b[31m5.525951200979762e-05\u001b[0m\n",
      "Best Loss 5.525951200979762e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.5311589676421136e-05)\u001b[0m\n",
      "Learning Rate 0.0030447499999999997\n",
      "\u001b[31mEpoch:\u001b[0m 159 loss: \u001b[31m7.917732000350952e-05\u001b[0m\n",
      "Best Loss 5.525951200979762e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(4.1078e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 8.619484287919477e-05)\u001b[0m\n",
      "Learning Rate 0.0030323750000000003\n",
      "\u001b[31mEpoch:\u001b[0m 160 loss: \u001b[31m7.400075264740735e-05\u001b[0m\n",
      "Best Loss 5.525951200979762e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(3.3062e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.306223879917525e-05)\u001b[0m\n",
      "Learning Rate 0.00302\n",
      "\u001b[31mEpoch:\u001b[0m 161 loss: \u001b[31m7.27831429685466e-05\u001b[0m\n",
      "Best Loss 5.525951200979762e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.9697795980609953e-05)\u001b[0m\n",
      "Learning Rate 0.003007625\n",
      "\u001b[31mEpoch:\u001b[0m 162 loss: \u001b[31m7.523116073571146e-05\u001b[0m\n",
      "Best Loss 5.525951200979762e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.537080894806422e-05)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0029952499999999996\n",
      "\u001b[31mEpoch:\u001b[0m 163 loss: \u001b[31m5.912885899306275e-05\u001b[0m\n",
      "Best Loss 5.525951200979762e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.661808295873925e-05)\u001b[0m\n",
      "Learning Rate 0.0029828750000000003\n",
      "\u001b[31mEpoch:\u001b[0m 164 loss: \u001b[31m6.670345464954153e-05\u001b[0m\n",
      "Best Loss 5.525951200979762e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 8.043929119594395e-05)\u001b[0m\n",
      "Learning Rate 0.0029705000000000005\n",
      "\u001b[31mEpoch:\u001b[0m 165 loss: \u001b[31m6.652166484855115e-05\u001b[0m\n",
      "Best Loss 5.525951200979762e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.055263318354264e-05)\u001b[0m\n",
      "Learning Rate 0.002958125\n",
      "\u001b[31mEpoch:\u001b[0m 166 loss: \u001b[31m4.897929829894565e-05\u001b[0m\n",
      "Best Loss 4.897929829894565e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 7.746279879938811e-05)\u001b[0m\n",
      "Learning Rate 0.00294575\n",
      "\u001b[31mEpoch:\u001b[0m 167 loss: \u001b[31m7.170877506723627e-05\u001b[0m\n",
      "Best Loss 4.897929829894565e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.656786379404366e-05)\u001b[0m\n",
      "Learning Rate 0.0029333750000000002\n",
      "\u001b[31mEpoch:\u001b[0m 168 loss: \u001b[31m5.8485213230596855e-05\u001b[0m\n",
      "Best Loss 4.897929829894565e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.9248534449143335e-05)\u001b[0m\n",
      "Learning Rate 0.0029210000000000004\n",
      "\u001b[31mEpoch:\u001b[0m 169 loss: \u001b[31m6.0761642089346424e-05\u001b[0m\n",
      "Best Loss 4.897929829894565e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.577948154998012e-05)\u001b[0m\n",
      "Learning Rate 0.002908625\n",
      "\u001b[31mEpoch:\u001b[0m 170 loss: \u001b[31m6.679387297481298e-05\u001b[0m\n",
      "Best Loss 4.897929829894565e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 7.357013964792714e-05)\u001b[0m\n",
      "Learning Rate 0.00289625\n",
      "\u001b[31mEpoch:\u001b[0m 171 loss: \u001b[31m7.607432053191587e-05\u001b[0m\n",
      "Best Loss 4.897929829894565e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00011651233944576234)\u001b[0m\n",
      "Learning Rate 0.002883875\n",
      "\u001b[31mEpoch:\u001b[0m 172 loss: \u001b[31m5.568585402215831e-05\u001b[0m\n",
      "Best Loss 4.897929829894565e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.397406130214222e-05)\u001b[0m\n",
      "Learning Rate 0.0028715000000000004\n",
      "\u001b[31mEpoch:\u001b[0m 173 loss: \u001b[31m5.986159885651432e-05\u001b[0m\n",
      "Best Loss 4.897929829894565e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.166788832866587e-05)\u001b[0m\n",
      "Learning Rate 0.002859125\n",
      "\u001b[31mEpoch:\u001b[0m 174 loss: \u001b[31m7.830174581613392e-05\u001b[0m\n",
      "Best Loss 4.897929829894565e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.812762563233264e-05)\u001b[0m\n",
      "Learning Rate 0.00284675\n",
      "\u001b[31mEpoch:\u001b[0m 175 loss: \u001b[31m7.006541272858158e-05\u001b[0m\n",
      "Best Loss 4.897929829894565e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 9.941068128682673e-05)\u001b[0m\n",
      "Learning Rate 0.002834375\n",
      "\u001b[31mEpoch:\u001b[0m 176 loss: \u001b[31m5.363398668123409e-05\u001b[0m\n",
      "Best Loss 4.897929829894565e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.304058868205175e-05)\u001b[0m\n",
      "Learning Rate 0.0028220000000000003\n",
      "\u001b[31mEpoch:\u001b[0m 177 loss: \u001b[31m5.9203317505307496e-05\u001b[0m\n",
      "Best Loss 4.897929829894565e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.830358375329524e-05)\u001b[0m\n",
      "Learning Rate 0.002809625\n",
      "\u001b[31mEpoch:\u001b[0m 178 loss: \u001b[31m4.267595068085939e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.0264592826133594e-05)\u001b[0m\n",
      "Learning Rate 0.00279725\n",
      "\u001b[31mEpoch:\u001b[0m 179 loss: \u001b[31m9.346810838906094e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(2.1436e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.3312058803858235e-05)\u001b[0m\n",
      "Learning Rate 0.002784875\n",
      "\u001b[31mEpoch:\u001b[0m 180 loss: \u001b[31m4.6140285121509805e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(2.1425e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.525409920257516e-05)\u001b[0m\n",
      "Learning Rate 0.0027725000000000002\n",
      "\u001b[31mEpoch:\u001b[0m 181 loss: \u001b[31m4.315870683058165e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.9716e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.45907971879933e-05)\u001b[0m\n",
      "Learning Rate 0.002760125\n",
      "\u001b[31mEpoch:\u001b[0m 182 loss: \u001b[31m5.818294084747322e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.9716e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.098027395433746e-05)\u001b[0m\n",
      "Learning Rate 0.0027477499999999998\n",
      "\u001b[31mEpoch:\u001b[0m 183 loss: \u001b[31m5.3618132369592786e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.9716e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.602196713676676e-05)\u001b[0m\n",
      "Learning Rate 0.002735375\n",
      "\u001b[31mEpoch:\u001b[0m 184 loss: \u001b[31m6.30528011242859e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.9716e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.9910790544818155e-05)\u001b[0m\n",
      "Learning Rate 0.002723\n",
      "\u001b[31mEpoch:\u001b[0m 185 loss: \u001b[31m6.233868043636903e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.9716e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.066423935000785e-05)\u001b[0m\n",
      "Learning Rate 0.002710625\n",
      "\u001b[31mEpoch:\u001b[0m 186 loss: \u001b[31m4.9752288759918883e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.9716e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.0272672524442896e-05)\u001b[0m\n",
      "Learning Rate 0.0026982499999999997\n",
      "\u001b[31mEpoch:\u001b[0m 187 loss: \u001b[31m5.5538690503453836e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.9716e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00010607277363305911)\u001b[0m\n",
      "Learning Rate 0.002685875\n",
      "\u001b[31mEpoch:\u001b[0m 188 loss: \u001b[31m5.367667836253531e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.9716e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 6.535887951031327e-05)\u001b[0m\n",
      "Learning Rate 0.0026735\n",
      "\u001b[31mEpoch:\u001b[0m 189 loss: \u001b[31m5.823795436299406e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.9716e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.430546712479554e-05)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0026611250000000007\n",
      "\u001b[31mEpoch:\u001b[0m 190 loss: \u001b[31m5.0786344218067825e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.9716e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.4083029933972284e-05)\u001b[0m\n",
      "Learning Rate 0.00264875\n",
      "\u001b[31mEpoch:\u001b[0m 191 loss: \u001b[31m5.177648927201517e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.9716e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.614127803710289e-05)\u001b[0m\n",
      "Learning Rate 0.002636375\n",
      "\u001b[31mEpoch:\u001b[0m 192 loss: \u001b[31m6.059815495973453e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.9716e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.684819188900292e-05)\u001b[0m\n",
      "Learning Rate 0.0026240000000000005\n",
      "\u001b[31mEpoch:\u001b[0m 193 loss: \u001b[31m5.4938886023592204e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.9716e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.115003295941278e-05)\u001b[0m\n",
      "Learning Rate 0.0026116250000000002\n",
      "\u001b[31mEpoch:\u001b[0m 194 loss: \u001b[31m5.299373151501641e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.9716e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.404361970955506e-05)\u001b[0m\n",
      "Learning Rate 0.00259925\n",
      "\u001b[31mEpoch:\u001b[0m 195 loss: \u001b[31m4.914406963507645e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.9716e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 6.725227285642177e-05)\u001b[0m\n",
      "Learning Rate 0.0025868749999999998\n",
      "\u001b[31mEpoch:\u001b[0m 196 loss: \u001b[31m4.7900652134558186e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.9716e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.5119937820127234e-05)\u001b[0m\n",
      "Learning Rate 0.0025745000000000004\n",
      "\u001b[31mEpoch:\u001b[0m 197 loss: \u001b[31m5.602363671641797e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.9716e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 6.101163671701215e-05)\u001b[0m\n",
      "Learning Rate 0.002562125\n",
      "\u001b[31mEpoch:\u001b[0m 198 loss: \u001b[31m5.614442488877103e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.9716e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.747594332206063e-05)\u001b[0m\n",
      "Learning Rate 0.00254975\n",
      "\u001b[31mEpoch:\u001b[0m 199 loss: \u001b[31m4.5571701775770634e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.5390e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.369642192614265e-05)\u001b[0m\n",
      "Learning Rate 0.002537375\n",
      "\u001b[31mEpoch:\u001b[0m 200 loss: \u001b[31m5.2066166972508654e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.5390e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.628927465295419e-05)\u001b[0m\n",
      "Learning Rate 0.002525\n",
      "\u001b[31mEpoch:\u001b[0m 201 loss: \u001b[31m5.1958824769826606e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.5390e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.92777032579761e-05)\u001b[0m\n",
      "Learning Rate 0.002512625\n",
      "\u001b[31mEpoch:\u001b[0m 202 loss: \u001b[31m5.1296399760758504e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.5390e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.8763071643188596e-05)\u001b[0m\n",
      "Learning Rate 0.0025002500000000003\n",
      "\u001b[31mEpoch:\u001b[0m 203 loss: \u001b[31m4.782849646289833e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.5390e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.5501576096285135e-05)\u001b[0m\n",
      "Learning Rate 0.002487875\n",
      "\u001b[31mEpoch:\u001b[0m 204 loss: \u001b[31m5.194757250137627e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.5390e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.4887645597336814e-05)\u001b[0m\n",
      "Learning Rate 0.0024755\n",
      "\u001b[31mEpoch:\u001b[0m 205 loss: \u001b[31m4.7883291699690744e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.5390e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.7338297261158004e-05)\u001b[0m\n",
      "Learning Rate 0.0024631250000000005\n",
      "\u001b[31mEpoch:\u001b[0m 206 loss: \u001b[31m4.717594129033387e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.5390e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.380093039595522e-05)\u001b[0m\n",
      "Learning Rate 0.0024507500000000002\n",
      "\u001b[31mEpoch:\u001b[0m 207 loss: \u001b[31m4.851694393437356e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.5390e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 0.00010513643064768985)\u001b[0m\n",
      "Learning Rate 0.002438375\n",
      "\u001b[31mEpoch:\u001b[0m 208 loss: \u001b[31m5.069515827926807e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.5390e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.623482189141214e-05)\u001b[0m\n",
      "Learning Rate 0.0024259999999999998\n",
      "\u001b[31mEpoch:\u001b[0m 209 loss: \u001b[31m4.6376804675674066e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.5390e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.1640869312686846e-05)\u001b[0m\n",
      "Learning Rate 0.0024136250000000004\n",
      "\u001b[31mEpoch:\u001b[0m 210 loss: \u001b[31m5.303119178279303e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.5390e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.9698e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.3256095775868744e-05)\u001b[0m\n",
      "Learning Rate 0.0024012499999999997\n",
      "\u001b[31mEpoch:\u001b[0m 211 loss: \u001b[31m5.305229205987416e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.5390e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8496e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.8495620426838286e-05)\u001b[0m\n",
      "Learning Rate 0.002388875\n",
      "\u001b[31mEpoch:\u001b[0m 212 loss: \u001b[31m4.94668165629264e-05\u001b[0m\n",
      "Best Loss 4.267595068085939e-05\n",
      "Best Batch Loss tensor(1.5390e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8496e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.1754574592923746e-05)\u001b[0m\n",
      "Learning Rate 0.0023765\n",
      "\u001b[31mEpoch:\u001b[0m 213 loss: \u001b[31m3.820361234829761e-05\u001b[0m\n",
      "Best Loss 3.820361234829761e-05\n",
      "Best Batch Loss tensor(1.5390e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8496e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.164071287959814e-05)\u001b[0m\n",
      "Learning Rate 0.0023641250000000003\n",
      "\u001b[31mEpoch:\u001b[0m 214 loss: \u001b[31m6.440112338168547e-05\u001b[0m\n",
      "Best Loss 3.820361234829761e-05\n",
      "Best Batch Loss tensor(1.5390e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8496e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.8393955037463456e-05)\u001b[0m\n",
      "Learning Rate 0.0023517499999999997\n",
      "\u001b[31mEpoch:\u001b[0m 215 loss: \u001b[31m4.1664388845674694e-05\u001b[0m\n",
      "Best Loss 3.820361234829761e-05\n",
      "Best Batch Loss tensor(1.5390e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8496e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 6.188947736518458e-05)\u001b[0m\n",
      "Learning Rate 0.0023393750000000003\n",
      "\u001b[31mEpoch:\u001b[0m 216 loss: \u001b[31m6.02399195486214e-05\u001b[0m\n",
      "Best Loss 3.820361234829761e-05\n",
      "Best Batch Loss tensor(1.5390e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8496e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.790831735590473e-05)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.002327\n",
      "\u001b[31mEpoch:\u001b[0m 217 loss: \u001b[31m3.5873239539796486e-05\u001b[0m\n",
      "Best Loss 3.5873239539796486e-05\n",
      "Best Batch Loss tensor(1.5390e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8496e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.3702315451810136e-05)\u001b[0m\n",
      "Learning Rate 0.002314625\n",
      "\u001b[31mEpoch:\u001b[0m 218 loss: \u001b[31m4.0604572859592736e-05\u001b[0m\n",
      "Best Loss 3.5873239539796486e-05\n",
      "Best Batch Loss tensor(1.5390e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8496e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.612076372723095e-05)\u001b[0m\n",
      "Learning Rate 0.0023022499999999996\n",
      "\u001b[31mEpoch:\u001b[0m 219 loss: \u001b[31m4.351396637503058e-05\u001b[0m\n",
      "Best Loss 3.5873239539796486e-05\n",
      "Best Batch Loss tensor(1.5390e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8496e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.883700967184268e-05)\u001b[0m\n",
      "Learning Rate 0.0022898750000000002\n",
      "\u001b[31mEpoch:\u001b[0m 220 loss: \u001b[31m5.391260128817521e-05\u001b[0m\n",
      "Best Loss 3.5873239539796486e-05\n",
      "Best Batch Loss tensor(1.5390e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8496e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.645805529435165e-05)\u001b[0m\n",
      "Learning Rate 0.0022775\n",
      "\u001b[31mEpoch:\u001b[0m 221 loss: \u001b[31m3.6065815947949886e-05\u001b[0m\n",
      "Best Loss 3.5873239539796486e-05\n",
      "Best Batch Loss tensor(1.5212e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8207e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.8207416107761674e-05)\u001b[0m\n",
      "Learning Rate 0.0022651249999999998\n",
      "\u001b[31mEpoch:\u001b[0m 222 loss: \u001b[31m4.55771223641932e-05\u001b[0m\n",
      "Best Loss 3.5873239539796486e-05\n",
      "Best Batch Loss tensor(1.5212e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8207e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.456417834968306e-05)\u001b[0m\n",
      "Learning Rate 0.00225275\n",
      "\u001b[31mEpoch:\u001b[0m 223 loss: \u001b[31m3.682012902572751e-05\u001b[0m\n",
      "Best Loss 3.5873239539796486e-05\n",
      "Best Batch Loss tensor(1.5212e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8207e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.5565098591614515e-05)\u001b[0m\n",
      "Learning Rate 0.002240375\n",
      "\u001b[31mEpoch:\u001b[0m 224 loss: \u001b[31m5.122425136505626e-05\u001b[0m\n",
      "Best Loss 3.5873239539796486e-05\n",
      "Best Batch Loss tensor(1.5212e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8207e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.523833194980398e-05)\u001b[0m\n",
      "Learning Rate 0.0022279999999999995\n",
      "\u001b[31mEpoch:\u001b[0m 225 loss: \u001b[31m3.740260945050977e-05\u001b[0m\n",
      "Best Loss 3.5873239539796486e-05\n",
      "Best Batch Loss tensor(1.5212e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8207e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.2863124943105504e-05)\u001b[0m\n",
      "Learning Rate 0.002215625\n",
      "\u001b[31mEpoch:\u001b[0m 226 loss: \u001b[31m4.014407750219107e-05\u001b[0m\n",
      "Best Loss 3.5873239539796486e-05\n",
      "Best Batch Loss tensor(1.5212e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8207e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.592779830796644e-05)\u001b[0m\n",
      "Learning Rate 0.0022032500000000003\n",
      "\u001b[31mEpoch:\u001b[0m 227 loss: \u001b[31m4.0964634536067024e-05\u001b[0m\n",
      "Best Loss 3.5873239539796486e-05\n",
      "Best Batch Loss tensor(1.5212e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8207e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.342529766494408e-05)\u001b[0m\n",
      "Learning Rate 0.002190875\n",
      "\u001b[31mEpoch:\u001b[0m 228 loss: \u001b[31m4.245100717525929e-05\u001b[0m\n",
      "Best Loss 3.5873239539796486e-05\n",
      "Best Batch Loss tensor(1.5212e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8207e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.046778460382484e-05)\u001b[0m\n",
      "Learning Rate 0.0021785000000000003\n",
      "\u001b[31mEpoch:\u001b[0m 229 loss: \u001b[31m5.0328140787314624e-05\u001b[0m\n",
      "Best Loss 3.5873239539796486e-05\n",
      "Best Batch Loss tensor(1.5212e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8207e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.695590567076579e-05)\u001b[0m\n",
      "Learning Rate 0.002166125\n",
      "\u001b[31mEpoch:\u001b[0m 230 loss: \u001b[31m3.44588806910906e-05\u001b[0m\n",
      "Best Loss 3.44588806910906e-05\n",
      "Best Batch Loss tensor(1.5212e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8207e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.131675475742668e-05)\u001b[0m\n",
      "Learning Rate 0.0021537500000000003\n",
      "\u001b[31mEpoch:\u001b[0m 231 loss: \u001b[31m3.6061501305084676e-05\u001b[0m\n",
      "Best Loss 3.44588806910906e-05\n",
      "Best Batch Loss tensor(1.5212e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8207e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.89912020182237e-05)\u001b[0m\n",
      "Learning Rate 0.0021413749999999996\n",
      "\u001b[31mEpoch:\u001b[0m 232 loss: \u001b[31m4.831303522223607e-05\u001b[0m\n",
      "Best Loss 3.44588806910906e-05\n",
      "Best Batch Loss tensor(1.5212e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8207e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.3197778975591063e-05)\u001b[0m\n",
      "Learning Rate 0.0021290000000000002\n",
      "\u001b[31mEpoch:\u001b[0m 233 loss: \u001b[31m3.779223334277049e-05\u001b[0m\n",
      "Best Loss 3.44588806910906e-05\n",
      "Best Batch Loss tensor(1.5212e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8207e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 8.38147389004007e-05)\u001b[0m\n",
      "Learning Rate 0.002116625\n",
      "\u001b[31mEpoch:\u001b[0m 234 loss: \u001b[31m4.218979302095249e-05\u001b[0m\n",
      "Best Loss 3.44588806910906e-05\n",
      "Best Batch Loss tensor(1.5212e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8207e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.3128693252801895e-05)\u001b[0m\n",
      "Learning Rate 0.0021042500000000002\n",
      "\u001b[31mEpoch:\u001b[0m 235 loss: \u001b[31m3.7989339034538716e-05\u001b[0m\n",
      "Best Loss 3.44588806910906e-05\n",
      "Best Batch Loss tensor(1.5212e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8207e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.0299130230559967e-05)\u001b[0m\n",
      "Learning Rate 0.002091875\n",
      "\u001b[31mEpoch:\u001b[0m 236 loss: \u001b[31m3.5999368265038356e-05\u001b[0m\n",
      "Best Loss 3.44588806910906e-05\n",
      "Best Batch Loss tensor(1.5212e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8207e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.3801712561398745e-05)\u001b[0m\n",
      "Learning Rate 0.0020795\n",
      "\u001b[31mEpoch:\u001b[0m 237 loss: \u001b[31m4.176023503532633e-05\u001b[0m\n",
      "Best Loss 3.44588806910906e-05\n",
      "Best Batch Loss tensor(1.5212e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8207e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.266591324470937e-05)\u001b[0m\n",
      "Learning Rate 0.002067125\n",
      "\u001b[31mEpoch:\u001b[0m 238 loss: \u001b[31m3.861723598674871e-05\u001b[0m\n",
      "Best Loss 3.44588806910906e-05\n",
      "Best Batch Loss tensor(1.5212e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.8207e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.455091791693121e-05)\u001b[0m\n",
      "Learning Rate 0.00205475\n",
      "\u001b[31mEpoch:\u001b[0m 239 loss: \u001b[31m3.746993388631381e-05\u001b[0m\n",
      "Best Loss 3.44588806910906e-05\n",
      "Best Batch Loss tensor(1.5212e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.6383e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.6382891519460827e-05)\u001b[0m\n",
      "Learning Rate 0.002042375\n",
      "\u001b[31mEpoch:\u001b[0m 240 loss: \u001b[31m4.100100704818033e-05\u001b[0m\n",
      "Best Loss 3.44588806910906e-05\n",
      "Best Batch Loss tensor(1.5212e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.6383e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.179421037202701e-05)\u001b[0m\n",
      "Learning Rate 0.00203\n",
      "\u001b[31mEpoch:\u001b[0m 241 loss: \u001b[31m3.565131191862747e-05\u001b[0m\n",
      "Best Loss 3.44588806910906e-05\n",
      "Best Batch Loss tensor(1.4902e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.6383e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.8399606890161522e-05)\u001b[0m\n",
      "Learning Rate 0.002017625\n",
      "\u001b[31mEpoch:\u001b[0m 242 loss: \u001b[31m3.183207081747241e-05\u001b[0m\n",
      "Best Loss 3.183207081747241e-05\n",
      "Best Batch Loss tensor(1.4902e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.6383e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.6597577743814327e-05)\u001b[0m\n",
      "Learning Rate 0.00200525\n",
      "\u001b[31mEpoch:\u001b[0m 243 loss: \u001b[31m3.740523607120849e-05\u001b[0m\n",
      "Best Loss 3.183207081747241e-05\n",
      "Best Batch Loss tensor(1.3982e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.6383e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.572741090669297e-05)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.001992875\n",
      "\u001b[31mEpoch:\u001b[0m 244 loss: \u001b[31m3.822493818006478e-05\u001b[0m\n",
      "Best Loss 3.183207081747241e-05\n",
      "Best Batch Loss tensor(1.3982e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.6383e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 7.354728586506099e-05)\u001b[0m\n",
      "Learning Rate 0.0019805\n",
      "\u001b[31mEpoch:\u001b[0m 245 loss: \u001b[31m3.88432017643936e-05\u001b[0m\n",
      "Best Loss 3.183207081747241e-05\n",
      "Best Batch Loss tensor(1.3982e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.6383e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 9.27077344385907e-05)\u001b[0m\n",
      "Learning Rate 0.001968125\n",
      "\u001b[31mEpoch:\u001b[0m 246 loss: \u001b[31m3.640643262770027e-05\u001b[0m\n",
      "Best Loss 3.183207081747241e-05\n",
      "Best Batch Loss tensor(1.3982e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.6383e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.7712956580216996e-05)\u001b[0m\n",
      "Learning Rate 0.00195575\n",
      "\u001b[31mEpoch:\u001b[0m 247 loss: \u001b[31m3.290615859441459e-05\u001b[0m\n",
      "Best Loss 3.183207081747241e-05\n",
      "Best Batch Loss tensor(1.3982e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.6383e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.050678969884757e-05)\u001b[0m\n",
      "Learning Rate 0.0019433749999999998\n",
      "\u001b[31mEpoch:\u001b[0m 248 loss: \u001b[31m3.623769953264855e-05\u001b[0m\n",
      "Best Loss 3.183207081747241e-05\n",
      "Best Batch Loss tensor(1.3982e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.6383e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.185807872796431e-05)\u001b[0m\n",
      "Learning Rate 0.001931\n",
      "\u001b[31mEpoch:\u001b[0m 249 loss: \u001b[31m3.095308056799695e-05\u001b[0m\n",
      "Best Loss 3.095308056799695e-05\n",
      "Best Batch Loss tensor(1.3982e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.6383e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.4924512369325384e-05)\u001b[0m\n",
      "Learning Rate 0.0019186249999999998\n",
      "\u001b[31mEpoch:\u001b[0m 250 loss: \u001b[31m4.232053106534295e-05\u001b[0m\n",
      "Best Loss 3.095308056799695e-05\n",
      "Best Batch Loss tensor(1.3982e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.6383e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.5551838158862665e-05)\u001b[0m\n",
      "Learning Rate 0.00190625\n",
      "\u001b[31mEpoch:\u001b[0m 251 loss: \u001b[31m3.7131208955543116e-05\u001b[0m\n",
      "Best Loss 3.095308056799695e-05\n",
      "Best Batch Loss tensor(1.3982e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.6383e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.868294879794121e-05)\u001b[0m\n",
      "Learning Rate 0.0018938750000000002\n",
      "\u001b[31mEpoch:\u001b[0m 252 loss: \u001b[31m3.27246161759831e-05\u001b[0m\n",
      "Best Loss 3.095308056799695e-05\n",
      "Best Batch Loss tensor(1.3982e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.2585e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.2584756152355112e-05)\u001b[0m\n",
      "Learning Rate 0.0018815000000000001\n",
      "\u001b[31mEpoch:\u001b[0m 253 loss: \u001b[31m3.2090869353851303e-05\u001b[0m\n",
      "Best Loss 3.095308056799695e-05\n",
      "Best Batch Loss tensor(1.3982e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.2585e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.54412586602848e-05)\u001b[0m\n",
      "Learning Rate 0.0018691250000000003\n",
      "\u001b[31mEpoch:\u001b[0m 254 loss: \u001b[31m3.443274545134045e-05\u001b[0m\n",
      "Best Loss 3.095308056799695e-05\n",
      "Best Batch Loss tensor(1.3982e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.2585e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.2988881028140895e-05)\u001b[0m\n",
      "Learning Rate 0.0018567500000000001\n",
      "\u001b[31mEpoch:\u001b[0m 255 loss: \u001b[31m3.057848152820952e-05\u001b[0m\n",
      "Best Loss 3.057848152820952e-05\n",
      "Best Batch Loss tensor(1.2902e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.2585e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.7023876100429334e-05)\u001b[0m\n",
      "Learning Rate 0.0018443750000000003\n",
      "\u001b[31mEpoch:\u001b[0m 256 loss: \u001b[31m3.701096284203231e-05\u001b[0m\n",
      "Best Loss 3.057848152820952e-05\n",
      "Best Batch Loss tensor(1.2902e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.2585e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.402800211915746e-05)\u001b[0m\n",
      "Learning Rate 0.0018319999999999999\n",
      "\u001b[31mEpoch:\u001b[0m 257 loss: \u001b[31m3.609958730521612e-05\u001b[0m\n",
      "Best Loss 3.057848152820952e-05\n",
      "Best Batch Loss tensor(1.2902e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.2585e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.332734195282683e-05)\u001b[0m\n",
      "Learning Rate 0.0018196250000000003\n",
      "\u001b[31mEpoch:\u001b[0m 258 loss: \u001b[31m3.256091440562159e-05\u001b[0m\n",
      "Best Loss 3.057848152820952e-05\n",
      "Best Batch Loss tensor(1.2902e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.2585e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.9624228520551696e-05)\u001b[0m\n",
      "Learning Rate 0.0018072499999999998\n",
      "\u001b[31mEpoch:\u001b[0m 259 loss: \u001b[31m3.5333541745785624e-05\u001b[0m\n",
      "Best Loss 3.057848152820952e-05\n",
      "Best Batch Loss tensor(1.2902e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.2585e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.5816952984314412e-05)\u001b[0m\n",
      "Learning Rate 0.0017948750000000002\n",
      "\u001b[31mEpoch:\u001b[0m 260 loss: \u001b[31m2.902166852436494e-05\u001b[0m\n",
      "Best Loss 2.902166852436494e-05\n",
      "Best Batch Loss tensor(1.2902e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.2585e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.625505632953718e-05)\u001b[0m\n",
      "Learning Rate 0.0017824999999999998\n",
      "\u001b[31mEpoch:\u001b[0m 261 loss: \u001b[31m3.055620982195251e-05\u001b[0m\n",
      "Best Loss 2.902166852436494e-05\n",
      "Best Batch Loss tensor(1.2902e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.2585e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.147437746520154e-05)\u001b[0m\n",
      "Learning Rate 0.0017701250000000002\n",
      "\u001b[31mEpoch:\u001b[0m 262 loss: \u001b[31m2.9418377380352467e-05\u001b[0m\n",
      "Best Loss 2.902166852436494e-05\n",
      "Best Batch Loss tensor(1.2902e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.2585e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.3409855202771723e-05)\u001b[0m\n",
      "Learning Rate 0.00175775\n",
      "\u001b[31mEpoch:\u001b[0m 263 loss: \u001b[31m2.9830984203726985e-05\u001b[0m\n",
      "Best Loss 2.902166852436494e-05\n",
      "Best Batch Loss tensor(1.2902e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.2585e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.3645097573753446e-05)\u001b[0m\n",
      "Learning Rate 0.0017453750000000002\n",
      "\u001b[31mEpoch:\u001b[0m 264 loss: \u001b[31m3.488241782179102e-05\u001b[0m\n",
      "Best Loss 2.902166852436494e-05\n",
      "Best Batch Loss tensor(1.2902e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.2585e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.320347605040297e-05)\u001b[0m\n",
      "Learning Rate 0.001733\n",
      "\u001b[31mEpoch:\u001b[0m 265 loss: \u001b[31m3.146026938338764e-05\u001b[0m\n",
      "Best Loss 2.902166852436494e-05\n",
      "Best Batch Loss tensor(1.2902e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.2585e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.795479617314413e-05)\u001b[0m\n",
      "Learning Rate 0.0017206250000000001\n",
      "\u001b[31mEpoch:\u001b[0m 266 loss: \u001b[31m3.601290882215835e-05\u001b[0m\n",
      "Best Loss 2.902166852436494e-05\n",
      "Best Batch Loss tensor(1.2274e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.2585e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.244437903049402e-05)\u001b[0m\n",
      "Learning Rate 0.00170825\n",
      "\u001b[31mEpoch:\u001b[0m 267 loss: \u001b[31m2.874417623388581e-05\u001b[0m\n",
      "Best Loss 2.874417623388581e-05\n",
      "Best Batch Loss tensor(1.2274e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.2503e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.2502950741909444e-05)\u001b[0m\n",
      "Learning Rate 0.0016958750000000001\n",
      "\u001b[31mEpoch:\u001b[0m 268 loss: \u001b[31m2.79147679975722e-05\u001b[0m\n",
      "Best Loss 2.79147679975722e-05\n",
      "Best Batch Loss tensor(1.2274e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.2503e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.3221902665682137e-05)\u001b[0m\n",
      "Learning Rate 0.0016834999999999997\n",
      "\u001b[31mEpoch:\u001b[0m 269 loss: \u001b[31m3.1030467653181404e-05\u001b[0m\n",
      "Best Loss 2.79147679975722e-05\n",
      "Best Batch Loss tensor(1.2274e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.2503e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.4260505597339943e-05)\u001b[0m\n",
      "Learning Rate 0.001671125\n",
      "\u001b[31mEpoch:\u001b[0m 270 loss: \u001b[31m2.704241887840908e-05\u001b[0m\n",
      "Best Loss 2.704241887840908e-05\n",
      "Best Batch Loss tensor(1.2274e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.2503e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.771187064354308e-05)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0016587499999999996\n",
      "\u001b[31mEpoch:\u001b[0m 271 loss: \u001b[31m2.8048008971381932e-05\u001b[0m\n",
      "Best Loss 2.704241887840908e-05\n",
      "Best Batch Loss tensor(1.2274e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.2503e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.453766112215817e-05)\u001b[0m\n",
      "Learning Rate 0.001646375\n",
      "\u001b[31mEpoch:\u001b[0m 272 loss: \u001b[31m2.8373755412758328e-05\u001b[0m\n",
      "Best Loss 2.704241887840908e-05\n",
      "Best Batch Loss tensor(1.2274e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.2503e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.5038580133696087e-05)\u001b[0m\n",
      "Learning Rate 0.0016339999999999998\n",
      "\u001b[31mEpoch:\u001b[0m 273 loss: \u001b[31m3.169446063111536e-05\u001b[0m\n",
      "Best Loss 2.704241887840908e-05\n",
      "Best Batch Loss tensor(1.2274e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.2503e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.7396486732177436e-05)\u001b[0m\n",
      "Learning Rate 0.0016216250000000002\n",
      "\u001b[31mEpoch:\u001b[0m 274 loss: \u001b[31m3.259752702433616e-05\u001b[0m\n",
      "Best Loss 2.704241887840908e-05\n",
      "Best Batch Loss tensor(1.2274e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(2.2503e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.110062582185492e-05)\u001b[0m\n",
      "Learning Rate 0.0016092499999999998\n",
      "\u001b[31mEpoch:\u001b[0m 275 loss: \u001b[31m2.9439295758493245e-05\u001b[0m\n",
      "Best Loss 2.704241887840908e-05\n",
      "Best Batch Loss tensor(1.2274e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.8695e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.869543848442845e-05)\u001b[0m\n",
      "Learning Rate 0.001596875\n",
      "\u001b[31mEpoch:\u001b[0m 276 loss: \u001b[31m2.550882527430076e-05\u001b[0m\n",
      "Best Loss 2.550882527430076e-05\n",
      "Best Batch Loss tensor(1.2274e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.8695e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.014089790871367e-05)\u001b[0m\n",
      "Learning Rate 0.0015845000000000004\n",
      "\u001b[31mEpoch:\u001b[0m 277 loss: \u001b[31m2.827072785294149e-05\u001b[0m\n",
      "Best Loss 2.550882527430076e-05\n",
      "Best Batch Loss tensor(1.2274e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.8695e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.1797908630105667e-05)\u001b[0m\n",
      "Learning Rate 0.001572125\n",
      "\u001b[31mEpoch:\u001b[0m 278 loss: \u001b[31m2.693330498004798e-05\u001b[0m\n",
      "Best Loss 2.550882527430076e-05\n",
      "Best Batch Loss tensor(1.2274e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.6920e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.692038495093584e-05)\u001b[0m\n",
      "Learning Rate 0.0015597500000000004\n",
      "\u001b[31mEpoch:\u001b[0m 279 loss: \u001b[31m2.8424396077753045e-05\u001b[0m\n",
      "Best Loss 2.550882527430076e-05\n",
      "Best Batch Loss tensor(1.2274e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.6920e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 5.557843542192131e-05)\u001b[0m\n",
      "Learning Rate 0.001547375\n",
      "\u001b[31mEpoch:\u001b[0m 280 loss: \u001b[31m3.168331750202924e-05\u001b[0m\n",
      "Best Loss 2.550882527430076e-05\n",
      "Best Batch Loss tensor(1.2274e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.6920e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.3956303266459145e-05)\u001b[0m\n",
      "Learning Rate 0.0015350000000000003\n",
      "\u001b[31mEpoch:\u001b[0m 281 loss: \u001b[31m2.577369741629809e-05\u001b[0m\n",
      "Best Loss 2.550882527430076e-05\n",
      "Best Batch Loss tensor(1.2274e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.6920e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.0776460587512702e-05)\u001b[0m\n",
      "Learning Rate 0.0015226249999999999\n",
      "\u001b[31mEpoch:\u001b[0m 282 loss: \u001b[31m2.9880980946472846e-05\u001b[0m\n",
      "Best Loss 2.550882527430076e-05\n",
      "Best Batch Loss tensor(1.2274e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.6920e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.290366268833168e-05)\u001b[0m\n",
      "Learning Rate 0.00151025\n",
      "\u001b[31mEpoch:\u001b[0m 283 loss: \u001b[31m2.445834979880601e-05\u001b[0m\n",
      "Best Loss 2.445834979880601e-05\n",
      "Best Batch Loss tensor(1.1467e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.6920e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.8307696993579157e-05)\u001b[0m\n",
      "Learning Rate 0.001497875\n",
      "\u001b[31mEpoch:\u001b[0m 284 loss: \u001b[31m2.4946832127170637e-05\u001b[0m\n",
      "Best Loss 2.445834979880601e-05\n",
      "Best Batch Loss tensor(1.1451e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.6920e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.07753034576308e-05)\u001b[0m\n",
      "Learning Rate 0.0014855\n",
      "\u001b[31mEpoch:\u001b[0m 285 loss: \u001b[31m3.238516001147218e-05\u001b[0m\n",
      "Best Loss 2.445834979880601e-05\n",
      "Best Batch Loss tensor(1.1451e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.6920e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.4594273781985976e-05)\u001b[0m\n",
      "Learning Rate 0.001473125\n",
      "\u001b[31mEpoch:\u001b[0m 286 loss: \u001b[31m2.7159174351254478e-05\u001b[0m\n",
      "Best Loss 2.445834979880601e-05\n",
      "Best Batch Loss tensor(1.1451e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.6920e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.4222877729916945e-05)\u001b[0m\n",
      "Learning Rate 0.0014607500000000002\n",
      "\u001b[31mEpoch:\u001b[0m 287 loss: \u001b[31m2.516022686904762e-05\u001b[0m\n",
      "Best Loss 2.445834979880601e-05\n",
      "Best Batch Loss tensor(1.1451e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.6920e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.1157813534955494e-05)\u001b[0m\n",
      "Learning Rate 0.001448375\n",
      "\u001b[31mEpoch:\u001b[0m 288 loss: \u001b[31m2.519500776543282e-05\u001b[0m\n",
      "Best Loss 2.445834979880601e-05\n",
      "Best Batch Loss tensor(1.1451e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.6920e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 4.8060592234833166e-05)\u001b[0m\n",
      "Learning Rate 0.0014360000000000002\n",
      "\u001b[31mEpoch:\u001b[0m 289 loss: \u001b[31m2.594866782601457e-05\u001b[0m\n",
      "Best Loss 2.445834979880601e-05\n",
      "Best Batch Loss tensor(1.1451e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.6920e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.620659117586911e-05)\u001b[0m\n",
      "Learning Rate 0.0014236249999999998\n",
      "\u001b[31mEpoch:\u001b[0m 290 loss: \u001b[31m2.3311466065933928e-05\u001b[0m\n",
      "Best Loss 2.3311466065933928e-05\n",
      "Best Batch Loss tensor(1.1451e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.6920e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.6342437195125967e-05)\u001b[0m\n",
      "Learning Rate 0.0014112500000000002\n",
      "\u001b[31mEpoch:\u001b[0m 291 loss: \u001b[31m2.3782846255926415e-05\u001b[0m\n",
      "Best Loss 2.3311466065933928e-05\n",
      "Best Batch Loss tensor(1.1451e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.4235463822842576e-05)\u001b[0m\n",
      "Learning Rate 0.0013988749999999997\n",
      "\u001b[31mEpoch:\u001b[0m 292 loss: \u001b[31m2.8427193683455698e-05\u001b[0m\n",
      "Best Loss 2.3311466065933928e-05\n",
      "Best Batch Loss tensor(1.1451e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.0129111362621188e-05)\u001b[0m\n",
      "Learning Rate 0.0013865000000000001\n",
      "\u001b[31mEpoch:\u001b[0m 293 loss: \u001b[31m2.4240591301349923e-05\u001b[0m\n",
      "Best Loss 2.3311466065933928e-05\n",
      "Best Batch Loss tensor(1.1451e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.1260979337967e-05)\u001b[0m\n",
      "Learning Rate 0.001374125\n",
      "\u001b[31mEpoch:\u001b[0m 294 loss: \u001b[31m2.430626227578614e-05\u001b[0m\n",
      "Best Loss 2.3311466065933928e-05\n",
      "Best Batch Loss tensor(1.1451e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.5961098799598403e-05)\u001b[0m\n",
      "Learning Rate 0.0013617500000000001\n",
      "\u001b[31mEpoch:\u001b[0m 295 loss: \u001b[31m2.2990847355686128e-05\u001b[0m\n",
      "Best Loss 2.2990847355686128e-05\n",
      "Best Batch Loss tensor(1.1451e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.98844718397595e-05)\u001b[0m\n",
      "Learning Rate 0.0013493749999999999\n",
      "\u001b[31mEpoch:\u001b[0m 296 loss: \u001b[31m2.3076356228557415e-05\u001b[0m\n",
      "Best Loss 2.2990847355686128e-05\n",
      "Best Batch Loss tensor(1.0399e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.368256900808774e-05)\u001b[0m\n",
      "Learning Rate 0.001337\n",
      "\u001b[31mEpoch:\u001b[0m 297 loss: \u001b[31m2.3880631488282233e-05\u001b[0m\n",
      "Best Loss 2.2990847355686128e-05\n",
      "Best Batch Loss tensor(1.0399e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.683502134459559e-05)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0013246249999999998\n",
      "\u001b[31mEpoch:\u001b[0m 298 loss: \u001b[31m2.2695554434903897e-05\u001b[0m\n",
      "Best Loss 2.2695554434903897e-05\n",
      "Best Batch Loss tensor(1.0399e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.936527380370535e-05)\u001b[0m\n",
      "Learning Rate 0.00131225\n",
      "\u001b[31mEpoch:\u001b[0m 299 loss: \u001b[31m2.6963791242451407e-05\u001b[0m\n",
      "Best Loss 2.2695554434903897e-05\n",
      "Best Batch Loss tensor(1.0399e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.05326359719038e-05)\u001b[0m\n",
      "Learning Rate 0.0012998749999999998\n",
      "\u001b[31mEpoch:\u001b[0m 300 loss: \u001b[31m2.2879761672811583e-05\u001b[0m\n",
      "Best Loss 2.2695554434903897e-05\n",
      "Best Batch Loss tensor(1.0399e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.6555266483919695e-05)\u001b[0m\n",
      "Learning Rate 0.0012875\n",
      "\u001b[31mEpoch:\u001b[0m 301 loss: \u001b[31m2.250895522593055e-05\u001b[0m\n",
      "Best Loss 2.250895522593055e-05\n",
      "Best Batch Loss tensor(1.0399e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.93887699424522e-05)\u001b[0m\n",
      "Learning Rate 0.0012751250000000002\n",
      "\u001b[31mEpoch:\u001b[0m 302 loss: \u001b[31m2.3762253476888873e-05\u001b[0m\n",
      "Best Loss 2.250895522593055e-05\n",
      "Best Batch Loss tensor(1.0399e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.8084848963771947e-05)\u001b[0m\n",
      "Learning Rate 0.00126275\n",
      "\u001b[31mEpoch:\u001b[0m 303 loss: \u001b[31m2.2499560145661235e-05\u001b[0m\n",
      "Best Loss 2.2499560145661235e-05\n",
      "Best Batch Loss tensor(1.0399e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.8814474717364646e-05)\u001b[0m\n",
      "Learning Rate 0.0012503750000000002\n",
      "\u001b[31mEpoch:\u001b[0m 304 loss: \u001b[31m2.4124759875121526e-05\u001b[0m\n",
      "Best Loss 2.2499560145661235e-05\n",
      "Best Batch Loss tensor(1.0102e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.2730135242454708e-05)\u001b[0m\n",
      "Learning Rate 0.001238\n",
      "\u001b[31mEpoch:\u001b[0m 305 loss: \u001b[31m2.338797457923647e-05\u001b[0m\n",
      "Best Loss 2.2499560145661235e-05\n",
      "Best Batch Loss tensor(1.0102e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.377458440605551e-05)\u001b[0m\n",
      "Learning Rate 0.0012256250000000002\n",
      "\u001b[31mEpoch:\u001b[0m 306 loss: \u001b[31m2.16294592974009e-05\u001b[0m\n",
      "Best Loss 2.16294592974009e-05\n",
      "Best Batch Loss tensor(1.0102e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.8956137864734046e-05)\u001b[0m\n",
      "Learning Rate 0.0012132500000000001\n",
      "\u001b[31mEpoch:\u001b[0m 307 loss: \u001b[31m2.4309892978635617e-05\u001b[0m\n",
      "Best Loss 2.16294592974009e-05\n",
      "Best Batch Loss tensor(1.0102e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.276127452205401e-05)\u001b[0m\n",
      "Learning Rate 0.0012008750000000001\n",
      "\u001b[31mEpoch:\u001b[0m 308 loss: \u001b[31m1.9872824850608595e-05\u001b[0m\n",
      "Best Loss 1.9872824850608595e-05\n",
      "Best Batch Loss tensor(1.0102e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.737074853735976e-05)\u001b[0m\n",
      "Learning Rate 0.0011884999999999999\n",
      "\u001b[31mEpoch:\u001b[0m 309 loss: \u001b[31m2.051629599009175e-05\u001b[0m\n",
      "Best Loss 1.9872824850608595e-05\n",
      "Best Batch Loss tensor(1.0102e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.099919402098749e-05)\u001b[0m\n",
      "Learning Rate 0.0011761250000000003\n",
      "\u001b[31mEpoch:\u001b[0m 310 loss: \u001b[31m2.262832276755944e-05\u001b[0m\n",
      "Best Loss 1.9872824850608595e-05\n",
      "Best Batch Loss tensor(1.0102e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.8517539249151014e-05)\u001b[0m\n",
      "Learning Rate 0.0011637499999999999\n",
      "\u001b[31mEpoch:\u001b[0m 311 loss: \u001b[31m2.4973807740025222e-05\u001b[0m\n",
      "Best Loss 1.9872824850608595e-05\n",
      "Best Batch Loss tensor(1.0102e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.0084858988411725e-05)\u001b[0m\n",
      "Learning Rate 0.0011513750000000003\n",
      "\u001b[31mEpoch:\u001b[0m 312 loss: \u001b[31m1.9108909327769652e-05\u001b[0m\n",
      "Best Loss 1.9108909327769652e-05\n",
      "Best Batch Loss tensor(1.0102e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 3.45737898896914e-05)\u001b[0m\n",
      "Learning Rate 0.0011389999999999998\n",
      "\u001b[31mEpoch:\u001b[0m 313 loss: \u001b[31m2.0998248146497644e-05\u001b[0m\n",
      "Best Loss 1.9108909327769652e-05\n",
      "Best Batch Loss tensor(1.0102e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.7757582099875435e-05)\u001b[0m\n",
      "Learning Rate 0.0011266250000000002\n",
      "\u001b[31mEpoch:\u001b[0m 314 loss: \u001b[31m2.3100321413949132e-05\u001b[0m\n",
      "Best Loss 1.9108909327769652e-05\n",
      "Best Batch Loss tensor(1.0102e-05, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.3048547518556006e-05)\u001b[0m\n",
      "Learning Rate 0.0011142499999999998\n",
      "\u001b[31mEpoch:\u001b[0m 315 loss: \u001b[31m2.1310781448846683e-05\u001b[0m\n",
      "Best Loss 1.9108909327769652e-05\n",
      "Best Batch Loss tensor(9.9579e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.0668909201049246e-05)\u001b[0m\n",
      "Learning Rate 0.001101875\n",
      "\u001b[31mEpoch:\u001b[0m 316 loss: \u001b[31m2.0143346773693338e-05\u001b[0m\n",
      "Best Loss 1.9108909327769652e-05\n",
      "Best Batch Loss tensor(9.9579e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.9288010662421584e-05)\u001b[0m\n",
      "Learning Rate 0.0010895\n",
      "\u001b[31mEpoch:\u001b[0m 317 loss: \u001b[31m1.8082095266436227e-05\u001b[0m\n",
      "Best Loss 1.8082095266436227e-05\n",
      "Best Batch Loss tensor(9.9579e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.6097091904375702e-05)\u001b[0m\n",
      "Learning Rate 0.001077125\n",
      "\u001b[31mEpoch:\u001b[0m 318 loss: \u001b[31m1.9877277736668475e-05\u001b[0m\n",
      "Best Loss 1.8082095266436227e-05\n",
      "Best Batch Loss tensor(9.9579e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.1699243006878532e-05)\u001b[0m\n",
      "Learning Rate 0.00106475\n",
      "\u001b[31mEpoch:\u001b[0m 319 loss: \u001b[31m2.1290761651471257e-05\u001b[0m\n",
      "Best Loss 1.8082095266436227e-05\n",
      "Best Batch Loss tensor(9.9168e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.554217487864662e-05)\u001b[0m\n",
      "Learning Rate 0.0010523750000000001\n",
      "\u001b[31mEpoch:\u001b[0m 320 loss: \u001b[31m1.7707769075059332e-05\u001b[0m\n",
      "Best Loss 1.7707769075059332e-05\n",
      "Best Batch Loss tensor(9.9168e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4235e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.9615401470218785e-05)\u001b[0m\n",
      "Learning Rate 0.00104\n",
      "\u001b[31mEpoch:\u001b[0m 321 loss: \u001b[31m2.2642629119218327e-05\u001b[0m\n",
      "Best Loss 1.7707769075059332e-05\n",
      "Best Batch Loss tensor(9.9168e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4158e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.4157782970869448e-05)\u001b[0m\n",
      "Learning Rate 0.0010276250000000001\n",
      "\u001b[31mEpoch:\u001b[0m 322 loss: \u001b[31m1.954497020051349e-05\u001b[0m\n",
      "Best Loss 1.7707769075059332e-05\n",
      "Best Batch Loss tensor(9.9168e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4158e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.3092998162610456e-05)\u001b[0m\n",
      "Learning Rate 0.0010152499999999997\n",
      "\u001b[31mEpoch:\u001b[0m 323 loss: \u001b[31m1.8683918824535795e-05\u001b[0m\n",
      "Best Loss 1.7707769075059332e-05\n",
      "Best Batch Loss tensor(9.6930e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4158e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.6268062609015033e-05)\u001b[0m\n",
      "Learning Rate 0.001002875\n",
      "\u001b[31mEpoch:\u001b[0m 324 loss: \u001b[31m1.8035105313174427e-05\u001b[0m\n",
      "Best Loss 1.7707769075059332e-05\n",
      "Best Batch Loss tensor(9.6930e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4158e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.085970299958717e-05)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0009904999999999998\n",
      "\u001b[31mEpoch:\u001b[0m 325 loss: \u001b[31m1.8009830455412157e-05\u001b[0m\n",
      "Best Loss 1.7707769075059332e-05\n",
      "Best Batch Loss tensor(9.4125e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4158e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.9765288016060367e-05)\u001b[0m\n",
      "Learning Rate 0.000978125\n",
      "\u001b[31mEpoch:\u001b[0m 326 loss: \u001b[31m1.769687150954269e-05\u001b[0m\n",
      "Best Loss 1.769687150954269e-05\n",
      "Best Batch Loss tensor(9.4125e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4158e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.6535848772036843e-05)\u001b[0m\n",
      "Learning Rate 0.0009657500000000002\n",
      "\u001b[31mEpoch:\u001b[0m 327 loss: \u001b[31m1.7952816051547416e-05\u001b[0m\n",
      "Best Loss 1.769687150954269e-05\n",
      "Best Batch Loss tensor(9.0946e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4158e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.7627848137635738e-05)\u001b[0m\n",
      "Learning Rate 0.000953375\n",
      "\u001b[31mEpoch:\u001b[0m 328 loss: \u001b[31m1.9341705410624854e-05\u001b[0m\n",
      "Best Loss 1.769687150954269e-05\n",
      "Best Batch Loss tensor(9.0946e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4158e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.059238795482088e-05)\u001b[0m\n",
      "Learning Rate 0.0009410000000000003\n",
      "\u001b[31mEpoch:\u001b[0m 329 loss: \u001b[31m1.9109671484329738e-05\u001b[0m\n",
      "Best Loss 1.769687150954269e-05\n",
      "Best Batch Loss tensor(9.0946e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4158e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.716484439384658e-05)\u001b[0m\n",
      "Learning Rate 0.000928625\n",
      "\u001b[31mEpoch:\u001b[0m 330 loss: \u001b[31m1.7325506632914767e-05\u001b[0m\n",
      "Best Loss 1.7325506632914767e-05\n",
      "Best Batch Loss tensor(9.0946e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4158e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.4932992598915007e-05)\u001b[0m\n",
      "Learning Rate 0.0009162500000000002\n",
      "\u001b[31mEpoch:\u001b[0m 331 loss: \u001b[31m1.813899325497914e-05\u001b[0m\n",
      "Best Loss 1.7325506632914767e-05\n",
      "Best Batch Loss tensor(9.0946e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4158e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.634664658922702e-05)\u001b[0m\n",
      "Learning Rate 0.000903875\n",
      "\u001b[31mEpoch:\u001b[0m 332 loss: \u001b[31m1.9498864276101813e-05\u001b[0m\n",
      "Best Loss 1.7325506632914767e-05\n",
      "Best Batch Loss tensor(9.0946e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4158e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.567929211887531e-05)\u001b[0m\n",
      "Learning Rate 0.0008915000000000001\n",
      "\u001b[31mEpoch:\u001b[0m 333 loss: \u001b[31m1.7322350686299615e-05\u001b[0m\n",
      "Best Loss 1.7322350686299615e-05\n",
      "Best Batch Loss tensor(9.0946e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4158e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.8834984075510874e-05)\u001b[0m\n",
      "Learning Rate 0.0008791249999999999\n",
      "\u001b[31mEpoch:\u001b[0m 334 loss: \u001b[31m1.6476764358230866e-05\u001b[0m\n",
      "Best Loss 1.6476764358230866e-05\n",
      "Best Batch Loss tensor(9.0946e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4158e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.8849779735319316e-05)\u001b[0m\n",
      "Learning Rate 0.0008667500000000002\n",
      "\u001b[31mEpoch:\u001b[0m 335 loss: \u001b[31m1.8083972463500686e-05\u001b[0m\n",
      "Best Loss 1.6476764358230866e-05\n",
      "Best Batch Loss tensor(8.9895e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4158e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.4309517609945033e-05)\u001b[0m\n",
      "Learning Rate 0.0008543749999999999\n",
      "\u001b[31mEpoch:\u001b[0m 336 loss: \u001b[31m1.6548063285881653e-05\u001b[0m\n",
      "Best Loss 1.6476764358230866e-05\n",
      "Best Batch Loss tensor(8.9895e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.4158e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.4333380022435449e-05)\u001b[0m\n",
      "Learning Rate 0.0008420000000000002\n",
      "\u001b[31mEpoch:\u001b[0m 337 loss: \u001b[31m1.6766838598414324e-05\u001b[0m\n",
      "Best Loss 1.6476764358230866e-05\n",
      "Best Batch Loss tensor(8.5473e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.3400e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.3399588169704657e-05)\u001b[0m\n",
      "Learning Rate 0.0008296249999999999\n",
      "\u001b[31mEpoch:\u001b[0m 338 loss: \u001b[31m1.617443558643572e-05\u001b[0m\n",
      "Best Loss 1.617443558643572e-05\n",
      "Best Batch Loss tensor(8.5473e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.3400e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.866415550466627e-05)\u001b[0m\n",
      "Learning Rate 0.0008172500000000002\n",
      "\u001b[31mEpoch:\u001b[0m 339 loss: \u001b[31m1.5507715943385847e-05\u001b[0m\n",
      "Best Loss 1.5507715943385847e-05\n",
      "Best Batch Loss tensor(8.5473e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.3400e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.6315125321852975e-05)\u001b[0m\n",
      "Learning Rate 0.0008048749999999998\n",
      "\u001b[31mEpoch:\u001b[0m 340 loss: \u001b[31m1.8047963749268092e-05\u001b[0m\n",
      "Best Loss 1.5507715943385847e-05\n",
      "Best Batch Loss tensor(8.5473e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.3400e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.7256965293199755e-05)\u001b[0m\n",
      "Learning Rate 0.0007925000000000001\n",
      "\u001b[31mEpoch:\u001b[0m 341 loss: \u001b[31m1.5820904081920162e-05\u001b[0m\n",
      "Best Loss 1.5507715943385847e-05\n",
      "Best Batch Loss tensor(8.5473e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.3400e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.644339681661222e-05)\u001b[0m\n",
      "Learning Rate 0.0007801249999999999\n",
      "\u001b[31mEpoch:\u001b[0m 342 loss: \u001b[31m1.557696123200003e-05\u001b[0m\n",
      "Best Loss 1.5507715943385847e-05\n",
      "Best Batch Loss tensor(8.5473e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.3400e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.749644045252353e-05)\u001b[0m\n",
      "Learning Rate 0.0007677500000000001\n",
      "\u001b[31mEpoch:\u001b[0m 343 loss: \u001b[31m1.5616187738487497e-05\u001b[0m\n",
      "Best Loss 1.5507715943385847e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.3400e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.464646902604727e-05)\u001b[0m\n",
      "Learning Rate 0.0007553749999999999\n",
      "\u001b[31mEpoch:\u001b[0m 344 loss: \u001b[31m1.6024605429265648e-05\u001b[0m\n",
      "Best Loss 1.5507715943385847e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.3400e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.535309274913743e-05)\u001b[0m\n",
      "Learning Rate 0.0007430000000000001\n",
      "\u001b[31mEpoch:\u001b[0m 345 loss: \u001b[31m1.6393765690736473e-05\u001b[0m\n",
      "Best Loss 1.5507715943385847e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.3400e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.5616782548022456e-05)\u001b[0m\n",
      "Learning Rate 0.0007306249999999997\n",
      "\u001b[31mEpoch:\u001b[0m 346 loss: \u001b[31m1.6628209777991287e-05\u001b[0m\n",
      "Best Loss 1.5507715943385847e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.3400e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.7060110621969216e-05)\u001b[0m\n",
      "Learning Rate 0.00071825\n",
      "\u001b[31mEpoch:\u001b[0m 347 loss: \u001b[31m1.5165585864451714e-05\u001b[0m\n",
      "Best Loss 1.5165585864451714e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.3400e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.3992142157803755e-05)\u001b[0m\n",
      "Learning Rate 0.0007058749999999998\n",
      "\u001b[31mEpoch:\u001b[0m 348 loss: \u001b[31m1.559925112815108e-05\u001b[0m\n",
      "Best Loss 1.5165585864451714e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.3400e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.349420017504599e-05)\u001b[0m\n",
      "Learning Rate 0.0006935000000000001\n",
      "\u001b[31mEpoch:\u001b[0m 349 loss: \u001b[31m1.54609078890644e-05\u001b[0m\n",
      "Best Loss 1.5165585864451714e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.3400e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.4004306649439968e-05)\u001b[0m\n",
      "Learning Rate 0.0006811249999999998\n",
      "\u001b[31mEpoch:\u001b[0m 350 loss: \u001b[31m1.4004862350702751e-05\u001b[0m\n",
      "Best Loss 1.4004862350702751e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.3400e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.3662736819242127e-05)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.00066875\n",
      "\u001b[31mEpoch:\u001b[0m 351 loss: \u001b[31m1.491517286922317e-05\u001b[0m\n",
      "Best Loss 1.4004862350702751e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.3400e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.557225732540246e-05)\u001b[0m\n",
      "Learning Rate 0.0006563750000000003\n",
      "\u001b[31mEpoch:\u001b[0m 352 loss: \u001b[31m1.4641037523688283e-05\u001b[0m\n",
      "Best Loss 1.4004862350702751e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.3400e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.515642179583665e-05)\u001b[0m\n",
      "Learning Rate 0.0006439999999999999\n",
      "\u001b[31mEpoch:\u001b[0m 353 loss: \u001b[31m1.3697340364160482e-05\u001b[0m\n",
      "Best Loss 1.3697340364160482e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.3400e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.3402387594396714e-05)\u001b[0m\n",
      "Learning Rate 0.0006316250000000002\n",
      "\u001b[31mEpoch:\u001b[0m 354 loss: \u001b[31m1.4212499991117511e-05\u001b[0m\n",
      "Best Loss 1.3697340364160482e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.3185e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.3185485840949696e-05)\u001b[0m\n",
      "Learning Rate 0.00061925\n",
      "\u001b[31mEpoch:\u001b[0m 355 loss: \u001b[31m1.546004568808712e-05\u001b[0m\n",
      "Best Loss 1.3697340364160482e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.3185e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 2.680779289221391e-05)\u001b[0m\n",
      "Learning Rate 0.0006068750000000003\n",
      "\u001b[31mEpoch:\u001b[0m 356 loss: \u001b[31m1.4700279280077666e-05\u001b[0m\n",
      "Best Loss 1.3697340364160482e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.3185e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.6134215911733918e-05)\u001b[0m\n",
      "Learning Rate 0.0005945\n",
      "\u001b[31mEpoch:\u001b[0m 357 loss: \u001b[31m1.3722463336307555e-05\u001b[0m\n",
      "Best Loss 1.3697340364160482e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.3185e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.4852351341687609e-05)\u001b[0m\n",
      "Learning Rate 0.0005821250000000002\n",
      "\u001b[31mEpoch:\u001b[0m 358 loss: \u001b[31m1.371271355310455e-05\u001b[0m\n",
      "Best Loss 1.3697340364160482e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.3185e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.4542825738317333e-05)\u001b[0m\n",
      "Learning Rate 0.0005697499999999999\n",
      "\u001b[31mEpoch:\u001b[0m 359 loss: \u001b[31m1.4065398318052758e-05\u001b[0m\n",
      "Best Loss 1.3697340364160482e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.3185e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.4073505553824361e-05)\u001b[0m\n",
      "Learning Rate 0.0005573750000000001\n",
      "\u001b[31mEpoch:\u001b[0m 360 loss: \u001b[31m1.3373753063206095e-05\u001b[0m\n",
      "Best Loss 1.3373753063206095e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.3185e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.3903605577070266e-05)\u001b[0m\n",
      "Learning Rate 0.0005449999999999999\n",
      "\u001b[31mEpoch:\u001b[0m 361 loss: \u001b[31m1.3005701475776732e-05\u001b[0m\n",
      "Best Loss 1.3005701475776732e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.3185e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.4095920960244257e-05)\u001b[0m\n",
      "Learning Rate 0.0005326250000000001\n",
      "\u001b[31mEpoch:\u001b[0m 362 loss: \u001b[31m1.3314451280166395e-05\u001b[0m\n",
      "Best Loss 1.3005701475776732e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.2952e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.2952423276146874e-05)\u001b[0m\n",
      "Learning Rate 0.0005202499999999999\n",
      "\u001b[31mEpoch:\u001b[0m 363 loss: \u001b[31m1.2573700587381609e-05\u001b[0m\n",
      "Best Loss 1.2573700587381609e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.2952e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.3330576621228829e-05)\u001b[0m\n",
      "Learning Rate 0.0005078750000000001\n",
      "\u001b[31mEpoch:\u001b[0m 364 loss: \u001b[31m1.3204899005359039e-05\u001b[0m\n",
      "Best Loss 1.2573700587381609e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.2952e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.6693888028385118e-05)\u001b[0m\n",
      "Learning Rate 0.0004954999999999998\n",
      "\u001b[31mEpoch:\u001b[0m 365 loss: \u001b[31m1.3298906196723692e-05\u001b[0m\n",
      "Best Loss 1.2573700587381609e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.2952e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.298770439461805e-05)\u001b[0m\n",
      "Learning Rate 0.0004831250000000001\n",
      "\u001b[31mEpoch:\u001b[0m 366 loss: \u001b[31m1.3502688489097636e-05\u001b[0m\n",
      "Best Loss 1.2573700587381609e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.2260e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.2260456060175784e-05)\u001b[0m\n",
      "Learning Rate 0.0004707499999999998\n",
      "\u001b[31mEpoch:\u001b[0m 367 loss: \u001b[31m1.241760855918983e-05\u001b[0m\n",
      "Best Loss 1.241760855918983e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.2260e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.4145873137749732e-05)\u001b[0m\n",
      "Learning Rate 0.0004583750000000001\n",
      "\u001b[31mEpoch:\u001b[0m 368 loss: \u001b[31m1.2101671927666757e-05\u001b[0m\n",
      "Best Loss 1.2101671927666757e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.1938e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.1937760064029135e-05)\u001b[0m\n",
      "Learning Rate 0.0004459999999999998\n",
      "\u001b[31mEpoch:\u001b[0m 369 loss: \u001b[31m1.2493516805989202e-05\u001b[0m\n",
      "Best Loss 1.2101671927666757e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.1938e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.2027513548673596e-05)\u001b[0m\n",
      "Learning Rate 0.0004336250000000001\n",
      "\u001b[31mEpoch:\u001b[0m 370 loss: \u001b[31m1.1960421943513211e-05\u001b[0m\n",
      "Best Loss 1.1960421943513211e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.1938e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.2192504073027521e-05)\u001b[0m\n",
      "Learning Rate 0.0004212499999999998\n",
      "\u001b[31mEpoch:\u001b[0m 371 loss: \u001b[31m1.245069142896682e-05\u001b[0m\n",
      "Best Loss 1.1960421943513211e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.1770e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.1769731827371288e-05)\u001b[0m\n",
      "Learning Rate 0.00040887500000000006\n",
      "\u001b[31mEpoch:\u001b[0m 372 loss: \u001b[31m1.1783599802583922e-05\u001b[0m\n",
      "Best Loss 1.1783599802583922e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.1770e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.3251262316771317e-05)\u001b[0m\n",
      "Learning Rate 0.00039649999999999977\n",
      "\u001b[31mEpoch:\u001b[0m 373 loss: \u001b[31m1.2442769730114378e-05\u001b[0m\n",
      "Best Loss 1.1783599802583922e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.1065e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.1064840691688005e-05)\u001b[0m\n",
      "Learning Rate 0.000384125\n",
      "\u001b[31mEpoch:\u001b[0m 374 loss: \u001b[31m1.1664075827866327e-05\u001b[0m\n",
      "Best Loss 1.1664075827866327e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.1065e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.2439936654118355e-05)\u001b[0m\n",
      "Learning Rate 0.00037174999999999974\n",
      "\u001b[31mEpoch:\u001b[0m 375 loss: \u001b[31m1.1739197361748666e-05\u001b[0m\n",
      "Best Loss 1.1664075827866327e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.1065e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.2891850929008797e-05)\u001b[0m\n",
      "Learning Rate 0.000359375\n",
      "\u001b[31mEpoch:\u001b[0m 376 loss: \u001b[31m1.1562306099222042e-05\u001b[0m\n",
      "Best Loss 1.1562306099222042e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.1065e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.1920031283807475e-05)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0003470000000000003\n",
      "\u001b[31mEpoch:\u001b[0m 377 loss: \u001b[31m1.1426353921706323e-05\u001b[0m\n",
      "Best Loss 1.1426353921706323e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.1065e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.3335848962015007e-05)\u001b[0m\n",
      "Learning Rate 0.000334625\n",
      "\u001b[31mEpoch:\u001b[0m 378 loss: \u001b[31m1.1170766811119393e-05\u001b[0m\n",
      "Best Loss 1.1170766811119393e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.1065e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.1487803021736909e-05)\u001b[0m\n",
      "Learning Rate 0.0003222500000000002\n",
      "\u001b[31mEpoch:\u001b[0m 379 loss: \u001b[31m1.1118320799141657e-05\u001b[0m\n",
      "Best Loss 1.1118320799141657e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.1065e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.1875811651407275e-05)\u001b[0m\n",
      "Learning Rate 0.000309875\n",
      "\u001b[31mEpoch:\u001b[0m 380 loss: \u001b[31m1.0914708582276944e-05\u001b[0m\n",
      "Best Loss 1.0914708582276944e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.1065e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.157868518930627e-05)\u001b[0m\n",
      "Learning Rate 0.00029750000000000024\n",
      "\u001b[31mEpoch:\u001b[0m 381 loss: \u001b[31m1.108818469219841e-05\u001b[0m\n",
      "Best Loss 1.0914708582276944e-05\n",
      "Best Batch Loss tensor(6.7096e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.0762e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.07618661786546e-05)\u001b[0m\n",
      "Learning Rate 0.00028512499999999995\n",
      "\u001b[31mEpoch:\u001b[0m 382 loss: \u001b[31m1.1346097380737774e-05\u001b[0m\n",
      "Best Loss 1.0914708582276944e-05\n",
      "Best Batch Loss tensor(6.6678e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.0762e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.118920226872433e-05)\u001b[0m\n",
      "Learning Rate 0.0002727500000000002\n",
      "\u001b[31mEpoch:\u001b[0m 383 loss: \u001b[31m1.0982186722685583e-05\u001b[0m\n",
      "Best Loss 1.0914708582276944e-05\n",
      "Best Batch Loss tensor(6.6678e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.0657e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.0657284292392433e-05)\u001b[0m\n",
      "Learning Rate 0.0002603749999999999\n",
      "\u001b[31mEpoch:\u001b[0m 384 loss: \u001b[31m1.0755071343737654e-05\u001b[0m\n",
      "Best Loss 1.0755071343737654e-05\n",
      "Best Batch Loss tensor(6.6678e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.0553e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.0552965250099078e-05)\u001b[0m\n",
      "Learning Rate 0.0002480000000000002\n",
      "\u001b[31mEpoch:\u001b[0m 385 loss: \u001b[31m1.065488686435856e-05\u001b[0m\n",
      "Best Loss 1.065488686435856e-05\n",
      "Best Batch Loss tensor(6.5113e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.0553e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.0672290954971686e-05)\u001b[0m\n",
      "Learning Rate 0.00023562499999999989\n",
      "\u001b[31mEpoch:\u001b[0m 386 loss: \u001b[31m1.0686750101740472e-05\u001b[0m\n",
      "Best Loss 1.065488686435856e-05\n",
      "Best Batch Loss tensor(6.5113e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.0553e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.128443363995757e-05)\u001b[0m\n",
      "Learning Rate 0.00022325000000000017\n",
      "\u001b[31mEpoch:\u001b[0m 387 loss: \u001b[31m1.0431842383695766e-05\u001b[0m\n",
      "Best Loss 1.0431842383695766e-05\n",
      "Best Batch Loss tensor(6.5113e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.0553e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.2089796655345708e-05)\u001b[0m\n",
      "Learning Rate 0.00021087499999999985\n",
      "\u001b[31mEpoch:\u001b[0m 388 loss: \u001b[31m1.0536019544815645e-05\u001b[0m\n",
      "Best Loss 1.0431842383695766e-05\n",
      "Best Batch Loss tensor(6.5113e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.0306e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.030623388942331e-05)\u001b[0m\n",
      "Learning Rate 0.00019850000000000016\n",
      "\u001b[31mEpoch:\u001b[0m 389 loss: \u001b[31m9.960730494640302e-06\u001b[0m\n",
      "Best Loss 9.960730494640302e-06\n",
      "Best Batch Loss tensor(6.5113e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.0306e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.053554478858132e-05)\u001b[0m\n",
      "Learning Rate 0.00018612499999999985\n",
      "\u001b[31mEpoch:\u001b[0m 390 loss: \u001b[31m1.0048795047623571e-05\u001b[0m\n",
      "Best Loss 9.960730494640302e-06\n",
      "Best Batch Loss tensor(6.5113e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.0231e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.0231487067358103e-05)\u001b[0m\n",
      "Learning Rate 0.0001737500000000001\n",
      "\u001b[31mEpoch:\u001b[0m 391 loss: \u001b[31m9.786193004401866e-06\u001b[0m\n",
      "Best Loss 9.786193004401866e-06\n",
      "Best Batch Loss tensor(5.9220e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.0231e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.0433081115479581e-05)\u001b[0m\n",
      "Learning Rate 0.00016137499999999982\n",
      "\u001b[31mEpoch:\u001b[0m 392 loss: \u001b[31m9.71417648543138e-06\u001b[0m\n",
      "Best Loss 9.71417648543138e-06\n",
      "Best Batch Loss tensor(5.8460e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.0231e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.1317256394249853e-05)\u001b[0m\n",
      "Learning Rate 0.0001490000000000001\n",
      "\u001b[31mEpoch:\u001b[0m 393 loss: \u001b[31m9.916931958287023e-06\u001b[0m\n",
      "Best Loss 9.71417648543138e-06\n",
      "Best Batch Loss tensor(5.8460e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.0102e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.0101580301125068e-05)\u001b[0m\n",
      "Learning Rate 0.0001366249999999998\n",
      "\u001b[31mEpoch:\u001b[0m 394 loss: \u001b[31m9.715927262732293e-06\u001b[0m\n",
      "Best Loss 9.71417648543138e-06\n",
      "Best Batch Loss tensor(5.8460e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(1.0001e-05, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.0001301234296989e-05)\u001b[0m\n",
      "Learning Rate 0.00012425000000000007\n",
      "\u001b[31mEpoch:\u001b[0m 395 loss: \u001b[31m9.521736501483247e-06\u001b[0m\n",
      "Best Loss 9.521736501483247e-06\n",
      "Best Batch Loss tensor(5.8460e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(9.7843e-06, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 9.784260328160599e-06)\u001b[0m\n",
      "Learning Rate 0.00011187499999999979\n",
      "\u001b[31mEpoch:\u001b[0m 396 loss: \u001b[31m9.447689990338404e-06\u001b[0m\n",
      "Best Loss 9.447689990338404e-06\n",
      "Best Batch Loss tensor(5.8460e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(9.7843e-06, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 1.0079364983539563e-05)\u001b[0m\n",
      "Learning Rate 9.950000000000005e-05\n",
      "\u001b[31mEpoch:\u001b[0m 397 loss: \u001b[31m9.290909474657383e-06\u001b[0m\n",
      "Best Loss 9.290909474657383e-06\n",
      "Best Batch Loss tensor(5.8460e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(9.5734e-06, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 9.573403076501563e-06)\u001b[0m\n",
      "Learning Rate 8.712499999999977e-05\n",
      "\u001b[31mEpoch:\u001b[0m 398 loss: \u001b[31m9.253614734916482e-06\u001b[0m\n",
      "Best Loss 9.253614734916482e-06\n",
      "Best Batch Loss tensor(5.8460e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(9.5734e-06, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 9.62706053542206e-06)\u001b[0m\n",
      "Learning Rate 7.475000000000003e-05\n",
      "\u001b[31mEpoch:\u001b[0m 399 loss: \u001b[31m9.11076676857192e-06\u001b[0m\n",
      "Best Loss 9.11076676857192e-06\n",
      "Best Batch Loss tensor(5.8460e-06, grad_fn=<MseLossBackward>) \n",
      "\n",
      "\u001b[32mTEST\u001b[0m\n",
      "Min Test Loss tensor(9.5734e-06, grad_fn=<MseLossBackward>)\n",
      "\u001b[32m('Quadratic loss ', 9.632097317080479e-06)\u001b[0m\n",
      "\u001b[31mFinale Validation Loss\u001b[0m \u001b[31m9.11076676857192e-06\u001b[0m\n",
      "\u001b[32mBest Validation Loss\u001b[0m 9.11076676857192e-06\n",
      "\u001b[31mFinale Test Loss\u001b[0m tensor(9.6321e-06, grad_fn=<MseLossBackward>)\n",
      "\u001b[32mBest Test Loss\u001b[0m tensor(9.5734e-06, grad_fn=<MseLossBackward>)\n",
      "Data Size torch.Size([33792, 4])\n",
      "Training Time (h) 0.6835325278176202 \n",
      "\n",
      "Evolution during training\n",
      "Quadratic loss evolution\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rates\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Training\n",
    "\n",
    "t1 = time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    loss_b = 0\n",
    "    \n",
    "    in_sample_batch, out_sample_batch = shuffle_batch(model_parameters_training, implied_vols_training, batch_size)\n",
    "    \n",
    "    for in_sample, out_sample in zip(in_sample_batch, out_sample_batch) :\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out_sample = torch.reshape(matrix(out_sample, 8,11),(batch_size, 11, 8) )\n",
    "        out_sample = torch.reshape( out_sample , (batch_size, 1, 11,8 ) )\n",
    "\n",
    "        output = NN_cal(out_sample)\n",
    "        \n",
    "        loss_batch = loss(output,in_sample)\n",
    "\n",
    "        loss_b += loss_batch/nb_batch    \n",
    "        \n",
    "        loss_batch.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        min_loss = min(min_loss,loss_batch)\n",
    "      \n",
    "    LOSS_B.append(loss_b.item())\n",
    "  \n",
    "    print(\"Learning Rate\", training_rates[-1] )\n",
    "    print(colored(\"Epoch:\",'red'), epoch,\"loss:\", colored(loss_b.item(),'red') )\n",
    "    print( \"Best Loss\", min(LOSS_B))\n",
    "    print('Best Batch Loss',min_loss , '\\n')\n",
    "    \n",
    "    \n",
    "    output = NN_cal( torch.reshape( implied_vols_test , (implied_vols_test.shape[0], 1, 11, 8 ) ) )\n",
    "    \n",
    "    test = loss(output,model_parameters_test)\n",
    "    LOSS_test.append(test)\n",
    "    \n",
    "    if (test<=min(LOSS_test)):\n",
    "        with open(NN_cal_model_File, 'wb') as file:  \n",
    "            pickle.dump(NN_cal, file)      \n",
    "    \n",
    "    quadra = (\"Quadratic loss \", test.item())\n",
    "    print(colored(\"TEST\",'green') )\n",
    "    print(\"Min Test Loss\", min(LOSS_test))\n",
    "    print(colored(quadra,'green')) \n",
    "    \n",
    "    \n",
    "    if (epoch<tau):\n",
    "        rate = (1-(epoch/tau))*learning_rate + (epoch/tau)*min_rate # Cf deep learning Chap 8 p310\n",
    "        optimizer.param_groups[0]['lr'] = rate\n",
    "        training_rates.append(rate)\n",
    "        #NN_cal.elu = nn.ELU(alpha = rate, inplace=False)\n",
    "      \n",
    "    else:\n",
    "        rate = min_rate # Cf deep learning Chap 8 p310\n",
    "        optimizer.param_groups[0]['lr'] = rate\n",
    "        training_rates.append(rate)\n",
    "        #NN_cal.elu = nn.ELU(alpha = rate, inplace=False)\n",
    "    \n",
    "t2 = time()\n",
    "\n",
    "print( colored('Finale Validation Loss','red') , colored(loss_b.item(),'red'))\n",
    "print( colored('Best Validation Loss', 'green'), min(LOSS_B))\n",
    "print(colored('Finale Test Loss', 'red'), LOSS_test[-1])\n",
    "print(colored('Best Test Loss', 'green' ), min(LOSS_test) )\n",
    "print('Data Size', model_parameters_training.shape)\n",
    "print(\"Training Time (h)\", (t2-t1)/3600 ,'\\n') \n",
    "\n",
    "print( \"Evolution during training\")\n",
    "\n",
    "\n",
    "print( \"Quadratic loss evolution\")\n",
    "plt.plot(LOSS_test,color='green')\n",
    "plt.plot(LOSS_B,color='red')\n",
    "plt.show()\n",
    "\n",
    "print(\"Learning Rates\")\n",
    "plt.plot(training_rates)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(LOSS_test[10:],color='green')\n",
    "plt.plot(LOSS_B[10:],color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surface_plot (matrix, **kwargs):\n",
    "    (x, y) = np.meshgrid(strikes,maturities)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    surf = ax.plot_surface(x, y, matrix, **kwargs)\n",
    "    return (fig, ax, surf)\n",
    "\n",
    "def average_RE(pred, true):\n",
    "    return (torch.mean( abs(pred-true)/abs(true) , dim=0))\n",
    "\n",
    "def RE(pred,true):\n",
    "    return( abs(pred-true)/abs(true) )\n",
    "\n",
    "def std_RE(pred, true):\n",
    "    return (torch.std( abs(pred-true)/abs(true) , dim=0) )\n",
    "\n",
    "def max_RE(pred, true):\n",
    "    return(torch.max( abs(pred-true)/abs(true) , dim=0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average relative error for all parameters  0.007879579439759254\n"
     ]
    }
   ],
   "source": [
    "output = NN_cal( torch.reshape( implied_vols_test , (implied_vols_test.shape[0], 1, 11, 8 ) ) )\n",
    "\n",
    "\n",
    "Av_RE = average_RE(output, model_parameters_test)\n",
    "RE_ = RE(output, model_parameters_test)\n",
    "\n",
    "print( 'Average relative error for all parameters ', torch.mean(Av_RE).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "XI = model_parameters_test[:,0]\n",
    "NU = model_parameters_test[:,1]\n",
    "RHO = model_parameters_test[:,2]\n",
    "H = model_parameters_test[:,3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_xi = RE_[:,0]\n",
    "RE_nu = RE_[:,1]\n",
    "RE_rho = RE_[:,2]\n",
    "RE_H = RE_[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2025000000000001"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.42+0.72+2.23+1.44)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(r'$\\xi$', fontsize = 20)\n",
    "plt.text(0.08,0.2, 'Average='+str(np.round(Av_RE[0].item()*100,2))+'%', fontsize = 12)\n",
    "plt.plot(XI,RE_xi.detach().numpy(), '*')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.title(r'$\\nu$', fontsize = 20)\n",
    "plt.text(2,0.05, 'Average='+str(np.round(Av_RE[1].item()*100,2))+'%', fontsize = 12)\n",
    "plt.plot(NU,RE_nu.detach().numpy(), '*')\n",
    "plt.show()\n",
    "\n",
    "plt.title(r'$\\rho$', fontsize = 20)\n",
    "plt.text(-0.6,0.06, 'Average='+str(np.round(Av_RE[2].item()*100,2))+'%', fontsize = 12)\n",
    "plt.plot(RHO,RE_rho.detach().numpy(), '*')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.title('H', fontsize = 20)\n",
    "plt.text(0.3,0.2, 'Average='+str(np.round(Av_RE[3].item()*100,2))+'%', fontsize = 12)\n",
    "plt.plot(H,RE_H.detach().numpy(), '*')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
