{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Dau.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MASEF, University Paris-Dauphine 2021:   Bryan Delamour "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep calibration of rough Bergomi parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Reference : On deep calibration of (rough) stochastic volatility models\n",
    "\n",
    "C. Bayer\n",
    "B. Horvath\n",
    "A. Muguruza\n",
    "B. Stemper\n",
    "M. Tomas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cmath import * \n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "from time import time \n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from termcolor import colored\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strikes x Maturities grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "strikes = np.arange(0.5,1.6,0.1)\n",
    "maturities = np.array([0.1,0.3,0.6,0.9,1.2,1.5,1.8,2])\n",
    "S0=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximating the inverse map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calibrate rough Bergomi parameters, i.e., we consider the simple map\n",
    "\n",
    "$$\n",
    "\\Pi^{-1}\\left(\\Sigma_{\\mathrm{BS}}^{\\mathrm{r} \\mathrm{Bergomi}}\\right) \\rightarrow\\left(\\hat{\\xi}, \\hat{\\nu}, \\hat{\\rho}, \\hat{H}\\right)\n",
    "$$\n",
    "\n",
    "where $\\Sigma_{\\mathrm{BS}}^{\\mathrm{rBergomi}} \\in \\mathbb{R}^{n \\times m}$ is a rBergomi implied volatility surface and $\\left(\\hat{\\xi}, \\hat{\\nu}, \\hat{\\rho}, \\hat{H}\\right)$ the optimal solution to the corresponding calibration problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{w}=\\underset{w}{\\operatorname{argmin}} \\sum_{i=1}^{N_{T r a i n}}\\left(\\xi_i - \\hat{\\xi}_{i}\\right)^{2} + \\left( \\nu_i - \\hat{\\nu}_i \\right) + \\left( \\rho_i - \\hat{\\rho}_i\\right)^{2} + \\left( H_i - \\hat{H}_i\\right)^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{w}$ is the optimal network's weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1 convolutional layer with 16 filters and $3 \\times 3$ sliding window\n",
    "- MaxPooling layer with $2 \\times 2$ sliding window\n",
    "- 50 Neuron Feedforward Layer with Elu activation function\n",
    "- Output layer with linear activation function\n",
    "- Total number of parameters: $10 014$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"CNN_pic.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer's initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glorot initialization (xavier normal), layers' weights will have values sampled from : \n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{N}\\left(0,\\sqrt{\\frac{2}{\\text { fan_in }+\\text { fan_out }}} \\right)\n",
    "$$\n",
    "\n",
    "Where,\n",
    "\n",
    "$\\text{fan_in = layer's input dimension}$\n",
    "\n",
    "\n",
    "$\\text{fan_out = layer's output dimension}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reference: Understanding the difficulty of training deep feedforward neural networks \n",
    "X. Glorot, Y. Bengio (2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network with:\n",
    "\n",
    "hidden_dim = 30\n",
    "input_dim = 11*8 # 11x8 IV surface \n",
    "output_dim = 4 # xi, nu, rho, H \n",
    "\n",
    "class NN_CAL(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(NN_CAL, self).__init__()\n",
    "        \n",
    "        self.conv2D_1 = nn.Conv2d(1, 16, 3)\n",
    "        self.Maxpool_1 = nn.MaxPool2d(2, 2)\n",
    "        self.L1 = nn.Linear(16 * 4 * 3, 50)\n",
    "        self.L2 = nn.Linear(50, 4)\n",
    "        self.elu = nn.ELU(alpha=1, inplace=False)\n",
    "        \n",
    "\n",
    "# WEIGHTS INITIALIZATION         \n",
    "        for name, param in self.conv2D_1.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "        for name, param in self.Maxpool_1.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)  \n",
    "        for name, param in self.L1.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param) \n",
    "        for name, param in self.L2.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param) \n",
    "        \n",
    "# Feedforward       \n",
    "    def forward(self, iv_surface): # [ Batch_size , 1, 11, 8] \n",
    "                              \n",
    "        conv = self.conv2D_1(iv_surface) #[ Batch_size , 16, 9, 6]\n",
    "\n",
    "        activation1 = self.elu(conv) \n",
    "\n",
    "        maxpool = self.Maxpool_1(activation1)   #[Batch_size, 16, 4, 3]\n",
    "                \n",
    "        maxpool = maxpool.view(-1, 16 * 4 * 3) # [Batch_size, 16 * 4 * 3]\n",
    "        \n",
    "        lin1 = self.L1(maxpool)    # [Batch_size, 50]\n",
    "\n",
    "        activation2 = self.elu(lin1)\n",
    "\n",
    "        output = self.L2(activation2) # [Batch_size,  4 ]\n",
    "\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count network's parameters\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN_CAL(\n",
      "  (conv2D_1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (Maxpool_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (L1): Linear(in_features=192, out_features=50, bias=True)\n",
      "  (L2): Linear(in_features=50, out_features=4, bias=True)\n",
      "  (elu): ELU(alpha=1)\n",
      ")\n",
      "Number of parameters 10014\n"
     ]
    }
   ],
   "source": [
    "# Model summary\n",
    "\n",
    "NN_cal = NN_CAL()\n",
    "print(NN_cal)\n",
    "print(\"Number of parameters\", count_parameters(NN_CAL()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading iv surfaces and rBergomi parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data importation\n",
    "\n",
    "iv_training_File = \"iv_training_data.pkl\" \n",
    "model_parameters_training_File = \"model_parameters_training_data.pkl\" \n",
    "\n",
    "with open(iv_training_File, 'rb') as file:  \n",
    "    implied_vols = pickle.load(file)\n",
    "    \n",
    "with open(model_parameters_training_File, 'rb') as file:  \n",
    "    model_parameters = pickle.load(file)\n",
    "    \n",
    "XI = model_parameters[:,0]\n",
    "NU = model_parameters[:,1]\n",
    "RHO = model_parameters[:,2]\n",
    "H = model_parameters[:,3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of implied volatilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bar{iv}_{\\text {train }}$ the mean of the training set and the corresponding sample standard deviation $s_{\\text {train }} $. For each input $iv^{(i)}$, its standardized version $\\hat{iv}^{(i)}$ is given by\n",
    "\n",
    "$$\n",
    "\\hat{iv}^{(i)}:=\\frac{iv^{(i)}-\\bar{iv}_{\\text {train }}}{s_{\\text {train }}}, \\quad 1 \\leq i \\leq n\n",
    "$$\n",
    "\n",
    "where the operations are defined componentwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reference: Deep Learning Volatility\n",
    "\n",
    "B. Horvath, A. Muguruza, M. Tomas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "\n",
    "mean_data = np.mean(implied_vols, axis = 0)\n",
    "std_data = np.std(implied_vols, axis = 0)\n",
    "\n",
    "implied_vols_N = (implied_vols - mean_data)/std_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training and test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and test\n",
    "\n",
    "implied_vols_training =  torch.Tensor(implied_vols_N[:33792,:,:])\n",
    "model_parameters_training =  torch.Tensor(model_parameters[:33792,:])\n",
    "\n",
    "implied_vols_test =  torch.Tensor(implied_vols_N[34000:,:,:])\n",
    "model_parameters_test =  torch.Tensor(model_parameters[34000:,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam minibatch training scheme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w$ denotes the set of parameters - weights and biases $-$ of a neural network $F=F(w, x)$ Given parameters $0 \\leq \\beta_{1}, \\beta_{2}<1, \\epsilon, \\alpha$, initial iterates $u_{0}:=0, v_{0}:=0, w_{0} \\in \\Omega$, the Adam scheme has the following iterates:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "g_{n} &:=\\nabla^{w} \\sum_{i=1}^{m} \\mathcal{L}\\left(F\\left(w_{n-1}, X_{n, m}^{\\text {batch }}\\right), F^{*}\\left(X_{n, m}^{\\text {batch }}\\right)\\right) \\\\\n",
    "u_{n+1} &:=\\beta_{1} u_{n}+\\left(1-\\beta_{1}\\right) g_{n} \\\\\n",
    "v_{n+1} &:=\\beta_{2} v_{n}+\\left(1-\\beta_{2}\\right) g_{n}^{2} \\\\\n",
    "w_{n+1} &:=w_{n}-\\alpha \\frac{u_{n+1}}{1-\\beta_{1}^{n+1}} \\frac{1}{\\sqrt{v_{n} /\\left(1-\\beta_{2}^{n+1}\\right)}+\\epsilon}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Reference : On deep calibration of (rough) stochastic volatility models\n",
    "\n",
    "C. Bayer\n",
    "B. Horvath\n",
    "A. Muguruza\n",
    "B. Stemper\n",
    "M. Tomas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preventing overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent overfitting, between each epoch we sample new minibatches draw uniformly from the training set. This allows a better generalization of the network's solution. We will see its effect on the test set loss and the training set loss as both will follow the same decrease until the last epoch. The 'shuffle_batch' function below will therefore be used at the begining of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(in_sample, out_sample, batch_size):\n",
    "    rnd_idx = np.random.permutation(in_sample.shape[0])\n",
    "    in_sample_batch = []\n",
    "    out_sample_batch = []\n",
    "    n_batches = in_sample.shape[0] // batch_size\n",
    "    \n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        in_sample_batch.append(in_sample[batch_idx,:])\n",
    "        out_sample_batch.append(out_sample[batch_idx,:,:])\n",
    "        \n",
    "    return ( (in_sample_batch, out_sample_batch) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rate decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training, the solution provided by the network might come close to a local or global minimum, in this situation jumps in the loss can be seen and the loss won't be able to improve further. To prevent this situation we use a linear learning rate decay during starting at 0.005 at epoch 0, and decreasing until 0.0005 at the last epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate at epoch $k$:\n",
    "$$\n",
    "\\epsilon_{k}=(1-\\alpha) \\epsilon_{0}+\\alpha \\epsilon_{\\tau}\n",
    "$$\n",
    "\n",
    "with $\\alpha=\\frac{k}{\\tau}$ ,\n",
    "$\\tau=200$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reference: Deep Learning\n",
    "I. Goodfellow, Y. Bengio, A. Courville, Chapter 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep training the network until the last epoch, however we won't save the model unless the loss improved.\n",
    "\n",
    "In fact, we will see that in our case the loss keeps improving until the last epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Batch size} = 128$\n",
    "\n",
    "$\\text{Number of epochs} = 400$\n",
    "\n",
    "$ \\text{Starting learning rate} = 5\\times 10^{-3} $\n",
    "\n",
    "$ \\text{Ending learning rate} = 5\\times 10^{-5} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setting \n",
    "\n",
    "learning_rate = 0.005 \n",
    "batch_size = 128 \n",
    "epochs = 400\n",
    "nb_batch = model_parameters_training.shape[0]//batch_size #nb of patch / set\n",
    "optimizer = torch.optim.Adam(NN_cal.parameters(), lr=learning_rate)\n",
    "loss = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At each epoch, saving test/training loss, and learning rates \n",
    "\n",
    "min_loss = 10\n",
    "LOSS_B = []\n",
    "LOSS_test = []\n",
    "training_rates = [learning_rate]\n",
    "\n",
    "# Saving the network\n",
    "\n",
    "NN_cal_model_File = \"NN_cal_model.pkl\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rate decay\n",
    "\n",
    "tau = epochs\n",
    "min_rate = 0.01*learning_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions used to reshape network's output to a matrix\n",
    "\n",
    "def vector(matrix):\n",
    "    x = matrix[:,0,:]\n",
    "    for i in range(1,matrix.shape[1]):\n",
    "        x = torch.cat((x,matrix[:,i,:]), dim=1)\n",
    "    return(x)\n",
    "\n",
    "def matrix(vector,rows,colums):\n",
    "    x = torch.zeros((vector.shape[0],rows,colums))\n",
    "    for i in range(vector.shape[0]):\n",
    "        for j in range(rows):\n",
    "            x[i,j,:] = vector[i,j*colums:(j+1)*colums]\n",
    "    return(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEpoch:\u001b[0m 0 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.04135572165250778\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.007454408798366785\u001b[0m\n",
      "Current Learning Rate 0.005\n",
      "Best Training Loss 0.04135572165250778\n",
      "Best Batch Loss 0.005035483743995428\n",
      "Min Test Loss 0.007454408798366785 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 1 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00600444944575429\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.004569554701447487\u001b[0m\n",
      "Current Learning Rate 0.005\n",
      "Best Training Loss 0.00600444944575429\n",
      "Best Batch Loss 0.003477180376648903\n",
      "Min Test Loss 0.004569554701447487 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 2 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.003554594935849309\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.002521171234548092\u001b[0m\n",
      "Current Learning Rate 0.004987625000000001\n",
      "Best Training Loss 0.003554594935849309\n",
      "Best Batch Loss 0.0020538009703159332\n",
      "Min Test Loss 0.002521171234548092 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 3 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0022257990203797817\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0024189080577343702\u001b[0m\n",
      "Current Learning Rate 0.0049752500000000005\n",
      "Best Training Loss 0.0022257990203797817\n",
      "Best Batch Loss 0.0013490745332092047\n",
      "Min Test Loss 0.0024189080577343702 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 4 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0017000571824610233\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0021225500386208296\u001b[0m\n",
      "Current Learning Rate 0.004962875000000001\n",
      "Best Training Loss 0.0017000571824610233\n",
      "Best Batch Loss 0.0009565140935592353\n",
      "Min Test Loss 0.0021225500386208296 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 5 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0015578638995066285\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0015187232056632638\u001b[0m\n",
      "Current Learning Rate 0.0049505\n",
      "Best Training Loss 0.0015578638995066285\n",
      "Best Batch Loss 0.0008919329848140478\n",
      "Min Test Loss 0.0015187232056632638 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 6 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0012665307149291039\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0010487844701856375\u001b[0m\n",
      "Current Learning Rate 0.004938125\n",
      "Best Training Loss 0.0012665307149291039\n",
      "Best Batch Loss 0.0007766190101392567\n",
      "Min Test Loss 0.0010487844701856375 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 7 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.001151558244600892\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0012045538751408458\u001b[0m\n",
      "Current Learning Rate 0.00492575\n",
      "Best Training Loss 0.001151558244600892\n",
      "Best Batch Loss 0.0005860790843144059\n",
      "Min Test Loss 0.0010487844701856375 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 8 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0011252161348238587\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0015195624437183142\u001b[0m\n",
      "Current Learning Rate 0.004913375\n",
      "Best Training Loss 0.0011252161348238587\n",
      "Best Batch Loss 0.0005261808400973678\n",
      "Min Test Loss 0.0010487844701856375 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 9 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0010421737097203732\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0011723146308213472\u001b[0m\n",
      "Current Learning Rate 0.004901\n",
      "Best Training Loss 0.0010421737097203732\n",
      "Best Batch Loss 0.0005261808400973678\n",
      "Min Test Loss 0.0010487844701856375 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 10 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0009022115846164525\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0008780321222729981\u001b[0m\n",
      "Current Learning Rate 0.004888625000000001\n",
      "Best Training Loss 0.0009022115846164525\n",
      "Best Batch Loss 0.000435684050898999\n",
      "Min Test Loss 0.0008780321222729981 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 11 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0010606570867821574\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.000669410394039005\u001b[0m\n",
      "Current Learning Rate 0.00487625\n",
      "Best Training Loss 0.0009022115846164525\n",
      "Best Batch Loss 0.000435684050898999\n",
      "Min Test Loss 0.000669410394039005 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 12 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0008823754033073783\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0006784350844100118\u001b[0m\n",
      "Current Learning Rate 0.004863875\n",
      "Best Training Loss 0.0008823754033073783\n",
      "Best Batch Loss 0.00043113488936796784\n",
      "Min Test Loss 0.000669410394039005 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 13 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0008821945521049201\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0007478973711840808\u001b[0m\n",
      "Current Learning Rate 0.0048515\n",
      "Best Training Loss 0.0008821945521049201\n",
      "Best Batch Loss 0.00043113488936796784\n",
      "Min Test Loss 0.000669410394039005 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 14 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0008494044304825366\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0007875935407355428\u001b[0m\n",
      "Current Learning Rate 0.0048391250000000005\n",
      "Best Training Loss 0.0008494044304825366\n",
      "Best Batch Loss 0.0003807678585872054\n",
      "Min Test Loss 0.000669410394039005 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 15 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0007788730436004698\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0007359671872109175\u001b[0m\n",
      "Current Learning Rate 0.00482675\n",
      "Best Training Loss 0.0007788730436004698\n",
      "Best Batch Loss 0.000347740511642769\n",
      "Min Test Loss 0.000669410394039005 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 16 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0008174098911695182\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0005344434757716954\u001b[0m\n",
      "Current Learning Rate 0.004814375\n",
      "Best Training Loss 0.0007788730436004698\n",
      "Best Batch Loss 0.0002948699693661183\n",
      "Min Test Loss 0.0005344434757716954 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 17 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0007063732482492924\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0006771122571080923\u001b[0m\n",
      "Current Learning Rate 0.004802\n",
      "Best Training Loss 0.0007063732482492924\n",
      "Best Batch Loss 0.0002948699693661183\n",
      "Min Test Loss 0.0005344434757716954 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 18 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0005863308906555176\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0005846985732205212\u001b[0m\n",
      "Current Learning Rate 0.0047896250000000005\n",
      "Best Training Loss 0.0005863308906555176\n",
      "Best Batch Loss 0.0002948699693661183\n",
      "Min Test Loss 0.0005344434757716954 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 19 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00055545853683725\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0010562893003225327\u001b[0m\n",
      "Current Learning Rate 0.004777249999999999\n",
      "Best Training Loss 0.00055545853683725\n",
      "Best Batch Loss 0.0002948699693661183\n",
      "Min Test Loss 0.0005344434757716954 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 20 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0006349250325001776\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.000574987439904362\u001b[0m\n",
      "Current Learning Rate 0.004764875\n",
      "Best Training Loss 0.00055545853683725\n",
      "Best Batch Loss 0.0002948699693661183\n",
      "Min Test Loss 0.0005344434757716954 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 21 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0005815841723233461\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.000417210889281705\u001b[0m\n",
      "Current Learning Rate 0.0047525\n",
      "Best Training Loss 0.00055545853683725\n",
      "Best Batch Loss 0.00021912864758633077\n",
      "Min Test Loss 0.000417210889281705 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 22 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0005871355533599854\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0004542952519841492\u001b[0m\n",
      "Current Learning Rate 0.004740125\n",
      "Best Training Loss 0.00055545853683725\n",
      "Best Batch Loss 0.00021912864758633077\n",
      "Min Test Loss 0.000417210889281705 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 23 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0005108448676764965\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0005494426004588604\u001b[0m\n",
      "Current Learning Rate 0.00472775\n",
      "Best Training Loss 0.0005108448676764965\n",
      "Best Batch Loss 0.00018729227303992957\n",
      "Min Test Loss 0.000417210889281705 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 24 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0004919139319099486\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0004928410635329783\u001b[0m\n",
      "Current Learning Rate 0.004715375000000001\n",
      "Best Training Loss 0.0004919139319099486\n",
      "Best Batch Loss 0.00018729227303992957\n",
      "Min Test Loss 0.000417210889281705 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 25 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0005396113265305758\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0006344430730678141\u001b[0m\n",
      "Current Learning Rate 0.004703000000000001\n",
      "Best Training Loss 0.0004919139319099486\n",
      "Best Batch Loss 0.00018729227303992957\n",
      "Min Test Loss 0.000417210889281705 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 26 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0003931360843125731\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.000644118816126138\u001b[0m\n",
      "Current Learning Rate 0.0046906249999999995\n",
      "Best Training Loss 0.0003931360843125731\n",
      "Best Batch Loss 0.00018729227303992957\n",
      "Min Test Loss 0.000417210889281705 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 27 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0006278347573243082\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00030574348056688905\u001b[0m\n",
      "Current Learning Rate 0.00467825\n",
      "Best Training Loss 0.0003931360843125731\n",
      "Best Batch Loss 0.00018552568508312106\n",
      "Min Test Loss 0.00030574348056688905 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 28 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0005347350961528718\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00031113915611058474\u001b[0m\n",
      "Current Learning Rate 0.004665875\n",
      "Best Training Loss 0.0003931360843125731\n",
      "Best Batch Loss 0.00018552568508312106\n",
      "Min Test Loss 0.00030574348056688905 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 29 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0003899073344655335\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00041560642421245575\u001b[0m\n",
      "Current Learning Rate 0.0046535\n",
      "Best Training Loss 0.0003899073344655335\n",
      "Best Batch Loss 0.00018552568508312106\n",
      "Min Test Loss 0.00030574348056688905 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEpoch:\u001b[0m 30 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.000433989567682147\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0004138005315326154\u001b[0m\n",
      "Current Learning Rate 0.004641125\n",
      "Best Training Loss 0.0003899073344655335\n",
      "Best Batch Loss 0.00018432103388477117\n",
      "Min Test Loss 0.00030574348056688905 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 31 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0003833983209915459\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0005139975692145526\u001b[0m\n",
      "Current Learning Rate 0.004628750000000001\n",
      "Best Training Loss 0.0003833983209915459\n",
      "Best Batch Loss 0.00016638835950288922\n",
      "Min Test Loss 0.00030574348056688905 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 32 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0004227440804243088\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0005762676009908319\u001b[0m\n",
      "Current Learning Rate 0.004616375000000001\n",
      "Best Training Loss 0.0003833983209915459\n",
      "Best Batch Loss 0.00016638835950288922\n",
      "Min Test Loss 0.00030574348056688905 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 33 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0004675083328038454\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00033466864260844886\u001b[0m\n",
      "Current Learning Rate 0.004604\n",
      "Best Training Loss 0.0003833983209915459\n",
      "Best Batch Loss 0.00015463701856788248\n",
      "Min Test Loss 0.00030574348056688905 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 34 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0003659138747025281\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0002514197549317032\u001b[0m\n",
      "Current Learning Rate 0.004591625\n",
      "Best Training Loss 0.0003659138747025281\n",
      "Best Batch Loss 0.00015463701856788248\n",
      "Min Test Loss 0.0002514197549317032 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 35 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00032918958459049463\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00040310434997081757\u001b[0m\n",
      "Current Learning Rate 0.00457925\n",
      "Best Training Loss 0.00032918958459049463\n",
      "Best Batch Loss 0.00015463701856788248\n",
      "Min Test Loss 0.0002514197549317032 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 36 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00039353669853881\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0002217432192992419\u001b[0m\n",
      "Current Learning Rate 0.004566875\n",
      "Best Training Loss 0.00032918958459049463\n",
      "Best Batch Loss 0.00015463701856788248\n",
      "Min Test Loss 0.0002217432192992419 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 37 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00035039943759329617\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0006517723668366671\u001b[0m\n",
      "Current Learning Rate 0.0045545\n",
      "Best Training Loss 0.00032918958459049463\n",
      "Best Batch Loss 0.00015463701856788248\n",
      "Min Test Loss 0.0002217432192992419 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 38 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0003851378569379449\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0002988067572005093\u001b[0m\n",
      "Current Learning Rate 0.004542125\n",
      "Best Training Loss 0.00032918958459049463\n",
      "Best Batch Loss 0.00014738364552613348\n",
      "Min Test Loss 0.0002217432192992419 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 39 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00032617602846585214\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00024777810904197395\u001b[0m\n",
      "Current Learning Rate 0.004529750000000001\n",
      "Best Training Loss 0.00032617602846585214\n",
      "Best Batch Loss 0.00014738364552613348\n",
      "Min Test Loss 0.0002217432192992419 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 40 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0003483952023088932\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0003318239178042859\u001b[0m\n",
      "Current Learning Rate 0.004517375\n",
      "Best Training Loss 0.00032617602846585214\n",
      "Best Batch Loss 0.00014738364552613348\n",
      "Min Test Loss 0.0002217432192992419 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 41 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0003757743106689304\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00036452189669944346\u001b[0m\n",
      "Current Learning Rate 0.004505\n",
      "Best Training Loss 0.00032617602846585214\n",
      "Best Batch Loss 0.00014738364552613348\n",
      "Min Test Loss 0.0002217432192992419 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 42 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0003495125565677881\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00027434929506853223\u001b[0m\n",
      "Current Learning Rate 0.004492625\n",
      "Best Training Loss 0.00032617602846585214\n",
      "Best Batch Loss 0.00014738364552613348\n",
      "Min Test Loss 0.0002217432192992419 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 43 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0002792541345115751\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0002606358611956239\u001b[0m\n",
      "Current Learning Rate 0.00448025\n",
      "Best Training Loss 0.0002792541345115751\n",
      "Best Batch Loss 0.00013267337635625154\n",
      "Min Test Loss 0.0002217432192992419 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 44 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0003173957229591906\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00023876375053077936\u001b[0m\n",
      "Current Learning Rate 0.0044678750000000005\n",
      "Best Training Loss 0.0002792541345115751\n",
      "Best Batch Loss 0.00012909102952107787\n",
      "Min Test Loss 0.0002217432192992419 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 45 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0003401294816285372\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00033424553112126887\u001b[0m\n",
      "Current Learning Rate 0.0044555\n",
      "Best Training Loss 0.0002792541345115751\n",
      "Best Batch Loss 0.00012570584658533335\n",
      "Min Test Loss 0.0002217432192992419 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 46 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0002863830595742911\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0003913042019121349\u001b[0m\n",
      "Current Learning Rate 0.004443125\n",
      "Best Training Loss 0.0002792541345115751\n",
      "Best Batch Loss 0.00012570584658533335\n",
      "Min Test Loss 0.0002217432192992419 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 47 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00027754975599236786\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0002442077675368637\u001b[0m\n",
      "Current Learning Rate 0.00443075\n",
      "Best Training Loss 0.00027754975599236786\n",
      "Best Batch Loss 0.00012570584658533335\n",
      "Min Test Loss 0.0002217432192992419 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 48 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00030180293833836913\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00028043510974384844\u001b[0m\n",
      "Current Learning Rate 0.004418375\n",
      "Best Training Loss 0.00027754975599236786\n",
      "Best Batch Loss 0.00012570584658533335\n",
      "Min Test Loss 0.0002217432192992419 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 49 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0003078114823438227\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00042033943464048207\u001b[0m\n",
      "Current Learning Rate 0.004406\n",
      "Best Training Loss 0.00027754975599236786\n",
      "Best Batch Loss 0.00012570584658533335\n",
      "Min Test Loss 0.0002217432192992419 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 50 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00032696998096071184\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00017293608107138425\u001b[0m\n",
      "Current Learning Rate 0.004393625\n",
      "Best Training Loss 0.00027754975599236786\n",
      "Best Batch Loss 0.00012570584658533335\n",
      "Min Test Loss 0.00017293608107138425 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 51 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00026345704100094736\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00028999190544709563\u001b[0m\n",
      "Current Learning Rate 0.004381250000000001\n",
      "Best Training Loss 0.00026345704100094736\n",
      "Best Batch Loss 0.00011358357733115554\n",
      "Min Test Loss 0.00017293608107138425 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 52 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00026072096079587936\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0002495286171324551\u001b[0m\n",
      "Current Learning Rate 0.004368875\n",
      "Best Training Loss 0.00026072096079587936\n",
      "Best Batch Loss 0.00010053446021629497\n",
      "Min Test Loss 0.00017293608107138425 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 53 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0003150180564261973\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0003029920335393399\u001b[0m\n",
      "Current Learning Rate 0.0043565\n",
      "Best Training Loss 0.00026072096079587936\n",
      "Best Batch Loss 0.00010053446021629497\n",
      "Min Test Loss 0.00017293608107138425 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 54 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0002082947758026421\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00026841190992854536\u001b[0m\n",
      "Current Learning Rate 0.004344125\n",
      "Best Training Loss 0.0002082947758026421\n",
      "Best Batch Loss 0.00010053446021629497\n",
      "Min Test Loss 0.00017293608107138425 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 55 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0002649672969710082\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00021733521134592593\u001b[0m\n",
      "Current Learning Rate 0.00433175\n",
      "Best Training Loss 0.0002082947758026421\n",
      "Best Batch Loss 9.017697448143736e-05\n",
      "Min Test Loss 0.00017293608107138425 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 56 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0002804317919071764\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00019751083164010197\u001b[0m\n",
      "Current Learning Rate 0.004319375\n",
      "Best Training Loss 0.0002082947758026421\n",
      "Best Batch Loss 9.017697448143736e-05\n",
      "Min Test Loss 0.00017293608107138425 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 57 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00024064855824690312\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00028872728580608964\u001b[0m\n",
      "Current Learning Rate 0.004307\n",
      "Best Training Loss 0.0002082947758026421\n",
      "Best Batch Loss 9.017697448143736e-05\n",
      "Min Test Loss 0.00017293608107138425 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 58 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00027994869742542505\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0002942947030533105\u001b[0m\n",
      "Current Learning Rate 0.004294625000000001\n",
      "Best Training Loss 0.0002082947758026421\n",
      "Best Batch Loss 9.017697448143736e-05\n",
      "Min Test Loss 0.00017293608107138425 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 59 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0002581830194685608\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0002685682848095894\u001b[0m\n",
      "Current Learning Rate 0.0042822500000000005\n",
      "Best Training Loss 0.0002082947758026421\n",
      "Best Batch Loss 9.017697448143736e-05\n",
      "Min Test Loss 0.00017293608107138425 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEpoch:\u001b[0m 60 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00021294114412739873\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.000245393137447536\u001b[0m\n",
      "Current Learning Rate 0.004269875\n",
      "Best Training Loss 0.0002082947758026421\n",
      "Best Batch Loss 9.017697448143736e-05\n",
      "Min Test Loss 0.00017293608107138425 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 61 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00024219632905442268\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0002614004770293832\u001b[0m\n",
      "Current Learning Rate 0.0042575\n",
      "Best Training Loss 0.0002082947758026421\n",
      "Best Batch Loss 9.017697448143736e-05\n",
      "Min Test Loss 0.00017293608107138425 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 62 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0002591671363916248\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00013760697038378567\u001b[0m\n",
      "Current Learning Rate 0.004245125\n",
      "Best Training Loss 0.0002082947758026421\n",
      "Best Batch Loss 9.017697448143736e-05\n",
      "Min Test Loss 0.00013760697038378567 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 63 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0001982570975087583\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00016547449922654778\u001b[0m\n",
      "Current Learning Rate 0.0042327499999999995\n",
      "Best Training Loss 0.0001982570975087583\n",
      "Best Batch Loss 9.017697448143736e-05\n",
      "Min Test Loss 0.00013760697038378567 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 64 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00025868561351671815\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00031881005270406604\u001b[0m\n",
      "Current Learning Rate 0.004220375\n",
      "Best Training Loss 0.0001982570975087583\n",
      "Best Batch Loss 9.017697448143736e-05\n",
      "Min Test Loss 0.00013760697038378567 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 65 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00019721404532901943\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00020559922268148512\u001b[0m\n",
      "Current Learning Rate 0.004208\n",
      "Best Training Loss 0.00019721404532901943\n",
      "Best Batch Loss 9.017697448143736e-05\n",
      "Min Test Loss 0.00013760697038378567 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 66 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00022403649927582592\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00016968348063528538\u001b[0m\n",
      "Current Learning Rate 0.004195625000000001\n",
      "Best Training Loss 0.00019721404532901943\n",
      "Best Batch Loss 9.017697448143736e-05\n",
      "Min Test Loss 0.00013760697038378567 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 67 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00023511007020715624\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.000301972875604406\u001b[0m\n",
      "Current Learning Rate 0.0041832499999999995\n",
      "Best Training Loss 0.00019721404532901943\n",
      "Best Batch Loss 9.017697448143736e-05\n",
      "Min Test Loss 0.00013760697038378567 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 68 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00020494595810305327\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0001591254404047504\u001b[0m\n",
      "Current Learning Rate 0.004170875\n",
      "Best Training Loss 0.00019721404532901943\n",
      "Best Batch Loss 9.017697448143736e-05\n",
      "Min Test Loss 0.00013760697038378567 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 69 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00020031163876410574\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0001828197855502367\u001b[0m\n",
      "Current Learning Rate 0.0041585\n",
      "Best Training Loss 0.00019721404532901943\n",
      "Best Batch Loss 8.618681749794632e-05\n",
      "Min Test Loss 0.00013760697038378567 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 70 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00020133271755184978\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00016549020074307919\u001b[0m\n",
      "Current Learning Rate 0.0041461250000000005\n",
      "Best Training Loss 0.00019721404532901943\n",
      "Best Batch Loss 8.618681749794632e-05\n",
      "Min Test Loss 0.00013760697038378567 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 71 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0002327662950847298\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0003392654762137681\u001b[0m\n",
      "Current Learning Rate 0.00413375\n",
      "Best Training Loss 0.00019721404532901943\n",
      "Best Batch Loss 8.618681749794632e-05\n",
      "Min Test Loss 0.00013760697038378567 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 72 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00022251837071962655\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0002868151350412518\u001b[0m\n",
      "Current Learning Rate 0.004121375\n",
      "Best Training Loss 0.00019721404532901943\n",
      "Best Batch Loss 8.618681749794632e-05\n",
      "Min Test Loss 0.00013760697038378567 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 73 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0001926051190821454\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0002196463174186647\u001b[0m\n",
      "Current Learning Rate 0.004109000000000001\n",
      "Best Training Loss 0.0001926051190821454\n",
      "Best Batch Loss 8.618681749794632e-05\n",
      "Min Test Loss 0.00013760697038378567 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 74 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0002178680442739278\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00017774407751858234\u001b[0m\n",
      "Current Learning Rate 0.004096625\n",
      "Best Training Loss 0.0001926051190821454\n",
      "Best Batch Loss 8.618681749794632e-05\n",
      "Min Test Loss 0.00013760697038378567 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 75 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00022493819415103644\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00016888415848370641\u001b[0m\n",
      "Current Learning Rate 0.004084249999999999\n",
      "Best Training Loss 0.0001926051190821454\n",
      "Best Batch Loss 8.618681749794632e-05\n",
      "Min Test Loss 0.00013760697038378567 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 76 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00019672716734930873\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0001334826520178467\u001b[0m\n",
      "Current Learning Rate 0.004071875\n",
      "Best Training Loss 0.0001926051190821454\n",
      "Best Batch Loss 8.618681749794632e-05\n",
      "Min Test Loss 0.0001334826520178467 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 77 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00018855348753277212\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00028607522835955024\u001b[0m\n",
      "Current Learning Rate 0.004059500000000001\n",
      "Best Training Loss 0.00018855348753277212\n",
      "Best Batch Loss 8.618681749794632e-05\n",
      "Min Test Loss 0.0001334826520178467 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 78 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0002050210750894621\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00023374053125735372\u001b[0m\n",
      "Current Learning Rate 0.004047125\n",
      "Best Training Loss 0.00018855348753277212\n",
      "Best Batch Loss 8.618681749794632e-05\n",
      "Min Test Loss 0.0001334826520178467 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 79 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0002282903587911278\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00012198100739624351\u001b[0m\n",
      "Current Learning Rate 0.00403475\n",
      "Best Training Loss 0.00018855348753277212\n",
      "Best Batch Loss 8.618681749794632e-05\n",
      "Min Test Loss 0.00012198100739624351 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 80 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.000183434400241822\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00023314102145377547\u001b[0m\n",
      "Current Learning Rate 0.004022375000000001\n",
      "Best Training Loss 0.000183434400241822\n",
      "Best Batch Loss 8.618681749794632e-05\n",
      "Min Test Loss 0.00012198100739624351 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 81 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00018364217248745263\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00032754294807091355\u001b[0m\n",
      "Current Learning Rate 0.00401\n",
      "Best Training Loss 0.000183434400241822\n",
      "Best Batch Loss 7.961015944601968e-05\n",
      "Min Test Loss 0.00012198100739624351 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 82 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0001956511550815776\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.000262885179836303\u001b[0m\n",
      "Current Learning Rate 0.0039976249999999994\n",
      "Best Training Loss 0.000183434400241822\n",
      "Best Batch Loss 7.961015944601968e-05\n",
      "Min Test Loss 0.00012198100739624351 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 83 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00017651432426646352\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0001433517609257251\u001b[0m\n",
      "Current Learning Rate 0.00398525\n",
      "Best Training Loss 0.00017651432426646352\n",
      "Best Batch Loss 7.961015944601968e-05\n",
      "Min Test Loss 0.00012198100739624351 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 84 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00017998054681811482\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00019427042570896447\u001b[0m\n",
      "Current Learning Rate 0.003972875\n",
      "Best Training Loss 0.00017651432426646352\n",
      "Best Batch Loss 7.961015944601968e-05\n",
      "Min Test Loss 0.00012198100739624351 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 85 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00020114483777433634\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0001719821448205039\u001b[0m\n",
      "Current Learning Rate 0.0039605000000000005\n",
      "Best Training Loss 0.00017651432426646352\n",
      "Best Batch Loss 7.865666702855378e-05\n",
      "Min Test Loss 0.00012198100739624351 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 86 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0001820335746742785\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0001706424227450043\u001b[0m\n",
      "Current Learning Rate 0.003948125\n",
      "Best Training Loss 0.00017651432426646352\n",
      "Best Batch Loss 7.865666702855378e-05\n",
      "Min Test Loss 0.00012198100739624351 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 87 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00017674990522209555\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00021620797633659095\u001b[0m\n",
      "Current Learning Rate 0.003935750000000001\n",
      "Best Training Loss 0.00017651432426646352\n",
      "Best Batch Loss 7.865666702855378e-05\n",
      "Min Test Loss 0.00012198100739624351 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 88 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00019831530516967177\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00019666730077005923\u001b[0m\n",
      "Current Learning Rate 0.003923375\n",
      "Best Training Loss 0.00017651432426646352\n",
      "Best Batch Loss 7.865666702855378e-05\n",
      "Min Test Loss 0.00012198100739624351 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 89 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0002037326485151425\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00012616271851584315\u001b[0m\n",
      "Current Learning Rate 0.003911\n",
      "Best Training Loss 0.00017651432426646352\n",
      "Best Batch Loss 7.865666702855378e-05\n",
      "Min Test Loss 0.00012198100739624351 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEpoch:\u001b[0m 90 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00017844758986029774\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00013348330685403198\u001b[0m\n",
      "Current Learning Rate 0.0038986249999999997\n",
      "Best Training Loss 0.00017651432426646352\n",
      "Best Batch Loss 7.41094772820361e-05\n",
      "Min Test Loss 0.00012198100739624351 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 91 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00018192327115684748\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00015798948879819363\u001b[0m\n",
      "Current Learning Rate 0.0038862500000000004\n",
      "Best Training Loss 0.00017651432426646352\n",
      "Best Batch Loss 7.41094772820361e-05\n",
      "Min Test Loss 0.00012198100739624351 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 92 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00021769848535768688\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00010963942622765899\u001b[0m\n",
      "Current Learning Rate 0.003873875\n",
      "Best Training Loss 0.00017651432426646352\n",
      "Best Batch Loss 7.374430424533784e-05\n",
      "Min Test Loss 0.00010963942622765899 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 93 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0001669873745413497\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00021939339058008045\u001b[0m\n",
      "Current Learning Rate 0.0038615\n",
      "Best Training Loss 0.0001669873745413497\n",
      "Best Batch Loss 7.23130360711366e-05\n",
      "Min Test Loss 0.00010963942622765899 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 94 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00017002900131046772\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00015962094767019153\u001b[0m\n",
      "Current Learning Rate 0.0038491249999999997\n",
      "Best Training Loss 0.0001669873745413497\n",
      "Best Batch Loss 7.23130360711366e-05\n",
      "Min Test Loss 0.00010963942622765899 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 95 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0001672381185926497\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00019935861928388476\u001b[0m\n",
      "Current Learning Rate 0.0038367500000000003\n",
      "Best Training Loss 0.0001669873745413497\n",
      "Best Batch Loss 7.23130360711366e-05\n",
      "Min Test Loss 0.00010963942622765899 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 96 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00019078838522545993\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00018904860189650208\u001b[0m\n",
      "Current Learning Rate 0.003824375\n",
      "Best Training Loss 0.0001669873745413497\n",
      "Best Batch Loss 7.23130360711366e-05\n",
      "Min Test Loss 0.00010963942622765899 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 97 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00014929463213775307\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00017125743033830076\u001b[0m\n",
      "Current Learning Rate 0.003812\n",
      "Best Training Loss 0.00014929463213775307\n",
      "Best Batch Loss 7.23130360711366e-05\n",
      "Min Test Loss 0.00010963942622765899 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 98 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0001854164438555017\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00014539121184498072\u001b[0m\n",
      "Current Learning Rate 0.0037996250000000005\n",
      "Best Training Loss 0.00014929463213775307\n",
      "Best Batch Loss 7.23130360711366e-05\n",
      "Min Test Loss 0.00010963942622765899 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 99 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0001998158695641905\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00012880093709100038\u001b[0m\n",
      "Current Learning Rate 0.0037872500000000003\n",
      "Best Training Loss 0.00014929463213775307\n",
      "Best Batch Loss 7.23130360711366e-05\n",
      "Min Test Loss 0.00010963942622765899 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 100 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0001496554905315861\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00020906129793729633\u001b[0m\n",
      "Current Learning Rate 0.0037748749999999996\n",
      "Best Training Loss 0.00014929463213775307\n",
      "Best Batch Loss 7.23130360711366e-05\n",
      "Min Test Loss 0.00010963942622765899 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 101 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00015817228995729238\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00019278742547612637\u001b[0m\n",
      "Current Learning Rate 0.0037625\n",
      "Best Training Loss 0.00014929463213775307\n",
      "Best Batch Loss 6.849550845799968e-05\n",
      "Min Test Loss 0.00010963942622765899 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 102 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.000159873379743658\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00011984269076492637\u001b[0m\n",
      "Current Learning Rate 0.0037501250000000004\n",
      "Best Training Loss 0.00014929463213775307\n",
      "Best Batch Loss 6.849550845799968e-05\n",
      "Min Test Loss 0.00010963942622765899 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 103 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00017906872380990535\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00014698093582410365\u001b[0m\n",
      "Current Learning Rate 0.00373775\n",
      "Best Training Loss 0.00014929463213775307\n",
      "Best Batch Loss 6.849550845799968e-05\n",
      "Min Test Loss 0.00010963942622765899 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 104 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00014486546569969505\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0002740291238296777\u001b[0m\n",
      "Current Learning Rate 0.0037253749999999995\n",
      "Best Training Loss 0.00014486546569969505\n",
      "Best Batch Loss 6.657873746007681e-05\n",
      "Min Test Loss 0.00010963942622765899 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 105 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00017211928206961602\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00013888004468753934\u001b[0m\n",
      "Current Learning Rate 0.003713\n",
      "Best Training Loss 0.00014486546569969505\n",
      "Best Batch Loss 6.657873746007681e-05\n",
      "Min Test Loss 0.00010963942622765899 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 106 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00016824952035676688\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00020724475325550884\u001b[0m\n",
      "Current Learning Rate 0.0037006250000000004\n",
      "Best Training Loss 0.00014486546569969505\n",
      "Best Batch Loss 6.657873746007681e-05\n",
      "Min Test Loss 0.00010963942622765899 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 107 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00014260882744565606\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00010824672062881291\u001b[0m\n",
      "Current Learning Rate 0.0036882499999999997\n",
      "Best Training Loss 0.00014260882744565606\n",
      "Best Batch Loss 6.657873746007681e-05\n",
      "Min Test Loss 0.00010824672062881291 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 108 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0001594404166098684\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00019847415387630463\u001b[0m\n",
      "Current Learning Rate 0.0036758749999999995\n",
      "Best Training Loss 0.00014260882744565606\n",
      "Best Batch Loss 6.657873746007681e-05\n",
      "Min Test Loss 0.00010824672062881291 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 109 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00014878831279929727\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00018428605108056217\u001b[0m\n",
      "Current Learning Rate 0.0036635\n",
      "Best Training Loss 0.00014260882744565606\n",
      "Best Batch Loss 6.657873746007681e-05\n",
      "Min Test Loss 0.00010824672062881291 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 110 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0001498182537034154\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0002013146731769666\u001b[0m\n",
      "Current Learning Rate 0.0036511250000000003\n",
      "Best Training Loss 0.00014260882744565606\n",
      "Best Batch Loss 6.657873746007681e-05\n",
      "Min Test Loss 0.00010824672062881291 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 111 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0001564747653901577\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00012580344628077\u001b[0m\n",
      "Current Learning Rate 0.0036387499999999996\n",
      "Best Training Loss 0.00014260882744565606\n",
      "Best Batch Loss 6.657873746007681e-05\n",
      "Min Test Loss 0.00010824672062881291 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 112 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00016529925051145256\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.000274081714451313\u001b[0m\n",
      "Current Learning Rate 0.003626375\n",
      "Best Training Loss 0.00014260882744565606\n",
      "Best Batch Loss 6.657873746007681e-05\n",
      "Min Test Loss 0.00010824672062881291 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 113 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00013975742331240326\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00014971413474995643\u001b[0m\n",
      "Current Learning Rate 0.003614\n",
      "Best Training Loss 0.00013975742331240326\n",
      "Best Batch Loss 6.657873746007681e-05\n",
      "Min Test Loss 0.00010824672062881291 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 114 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00016934386803768575\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0001431462005712092\u001b[0m\n",
      "Current Learning Rate 0.0036016250000000002\n",
      "Best Training Loss 0.00013975742331240326\n",
      "Best Batch Loss 6.657873746007681e-05\n",
      "Min Test Loss 0.00010824672062881291 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 115 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.000134487243485637\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00012328366574365646\u001b[0m\n",
      "Current Learning Rate 0.0035892500000000004\n",
      "Best Training Loss 0.000134487243485637\n",
      "Best Batch Loss 6.657873746007681e-05\n",
      "Min Test Loss 0.00010824672062881291 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 116 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00018902678857557476\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00018386603915132582\u001b[0m\n",
      "Current Learning Rate 0.003576875\n",
      "Best Training Loss 0.000134487243485637\n",
      "Best Batch Loss 6.657873746007681e-05\n",
      "Min Test Loss 0.00010824672062881291 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 117 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00014530906628351659\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00013554032193496823\u001b[0m\n",
      "Current Learning Rate 0.0035645\n",
      "Best Training Loss 0.000134487243485637\n",
      "Best Batch Loss 6.657873746007681e-05\n",
      "Min Test Loss 0.00010824672062881291 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 118 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00014903428382240236\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00011299096513539553\u001b[0m\n",
      "Current Learning Rate 0.003552125\n",
      "Best Training Loss 0.000134487243485637\n",
      "Best Batch Loss 6.657873746007681e-05\n",
      "Min Test Loss 0.00010824672062881291 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEpoch:\u001b[0m 119 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0001487138360971585\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0001446460810257122\u001b[0m\n",
      "Current Learning Rate 0.0035397500000000004\n",
      "Best Training Loss 0.000134487243485637\n",
      "Best Batch Loss 6.657873746007681e-05\n",
      "Min Test Loss 0.00010824672062881291 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 120 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00015046699263621122\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m9.442267764825374e-05\u001b[0m\n",
      "Current Learning Rate 0.003527375\n",
      "Best Training Loss 0.000134487243485637\n",
      "Best Batch Loss 6.657873746007681e-05\n",
      "Min Test Loss 9.442267764825374e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 121 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0001231700589414686\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0001449449046049267\u001b[0m\n",
      "Current Learning Rate 0.0035149999999999995\n",
      "Best Training Loss 0.0001231700589414686\n",
      "Best Batch Loss 6.57590790069662e-05\n",
      "Min Test Loss 9.442267764825374e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 122 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00013945807586424053\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0002452525368425995\u001b[0m\n",
      "Current Learning Rate 0.003502625\n",
      "Best Training Loss 0.0001231700589414686\n",
      "Best Batch Loss 6.57590790069662e-05\n",
      "Min Test Loss 9.442267764825374e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 123 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00016470326227135956\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00019931128190364689\u001b[0m\n",
      "Current Learning Rate 0.0034902500000000003\n",
      "Best Training Loss 0.0001231700589414686\n",
      "Best Batch Loss 6.57590790069662e-05\n",
      "Min Test Loss 9.442267764825374e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 124 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00014691941032651812\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00011409623402869329\u001b[0m\n",
      "Current Learning Rate 0.0034778750000000005\n",
      "Best Training Loss 0.0001231700589414686\n",
      "Best Batch Loss 6.57590790069662e-05\n",
      "Min Test Loss 9.442267764825374e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 125 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00013494055019691586\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0001158594386652112\u001b[0m\n",
      "Current Learning Rate 0.0034655\n",
      "Best Training Loss 0.0001231700589414686\n",
      "Best Batch Loss 6.28486304776743e-05\n",
      "Min Test Loss 9.442267764825374e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 126 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00014166359324008226\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0002477632078807801\u001b[0m\n",
      "Current Learning Rate 0.003453125\n",
      "Best Training Loss 0.0001231700589414686\n",
      "Best Batch Loss 6.28486304776743e-05\n",
      "Min Test Loss 9.442267764825374e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 127 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00015772900951560587\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00010366276546847075\u001b[0m\n",
      "Current Learning Rate 0.0034407500000000007\n",
      "Best Training Loss 0.0001231700589414686\n",
      "Best Batch Loss 6.28486304776743e-05\n",
      "Min Test Loss 9.442267764825374e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 128 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00014167733024805784\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00013026015949435532\u001b[0m\n",
      "Current Learning Rate 0.003428375\n",
      "Best Training Loss 0.0001231700589414686\n",
      "Best Batch Loss 6.28486304776743e-05\n",
      "Min Test Loss 9.442267764825374e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 129 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00012134069402236491\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00010375701094744727\u001b[0m\n",
      "Current Learning Rate 0.0034159999999999998\n",
      "Best Training Loss 0.00012134069402236491\n",
      "Best Batch Loss 6.28486304776743e-05\n",
      "Min Test Loss 9.442267764825374e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 130 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00012381708074826747\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0001154773635789752\u001b[0m\n",
      "Current Learning Rate 0.003403625\n",
      "Best Training Loss 0.00012134069402236491\n",
      "Best Batch Loss 5.812058225274086e-05\n",
      "Min Test Loss 9.442267764825374e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 131 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0001407473027938977\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00018063781317323446\u001b[0m\n",
      "Current Learning Rate 0.0033912500000000006\n",
      "Best Training Loss 0.00012134069402236491\n",
      "Best Batch Loss 5.812058225274086e-05\n",
      "Min Test Loss 9.442267764825374e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 132 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00014248676598072052\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00013200740795582533\u001b[0m\n",
      "Current Learning Rate 0.003378875\n",
      "Best Training Loss 0.00012134069402236491\n",
      "Best Batch Loss 5.812058225274086e-05\n",
      "Min Test Loss 9.442267764825374e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 133 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0001245956082129851\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00013339502038434148\u001b[0m\n",
      "Current Learning Rate 0.0033664999999999997\n",
      "Best Training Loss 0.00012134069402236491\n",
      "Best Batch Loss 5.812058225274086e-05\n",
      "Min Test Loss 9.442267764825374e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 134 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00013710223720408976\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m9.546765795676038e-05\u001b[0m\n",
      "Current Learning Rate 0.0033541250000000003\n",
      "Best Training Loss 0.00012134069402236491\n",
      "Best Batch Loss 5.3543317335424945e-05\n",
      "Min Test Loss 9.442267764825374e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 135 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00012728662113659084\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0001299217256018892\u001b[0m\n",
      "Current Learning Rate 0.00334175\n",
      "Best Training Loss 0.00012134069402236491\n",
      "Best Batch Loss 5.3543317335424945e-05\n",
      "Min Test Loss 9.442267764825374e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 136 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00013891699200030416\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00016056065214797854\u001b[0m\n",
      "Current Learning Rate 0.003329375\n",
      "Best Training Loss 0.00012134069402236491\n",
      "Best Batch Loss 5.3543317335424945e-05\n",
      "Min Test Loss 9.442267764825374e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 137 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00012746105494443327\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00015033558884169906\u001b[0m\n",
      "Current Learning Rate 0.0033169999999999996\n",
      "Best Training Loss 0.00012134069402236491\n",
      "Best Batch Loss 5.3543317335424945e-05\n",
      "Min Test Loss 9.442267764825374e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 138 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0001265256287297234\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m9.044422768056393e-05\u001b[0m\n",
      "Current Learning Rate 0.0033046250000000003\n",
      "Best Training Loss 0.00012134069402236491\n",
      "Best Batch Loss 5.3543317335424945e-05\n",
      "Min Test Loss 9.044422768056393e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 139 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00014199882571119815\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0001017446702462621\u001b[0m\n",
      "Current Learning Rate 0.00329225\n",
      "Best Training Loss 0.00012134069402236491\n",
      "Best Batch Loss 5.3543317335424945e-05\n",
      "Min Test Loss 9.044422768056393e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 140 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00013592337199952453\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00010808200750034302\u001b[0m\n",
      "Current Learning Rate 0.0032798750000000007\n",
      "Best Training Loss 0.00012134069402236491\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 9.044422768056393e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 141 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00016064487863332033\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0001336353161605075\u001b[0m\n",
      "Current Learning Rate 0.0032675000000000004\n",
      "Best Training Loss 0.00012134069402236491\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 9.044422768056393e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 142 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00011575002281460911\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00012193252769066021\u001b[0m\n",
      "Current Learning Rate 0.0032551249999999998\n",
      "Best Training Loss 0.00011575002281460911\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 9.044422768056393e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 143 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00013174524065107107\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.847450953908265e-05\u001b[0m\n",
      "Current Learning Rate 0.00324275\n",
      "Best Training Loss 0.00011575002281460911\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.847450953908265e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 144 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00013228452007751912\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00012687747948803008\u001b[0m\n",
      "Current Learning Rate 0.0032303750000000006\n",
      "Best Training Loss 0.00011575002281460911\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.847450953908265e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 145 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0001120196538977325\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0001536462368676439\u001b[0m\n",
      "Current Learning Rate 0.003218\n",
      "Best Training Loss 0.0001120196538977325\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.847450953908265e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 146 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00012556262663565576\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00016403144400101155\u001b[0m\n",
      "Current Learning Rate 0.0032056249999999997\n",
      "Best Training Loss 0.0001120196538977325\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.847450953908265e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 147 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0001371536636725068\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00016093207523226738\u001b[0m\n",
      "Current Learning Rate 0.0031932500000000003\n",
      "Best Training Loss 0.0001120196538977325\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.847450953908265e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEpoch:\u001b[0m 148 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00012079054431524128\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0001043633819790557\u001b[0m\n",
      "Current Learning Rate 0.0031808750000000005\n",
      "Best Training Loss 0.0001120196538977325\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.847450953908265e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 149 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0001265722676180303\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00015758218069095165\u001b[0m\n",
      "Current Learning Rate 0.0031685\n",
      "Best Training Loss 0.0001120196538977325\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.847450953908265e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 150 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00011053212074330077\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00014753110008314252\u001b[0m\n",
      "Current Learning Rate 0.0031561249999999996\n",
      "Best Training Loss 0.00011053212074330077\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.847450953908265e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 151 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00011486824223538861\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00017863760876934975\u001b[0m\n",
      "Current Learning Rate 0.0031437500000000003\n",
      "Best Training Loss 0.00011053212074330077\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.847450953908265e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 152 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00012072762910975143\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m9.185434464598075e-05\u001b[0m\n",
      "Current Learning Rate 0.003131375\n",
      "Best Training Loss 0.00011053212074330077\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.847450953908265e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 153 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00011102880671387538\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00010860594193218276\u001b[0m\n",
      "Current Learning Rate 0.003119\n",
      "Best Training Loss 0.00011053212074330077\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.847450953908265e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 154 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00011086989979958162\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m9.658525232225657e-05\u001b[0m\n",
      "Current Learning Rate 0.003106625\n",
      "Best Training Loss 0.00011053212074330077\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.847450953908265e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 155 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00012322774273343384\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00011093135981354862\u001b[0m\n",
      "Current Learning Rate 0.00309425\n",
      "Best Training Loss 0.00011053212074330077\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.847450953908265e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 156 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00012143950880272314\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m9.107610821956769e-05\u001b[0m\n",
      "Current Learning Rate 0.003081875\n",
      "Best Training Loss 0.00011053212074330077\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.847450953908265e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 157 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00012369030446279794\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00013067512190900743\u001b[0m\n",
      "Current Learning Rate 0.0030695\n",
      "Best Training Loss 0.00011053212074330077\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.847450953908265e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 158 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00011339561024215072\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00017328788817394525\u001b[0m\n",
      "Current Learning Rate 0.003057125\n",
      "Best Training Loss 0.00011053212074330077\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.847450953908265e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 159 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0001259610871784389\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00012200830678921193\u001b[0m\n",
      "Current Learning Rate 0.0030447499999999997\n",
      "Best Training Loss 0.00011053212074330077\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.847450953908265e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 160 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00012208327825646847\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00010578315414022654\u001b[0m\n",
      "Current Learning Rate 0.0030323750000000003\n",
      "Best Training Loss 0.00011053212074330077\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.847450953908265e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 161 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0001052549559972249\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00011293731222394854\u001b[0m\n",
      "Current Learning Rate 0.00302\n",
      "Best Training Loss 0.0001052549559972249\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.847450953908265e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 162 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00011395519686629996\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00010938520426861942\u001b[0m\n",
      "Current Learning Rate 0.003007625\n",
      "Best Training Loss 0.0001052549559972249\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.847450953908265e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 163 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00012138686724938452\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0001018542461679317\u001b[0m\n",
      "Current Learning Rate 0.0029952499999999996\n",
      "Best Training Loss 0.0001052549559972249\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.847450953908265e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 164 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00011858642392326146\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.000119634838483762\u001b[0m\n",
      "Current Learning Rate 0.0029828750000000003\n",
      "Best Training Loss 0.0001052549559972249\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.847450953908265e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 165 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00010500843200134113\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0001043435768224299\u001b[0m\n",
      "Current Learning Rate 0.0029705000000000005\n",
      "Best Training Loss 0.00010500843200134113\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.847450953908265e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 166 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0001064983443939127\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00010880118497880176\u001b[0m\n",
      "Current Learning Rate 0.002958125\n",
      "Best Training Loss 0.00010500843200134113\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.847450953908265e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 167 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00012083842739230022\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m9.697060886537656e-05\u001b[0m\n",
      "Current Learning Rate 0.00294575\n",
      "Best Training Loss 0.00010500843200134113\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.847450953908265e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 168 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00011107765749329701\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.07429023552686e-05\u001b[0m\n",
      "Current Learning Rate 0.0029333750000000002\n",
      "Best Training Loss 0.00010500843200134113\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 169 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00010216821829089895\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.000170977771631442\u001b[0m\n",
      "Current Learning Rate 0.0029210000000000004\n",
      "Best Training Loss 0.00010216821829089895\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 170 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00011809429997811094\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00014447199646383524\u001b[0m\n",
      "Current Learning Rate 0.002908625\n",
      "Best Training Loss 0.00010216821829089895\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 171 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00010436790762469172\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m9.198697080137208e-05\u001b[0m\n",
      "Current Learning Rate 0.00289625\n",
      "Best Training Loss 0.00010216821829089895\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 172 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00011066155275329947\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0001496102922828868\u001b[0m\n",
      "Current Learning Rate 0.002883875\n",
      "Best Training Loss 0.00010216821829089895\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 173 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00011337860632920638\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0001435274025425315\u001b[0m\n",
      "Current Learning Rate 0.0028715000000000004\n",
      "Best Training Loss 0.00010216821829089895\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 174 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00011351919965818524\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0001296829868806526\u001b[0m\n",
      "Current Learning Rate 0.002859125\n",
      "Best Training Loss 0.00010216821829089895\n",
      "Best Batch Loss 5.177079947316088e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 175 \u001b[31mTraining Loss:\u001b[0m \u001b[31m9.731092723086476e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.893806079868227e-05\u001b[0m\n",
      "Current Learning Rate 0.00284675\n",
      "Best Training Loss 9.731092723086476e-05\n",
      "Best Batch Loss 4.8187896027229726e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 176 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00010063933586934581\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00011812336015282199\u001b[0m\n",
      "Current Learning Rate 0.002834375\n",
      "Best Training Loss 9.731092723086476e-05\n",
      "Best Batch Loss 4.8187896027229726e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 177 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00011151302169309929\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00013113601016812027\u001b[0m\n",
      "Current Learning Rate 0.0028220000000000003\n",
      "Best Training Loss 9.731092723086476e-05\n",
      "Best Batch Loss 4.8187896027229726e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEpoch:\u001b[0m 178 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00010052399011328816\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.573413651902229e-05\u001b[0m\n",
      "Current Learning Rate 0.002809625\n",
      "Best Training Loss 9.731092723086476e-05\n",
      "Best Batch Loss 4.8187896027229726e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 179 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00011274166899966076\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00017916352953761816\u001b[0m\n",
      "Current Learning Rate 0.00279725\n",
      "Best Training Loss 9.731092723086476e-05\n",
      "Best Batch Loss 4.8187896027229726e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 180 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00010497821494936943\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m9.708571451483294e-05\u001b[0m\n",
      "Current Learning Rate 0.002784875\n",
      "Best Training Loss 9.731092723086476e-05\n",
      "Best Batch Loss 4.8187896027229726e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 181 \u001b[31mTraining Loss:\u001b[0m \u001b[31m9.799025428947061e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00010075469617731869\u001b[0m\n",
      "Current Learning Rate 0.0027725000000000002\n",
      "Best Training Loss 9.731092723086476e-05\n",
      "Best Batch Loss 4.8187896027229726e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 182 \u001b[31mTraining Loss:\u001b[0m \u001b[31m9.721505921334028e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0001041175055433996\u001b[0m\n",
      "Current Learning Rate 0.002760125\n",
      "Best Training Loss 9.721505921334028e-05\n",
      "Best Batch Loss 4.8187896027229726e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 183 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00010283117444487289\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.764713129494339e-05\u001b[0m\n",
      "Current Learning Rate 0.0027477499999999998\n",
      "Best Training Loss 9.721505921334028e-05\n",
      "Best Batch Loss 4.8187896027229726e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 184 \u001b[31mTraining Loss:\u001b[0m \u001b[31m9.397857502335683e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00012505256745498627\u001b[0m\n",
      "Current Learning Rate 0.002735375\n",
      "Best Training Loss 9.397857502335683e-05\n",
      "Best Batch Loss 4.8187896027229726e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 185 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00010862217459362\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00011233310942770913\u001b[0m\n",
      "Current Learning Rate 0.002723\n",
      "Best Training Loss 9.397857502335683e-05\n",
      "Best Batch Loss 4.8187896027229726e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 186 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00010990552982548252\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.622481254860759e-05\u001b[0m\n",
      "Current Learning Rate 0.002710625\n",
      "Best Training Loss 9.397857502335683e-05\n",
      "Best Batch Loss 4.8187896027229726e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 187 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00010783695324789733\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m9.041032899403945e-05\u001b[0m\n",
      "Current Learning Rate 0.0026982499999999997\n",
      "Best Training Loss 9.397857502335683e-05\n",
      "Best Batch Loss 4.8187896027229726e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 188 \u001b[31mTraining Loss:\u001b[0m \u001b[31m8.868068107403815e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00010020940680988133\u001b[0m\n",
      "Current Learning Rate 0.002685875\n",
      "Best Training Loss 8.868068107403815e-05\n",
      "Best Batch Loss 4.8187896027229726e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 189 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00010771569213829935\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00012066867930116132\u001b[0m\n",
      "Current Learning Rate 0.0026735\n",
      "Best Training Loss 8.868068107403815e-05\n",
      "Best Batch Loss 4.8187896027229726e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 190 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00010490050772204995\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.101134153548628e-05\u001b[0m\n",
      "Current Learning Rate 0.0026611250000000007\n",
      "Best Training Loss 8.868068107403815e-05\n",
      "Best Batch Loss 4.8187896027229726e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 191 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.0001000554912025109\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m9.358594979858026e-05\u001b[0m\n",
      "Current Learning Rate 0.00264875\n",
      "Best Training Loss 8.868068107403815e-05\n",
      "Best Batch Loss 4.8187896027229726e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 192 \u001b[31mTraining Loss:\u001b[0m \u001b[31m8.978207915788516e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0001468149566790089\u001b[0m\n",
      "Current Learning Rate 0.002636375\n",
      "Best Training Loss 8.868068107403815e-05\n",
      "Best Batch Loss 4.8187896027229726e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 193 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00010492416913621128\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m9.868240158539265e-05\u001b[0m\n",
      "Current Learning Rate 0.0026240000000000005\n",
      "Best Training Loss 8.868068107403815e-05\n",
      "Best Batch Loss 4.8187896027229726e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 194 \u001b[31mTraining Loss:\u001b[0m \u001b[31m9.688216960057616e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00010061816283268854\u001b[0m\n",
      "Current Learning Rate 0.0026116250000000002\n",
      "Best Training Loss 8.868068107403815e-05\n",
      "Best Batch Loss 4.8072077333927155e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 195 \u001b[31mTraining Loss:\u001b[0m \u001b[31m8.820450602797791e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m9.740921086631715e-05\u001b[0m\n",
      "Current Learning Rate 0.00259925\n",
      "Best Training Loss 8.820450602797791e-05\n",
      "Best Batch Loss 4.4761058234144e-05\n",
      "Min Test Loss 8.07429023552686e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 196 \u001b[31mTraining Loss:\u001b[0m \u001b[31m9.995695290854201e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m7.984473631950095e-05\u001b[0m\n",
      "Current Learning Rate 0.0025868749999999998\n",
      "Best Training Loss 8.820450602797791e-05\n",
      "Best Batch Loss 4.136367351748049e-05\n",
      "Min Test Loss 7.984473631950095e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 197 \u001b[31mTraining Loss:\u001b[0m \u001b[31m9.034710092237219e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.776261529419571e-05\u001b[0m\n",
      "Current Learning Rate 0.0025745000000000004\n",
      "Best Training Loss 8.820450602797791e-05\n",
      "Best Batch Loss 4.136367351748049e-05\n",
      "Min Test Loss 7.984473631950095e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 198 \u001b[31mTraining Loss:\u001b[0m \u001b[31m9.416684770258144e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.517052629031241e-05\u001b[0m\n",
      "Current Learning Rate 0.002562125\n",
      "Best Training Loss 8.820450602797791e-05\n",
      "Best Batch Loss 4.136367351748049e-05\n",
      "Min Test Loss 7.984473631950095e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 199 \u001b[31mTraining Loss:\u001b[0m \u001b[31m0.00010136519267689437\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.192894165404141e-05\u001b[0m\n",
      "Current Learning Rate 0.00254975\n",
      "Best Training Loss 8.820450602797791e-05\n",
      "Best Batch Loss 4.136367351748049e-05\n",
      "Min Test Loss 7.984473631950095e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 200 \u001b[31mTraining Loss:\u001b[0m \u001b[31m9.666535333963111e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0001077131018973887\u001b[0m\n",
      "Current Learning Rate 0.002537375\n",
      "Best Training Loss 8.820450602797791e-05\n",
      "Best Batch Loss 4.136367351748049e-05\n",
      "Min Test Loss 7.984473631950095e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 201 \u001b[31mTraining Loss:\u001b[0m \u001b[31m9.268004941986874e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00011608647037064657\u001b[0m\n",
      "Current Learning Rate 0.002525\n",
      "Best Training Loss 8.820450602797791e-05\n",
      "Best Batch Loss 4.136367351748049e-05\n",
      "Min Test Loss 7.984473631950095e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 202 \u001b[31mTraining Loss:\u001b[0m \u001b[31m9.650792344473302e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.552908548153937e-05\u001b[0m\n",
      "Current Learning Rate 0.002512625\n",
      "Best Training Loss 8.820450602797791e-05\n",
      "Best Batch Loss 4.136367351748049e-05\n",
      "Min Test Loss 7.984473631950095e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 203 \u001b[31mTraining Loss:\u001b[0m \u001b[31m8.826178964227438e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00010452212154632434\u001b[0m\n",
      "Current Learning Rate 0.0025002500000000003\n",
      "Best Training Loss 8.820450602797791e-05\n",
      "Best Batch Loss 4.136367351748049e-05\n",
      "Min Test Loss 7.984473631950095e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 204 \u001b[31mTraining Loss:\u001b[0m \u001b[31m8.799086936051026e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.919654646888375e-05\u001b[0m\n",
      "Current Learning Rate 0.002487875\n",
      "Best Training Loss 8.799086936051026e-05\n",
      "Best Batch Loss 4.136367351748049e-05\n",
      "Min Test Loss 7.984473631950095e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 205 \u001b[31mTraining Loss:\u001b[0m \u001b[31m9.104813216254115e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m7.282342266989872e-05\u001b[0m\n",
      "Current Learning Rate 0.0024755\n",
      "Best Training Loss 8.799086936051026e-05\n",
      "Best Batch Loss 4.136367351748049e-05\n",
      "Min Test Loss 7.282342266989872e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 206 \u001b[31mTraining Loss:\u001b[0m \u001b[31m9.107157529797405e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00015157177404034883\u001b[0m\n",
      "Current Learning Rate 0.0024631250000000005\n",
      "Best Training Loss 8.799086936051026e-05\n",
      "Best Batch Loss 3.906807251041755e-05\n",
      "Min Test Loss 7.282342266989872e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 207 \u001b[31mTraining Loss:\u001b[0m \u001b[31m9.913938993122429e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00015290273586288095\u001b[0m\n",
      "Current Learning Rate 0.0024507500000000002\n",
      "Best Training Loss 8.799086936051026e-05\n",
      "Best Batch Loss 3.906807251041755e-05\n",
      "Min Test Loss 7.282342266989872e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEpoch:\u001b[0m 208 \u001b[31mTraining Loss:\u001b[0m \u001b[31m9.82332494459115e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.0001315791450906545\u001b[0m\n",
      "Current Learning Rate 0.002438375\n",
      "Best Training Loss 8.799086936051026e-05\n",
      "Best Batch Loss 3.906807251041755e-05\n",
      "Min Test Loss 7.282342266989872e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 209 \u001b[31mTraining Loss:\u001b[0m \u001b[31m9.061142918653786e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.761763456277549e-05\u001b[0m\n",
      "Current Learning Rate 0.0024259999999999998\n",
      "Best Training Loss 8.799086936051026e-05\n",
      "Best Batch Loss 3.906807251041755e-05\n",
      "Min Test Loss 7.282342266989872e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 210 \u001b[31mTraining Loss:\u001b[0m \u001b[31m8.436197822447866e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m7.552756142104045e-05\u001b[0m\n",
      "Current Learning Rate 0.0024136250000000004\n",
      "Best Training Loss 8.436197822447866e-05\n",
      "Best Batch Loss 3.906807251041755e-05\n",
      "Min Test Loss 7.282342266989872e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 211 \u001b[31mTraining Loss:\u001b[0m \u001b[31m9.081729513127357e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00011928159074159339\u001b[0m\n",
      "Current Learning Rate 0.0024012499999999997\n",
      "Best Training Loss 8.436197822447866e-05\n",
      "Best Batch Loss 3.906807251041755e-05\n",
      "Min Test Loss 7.282342266989872e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 212 \u001b[31mTraining Loss:\u001b[0m \u001b[31m8.784567035036162e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.647170761832967e-05\u001b[0m\n",
      "Current Learning Rate 0.002388875\n",
      "Best Training Loss 8.436197822447866e-05\n",
      "Best Batch Loss 3.906807251041755e-05\n",
      "Min Test Loss 7.282342266989872e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 213 \u001b[31mTraining Loss:\u001b[0m \u001b[31m9.70449618762359e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.818523201625794e-05\u001b[0m\n",
      "Current Learning Rate 0.0023765\n",
      "Best Training Loss 8.436197822447866e-05\n",
      "Best Batch Loss 3.906807251041755e-05\n",
      "Min Test Loss 7.282342266989872e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 214 \u001b[31mTraining Loss:\u001b[0m \u001b[31m8.823415555525571e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00010387509246356785\u001b[0m\n",
      "Current Learning Rate 0.0023641250000000003\n",
      "Best Training Loss 8.436197822447866e-05\n",
      "Best Batch Loss 3.906807251041755e-05\n",
      "Min Test Loss 7.282342266989872e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 215 \u001b[31mTraining Loss:\u001b[0m \u001b[31m8.704994979780167e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00010242357529932633\u001b[0m\n",
      "Current Learning Rate 0.0023517499999999997\n",
      "Best Training Loss 8.436197822447866e-05\n",
      "Best Batch Loss 3.906807251041755e-05\n",
      "Min Test Loss 7.282342266989872e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 216 \u001b[31mTraining Loss:\u001b[0m \u001b[31m9.17885554372333e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.69321302161552e-05\u001b[0m\n",
      "Current Learning Rate 0.0023393750000000003\n",
      "Best Training Loss 8.436197822447866e-05\n",
      "Best Batch Loss 3.906807251041755e-05\n",
      "Min Test Loss 7.282342266989872e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 217 \u001b[31mTraining Loss:\u001b[0m \u001b[31m8.564174640923738e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m9.566882363287732e-05\u001b[0m\n",
      "Current Learning Rate 0.002327\n",
      "Best Training Loss 8.436197822447866e-05\n",
      "Best Batch Loss 3.906807251041755e-05\n",
      "Min Test Loss 7.282342266989872e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 218 \u001b[31mTraining Loss:\u001b[0m \u001b[31m8.198117575375363e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.815847104415298e-05\u001b[0m\n",
      "Current Learning Rate 0.002314625\n",
      "Best Training Loss 8.198117575375363e-05\n",
      "Best Batch Loss 3.906807251041755e-05\n",
      "Min Test Loss 7.282342266989872e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 219 \u001b[31mTraining Loss:\u001b[0m \u001b[31m8.781465294305235e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.882702240953222e-05\u001b[0m\n",
      "Current Learning Rate 0.0023022499999999996\n",
      "Best Training Loss 8.198117575375363e-05\n",
      "Best Batch Loss 3.906807251041755e-05\n",
      "Min Test Loss 7.282342266989872e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 220 \u001b[31mTraining Loss:\u001b[0m \u001b[31m8.435333438683301e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m9.421631693840027e-05\u001b[0m\n",
      "Current Learning Rate 0.0022898750000000002\n",
      "Best Training Loss 8.198117575375363e-05\n",
      "Best Batch Loss 3.906807251041755e-05\n",
      "Min Test Loss 7.282342266989872e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 221 \u001b[31mTraining Loss:\u001b[0m \u001b[31m8.497339149471372e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.266601071227342e-05\u001b[0m\n",
      "Current Learning Rate 0.0022775\n",
      "Best Training Loss 8.198117575375363e-05\n",
      "Best Batch Loss 3.906807251041755e-05\n",
      "Min Test Loss 7.282342266989872e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 222 \u001b[31mTraining Loss:\u001b[0m \u001b[31m8.150024950737134e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m7.003876817179844e-05\u001b[0m\n",
      "Current Learning Rate 0.0022651249999999998\n",
      "Best Training Loss 8.150024950737134e-05\n",
      "Best Batch Loss 3.906807251041755e-05\n",
      "Min Test Loss 7.003876817179844e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 223 \u001b[31mTraining Loss:\u001b[0m \u001b[31m8.987293404061347e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m9.880380821414292e-05\u001b[0m\n",
      "Current Learning Rate 0.00225275\n",
      "Best Training Loss 8.150024950737134e-05\n",
      "Best Batch Loss 3.906807251041755e-05\n",
      "Min Test Loss 7.003876817179844e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 224 \u001b[31mTraining Loss:\u001b[0m \u001b[31m8.836237975629047e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m7.246972381835803e-05\u001b[0m\n",
      "Current Learning Rate 0.002240375\n",
      "Best Training Loss 8.150024950737134e-05\n",
      "Best Batch Loss 3.906807251041755e-05\n",
      "Min Test Loss 7.003876817179844e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 225 \u001b[31mTraining Loss:\u001b[0m \u001b[31m9.078729635803029e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m7.017397729214281e-05\u001b[0m\n",
      "Current Learning Rate 0.0022279999999999995\n",
      "Best Training Loss 8.150024950737134e-05\n",
      "Best Batch Loss 3.906807251041755e-05\n",
      "Min Test Loss 7.003876817179844e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 226 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.922897930257022e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.05722302175127e-05\u001b[0m\n",
      "Current Learning Rate 0.002215625\n",
      "Best Training Loss 7.922897930257022e-05\n",
      "Best Batch Loss 3.906807251041755e-05\n",
      "Min Test Loss 7.003876817179844e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 227 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.861532503739e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m7.177784573286772e-05\u001b[0m\n",
      "Current Learning Rate 0.0022032500000000003\n",
      "Best Training Loss 7.861532503739e-05\n",
      "Best Batch Loss 3.906807251041755e-05\n",
      "Min Test Loss 7.003876817179844e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 228 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.834792631911114e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m9.959032468032092e-05\u001b[0m\n",
      "Current Learning Rate 0.002190875\n",
      "Best Training Loss 7.834792631911114e-05\n",
      "Best Batch Loss 3.858529089484364e-05\n",
      "Min Test Loss 7.003876817179844e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 229 \u001b[31mTraining Loss:\u001b[0m \u001b[31m9.465768380323425e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00013143708929419518\u001b[0m\n",
      "Current Learning Rate 0.0021785000000000003\n",
      "Best Training Loss 7.834792631911114e-05\n",
      "Best Batch Loss 3.858529089484364e-05\n",
      "Min Test Loss 7.003876817179844e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 230 \u001b[31mTraining Loss:\u001b[0m \u001b[31m8.834877371555194e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m9.437357221031561e-05\u001b[0m\n",
      "Current Learning Rate 0.002166125\n",
      "Best Training Loss 7.834792631911114e-05\n",
      "Best Batch Loss 3.858529089484364e-05\n",
      "Min Test Loss 7.003876817179844e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 231 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.996358181117103e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.192337554646656e-05\u001b[0m\n",
      "Current Learning Rate 0.0021537500000000003\n",
      "Best Training Loss 7.834792631911114e-05\n",
      "Best Batch Loss 3.858529089484364e-05\n",
      "Min Test Loss 7.003876817179844e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 232 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.901665958343074e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.512015483574942e-05\u001b[0m\n",
      "Current Learning Rate 0.0021413749999999996\n",
      "Best Training Loss 7.834792631911114e-05\n",
      "Best Batch Loss 3.858529089484364e-05\n",
      "Min Test Loss 7.003876817179844e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 233 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.70337283029221e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.359171624761075e-05\u001b[0m\n",
      "Current Learning Rate 0.0021290000000000002\n",
      "Best Training Loss 7.70337283029221e-05\n",
      "Best Batch Loss 3.858529089484364e-05\n",
      "Min Test Loss 7.003876817179844e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 234 \u001b[31mTraining Loss:\u001b[0m \u001b[31m8.550038182875142e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.507260645274073e-05\u001b[0m\n",
      "Current Learning Rate 0.002116625\n",
      "Best Training Loss 7.70337283029221e-05\n",
      "Best Batch Loss 3.858529089484364e-05\n",
      "Min Test Loss 7.003876817179844e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 235 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.94482184574008e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.762071229284629e-05\u001b[0m\n",
      "Current Learning Rate 0.0021042500000000002\n",
      "Best Training Loss 7.70337283029221e-05\n",
      "Best Batch Loss 3.858529089484364e-05\n",
      "Min Test Loss 7.003876817179844e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 236 \u001b[31mTraining Loss:\u001b[0m \u001b[31m8.231274841818959e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00010459795885253698\u001b[0m\n",
      "Current Learning Rate 0.002091875\n",
      "Best Training Loss 7.70337283029221e-05\n",
      "Best Batch Loss 3.850465145660564e-05\n",
      "Min Test Loss 7.003876817179844e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 237 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.816843572072685e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m7.425911462632939e-05\u001b[0m\n",
      "Current Learning Rate 0.0020795\n",
      "Best Training Loss 7.70337283029221e-05\n",
      "Best Batch Loss 3.850465145660564e-05\n",
      "Min Test Loss 7.003876817179844e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEpoch:\u001b[0m 238 \u001b[31mTraining Loss:\u001b[0m \u001b[31m8.130849892040715e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.175389666575938e-05\u001b[0m\n",
      "Current Learning Rate 0.002067125\n",
      "Best Training Loss 7.70337283029221e-05\n",
      "Best Batch Loss 3.8038589991629124e-05\n",
      "Min Test Loss 7.003876817179844e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 239 \u001b[31mTraining Loss:\u001b[0m \u001b[31m8.320291090058163e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m6.998659227974713e-05\u001b[0m\n",
      "Current Learning Rate 0.00205475\n",
      "Best Training Loss 7.70337283029221e-05\n",
      "Best Batch Loss 3.8038589991629124e-05\n",
      "Min Test Loss 6.998659227974713e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 240 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.972079765750095e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.385636465391144e-05\u001b[0m\n",
      "Current Learning Rate 0.002042375\n",
      "Best Training Loss 7.70337283029221e-05\n",
      "Best Batch Loss 3.8038589991629124e-05\n",
      "Min Test Loss 6.998659227974713e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 241 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.669290789635852e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m7.156108040362597e-05\u001b[0m\n",
      "Current Learning Rate 0.00203\n",
      "Best Training Loss 7.669290789635852e-05\n",
      "Best Batch Loss 3.8038589991629124e-05\n",
      "Min Test Loss 6.998659227974713e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 242 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.815514982212335e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m7.536664634244516e-05\u001b[0m\n",
      "Current Learning Rate 0.002017625\n",
      "Best Training Loss 7.669290789635852e-05\n",
      "Best Batch Loss 3.8038589991629124e-05\n",
      "Min Test Loss 6.998659227974713e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 243 \u001b[31mTraining Loss:\u001b[0m \u001b[31m8.08686381787993e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.236201392719522e-05\u001b[0m\n",
      "Current Learning Rate 0.00200525\n",
      "Best Training Loss 7.669290789635852e-05\n",
      "Best Batch Loss 3.8038589991629124e-05\n",
      "Min Test Loss 6.998659227974713e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 244 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.372967957053334e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m6.371772906277329e-05\u001b[0m\n",
      "Current Learning Rate 0.001992875\n",
      "Best Training Loss 7.372967957053334e-05\n",
      "Best Batch Loss 3.8038589991629124e-05\n",
      "Min Test Loss 6.371772906277329e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 245 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.112206367310137e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m7.622314296895638e-05\u001b[0m\n",
      "Current Learning Rate 0.0019805\n",
      "Best Training Loss 7.112206367310137e-05\n",
      "Best Batch Loss 3.8038589991629124e-05\n",
      "Min Test Loss 6.371772906277329e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 246 \u001b[31mTraining Loss:\u001b[0m \u001b[31m8.024100679904222e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.436776988673955e-05\u001b[0m\n",
      "Current Learning Rate 0.001968125\n",
      "Best Training Loss 7.112206367310137e-05\n",
      "Best Batch Loss 3.8038589991629124e-05\n",
      "Min Test Loss 6.371772906277329e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 247 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.725021714577451e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m7.237298996187747e-05\u001b[0m\n",
      "Current Learning Rate 0.00195575\n",
      "Best Training Loss 7.112206367310137e-05\n",
      "Best Batch Loss 3.8038589991629124e-05\n",
      "Min Test Loss 6.371772906277329e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 248 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.376361463684589e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.9084759413963184e-05\u001b[0m\n",
      "Current Learning Rate 0.0019433749999999998\n",
      "Best Training Loss 7.112206367310137e-05\n",
      "Best Batch Loss 3.8038589991629124e-05\n",
      "Min Test Loss 5.9084759413963184e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 249 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.487148832296953e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m6.852124352008104e-05\u001b[0m\n",
      "Current Learning Rate 0.001931\n",
      "Best Training Loss 7.112206367310137e-05\n",
      "Best Batch Loss 3.8038589991629124e-05\n",
      "Min Test Loss 5.9084759413963184e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 250 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.513460150221363e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m7.849943358451128e-05\u001b[0m\n",
      "Current Learning Rate 0.0019186249999999998\n",
      "Best Training Loss 7.112206367310137e-05\n",
      "Best Batch Loss 3.8038589991629124e-05\n",
      "Min Test Loss 5.9084759413963184e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 251 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.32402186258696e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m7.728537457296625e-05\u001b[0m\n",
      "Current Learning Rate 0.00190625\n",
      "Best Training Loss 7.112206367310137e-05\n",
      "Best Batch Loss 3.8038589991629124e-05\n",
      "Min Test Loss 5.9084759413963184e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 252 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.72382554714568e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m7.321258453885093e-05\u001b[0m\n",
      "Current Learning Rate 0.0018938750000000002\n",
      "Best Training Loss 7.112206367310137e-05\n",
      "Best Batch Loss 3.8038589991629124e-05\n",
      "Min Test Loss 5.9084759413963184e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 253 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.489469862775877e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m9.139531175605953e-05\u001b[0m\n",
      "Current Learning Rate 0.0018815000000000001\n",
      "Best Training Loss 7.112206367310137e-05\n",
      "Best Batch Loss 3.8038589991629124e-05\n",
      "Min Test Loss 5.9084759413963184e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 254 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.673873915337026e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.679855818627402e-05\u001b[0m\n",
      "Current Learning Rate 0.0018691250000000003\n",
      "Best Training Loss 7.112206367310137e-05\n",
      "Best Batch Loss 3.8038589991629124e-05\n",
      "Min Test Loss 5.9084759413963184e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 255 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.018638279987499e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m9.449118078919128e-05\u001b[0m\n",
      "Current Learning Rate 0.0018567500000000001\n",
      "Best Training Loss 7.018638279987499e-05\n",
      "Best Batch Loss 3.8038589991629124e-05\n",
      "Min Test Loss 5.9084759413963184e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 256 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.3415765655227e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m0.00012247585982549936\u001b[0m\n",
      "Current Learning Rate 0.0018443750000000003\n",
      "Best Training Loss 7.018638279987499e-05\n",
      "Best Batch Loss 3.8038589991629124e-05\n",
      "Min Test Loss 5.9084759413963184e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 257 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.511779404012486e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m9.584350482327864e-05\u001b[0m\n",
      "Current Learning Rate 0.0018319999999999999\n",
      "Best Training Loss 7.018638279987499e-05\n",
      "Best Batch Loss 3.8038589991629124e-05\n",
      "Min Test Loss 5.9084759413963184e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 258 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.12400724296458e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m9.235302422894165e-05\u001b[0m\n",
      "Current Learning Rate 0.0018196250000000003\n",
      "Best Training Loss 7.018638279987499e-05\n",
      "Best Batch Loss 3.8038589991629124e-05\n",
      "Min Test Loss 5.9084759413963184e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 259 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.298168929992244e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m7.921097858343273e-05\u001b[0m\n",
      "Current Learning Rate 0.0018072499999999998\n",
      "Best Training Loss 7.018638279987499e-05\n",
      "Best Batch Loss 3.8038589991629124e-05\n",
      "Min Test Loss 5.9084759413963184e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 260 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.842256309231743e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.780864739790559e-05\u001b[0m\n",
      "Current Learning Rate 0.0017948750000000002\n",
      "Best Training Loss 7.018638279987499e-05\n",
      "Best Batch Loss 3.614578236010857e-05\n",
      "Min Test Loss 5.780864739790559e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 261 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.638951163040474e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.52634010091424e-05\u001b[0m\n",
      "Current Learning Rate 0.0017824999999999998\n",
      "Best Training Loss 6.638951163040474e-05\n",
      "Best Batch Loss 3.614578236010857e-05\n",
      "Min Test Loss 5.52634010091424e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 262 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.940465391380712e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m6.298820517258719e-05\u001b[0m\n",
      "Current Learning Rate 0.0017701250000000002\n",
      "Best Training Loss 6.638951163040474e-05\n",
      "Best Batch Loss 3.614578236010857e-05\n",
      "Min Test Loss 5.52634010091424e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 263 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.116367487469688e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m7.603958511026576e-05\u001b[0m\n",
      "Current Learning Rate 0.00175775\n",
      "Best Training Loss 6.638951163040474e-05\n",
      "Best Batch Loss 3.614578236010857e-05\n",
      "Min Test Loss 5.52634010091424e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 264 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.222538988571614e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m6.466600461862981e-05\u001b[0m\n",
      "Current Learning Rate 0.0017453750000000002\n",
      "Best Training Loss 6.638951163040474e-05\n",
      "Best Batch Loss 3.614578236010857e-05\n",
      "Min Test Loss 5.52634010091424e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 265 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.229054608615115e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m6.181194476084784e-05\u001b[0m\n",
      "Current Learning Rate 0.001733\n",
      "Best Training Loss 6.638951163040474e-05\n",
      "Best Batch Loss 3.614578236010857e-05\n",
      "Min Test Loss 5.52634010091424e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 266 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.918781582498923e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m6.993582064751536e-05\u001b[0m\n",
      "Current Learning Rate 0.0017206250000000001\n",
      "Best Training Loss 6.638951163040474e-05\n",
      "Best Batch Loss 3.614578236010857e-05\n",
      "Min Test Loss 5.52634010091424e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 267 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.652640149695799e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.896548100281507e-05\u001b[0m\n",
      "Current Learning Rate 0.00170825\n",
      "Best Training Loss 6.638951163040474e-05\n",
      "Best Batch Loss 3.614578236010857e-05\n",
      "Min Test Loss 5.52634010091424e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEpoch:\u001b[0m 268 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.790790939703584e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.8746813010657206e-05\u001b[0m\n",
      "Current Learning Rate 0.0016958750000000001\n",
      "Best Training Loss 6.638951163040474e-05\n",
      "Best Batch Loss 3.614578236010857e-05\n",
      "Min Test Loss 5.52634010091424e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 269 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.990831752773374e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.204425103031099e-05\u001b[0m\n",
      "Current Learning Rate 0.0016834999999999997\n",
      "Best Training Loss 6.638951163040474e-05\n",
      "Best Batch Loss 3.614578236010857e-05\n",
      "Min Test Loss 5.52634010091424e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 270 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.399156311294064e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.66260306793265e-05\u001b[0m\n",
      "Current Learning Rate 0.001671125\n",
      "Best Training Loss 6.638951163040474e-05\n",
      "Best Batch Loss 3.614578236010857e-05\n",
      "Min Test Loss 5.52634010091424e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 271 \u001b[31mTraining Loss:\u001b[0m \u001b[31m7.049295527394861e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m6.875292456243187e-05\u001b[0m\n",
      "Current Learning Rate 0.0016587499999999996\n",
      "Best Training Loss 6.638951163040474e-05\n",
      "Best Batch Loss 3.614578236010857e-05\n",
      "Min Test Loss 5.52634010091424e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 272 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.868541095172986e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.6055952882161364e-05\u001b[0m\n",
      "Current Learning Rate 0.001646375\n",
      "Best Training Loss 6.638951163040474e-05\n",
      "Best Batch Loss 3.614578236010857e-05\n",
      "Min Test Loss 5.52634010091424e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 273 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.770704203518108e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m6.785509322071448e-05\u001b[0m\n",
      "Current Learning Rate 0.0016339999999999998\n",
      "Best Training Loss 6.638951163040474e-05\n",
      "Best Batch Loss 3.5441691579762846e-05\n",
      "Min Test Loss 5.52634010091424e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 274 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.769646279281005e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.9501082432689145e-05\u001b[0m\n",
      "Current Learning Rate 0.0016216250000000002\n",
      "Best Training Loss 6.638951163040474e-05\n",
      "Best Batch Loss 3.5441691579762846e-05\n",
      "Min Test Loss 5.52634010091424e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 275 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.720318197039887e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m6.358770770020783e-05\u001b[0m\n",
      "Current Learning Rate 0.0016092499999999998\n",
      "Best Training Loss 6.638951163040474e-05\n",
      "Best Batch Loss 3.5441691579762846e-05\n",
      "Min Test Loss 5.52634010091424e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 276 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.894646503496915e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m6.203222437761724e-05\u001b[0m\n",
      "Current Learning Rate 0.001596875\n",
      "Best Training Loss 6.638951163040474e-05\n",
      "Best Batch Loss 3.1774827220942825e-05\n",
      "Min Test Loss 5.52634010091424e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 277 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.632263830397278e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.762030559708364e-05\u001b[0m\n",
      "Current Learning Rate 0.0015845000000000004\n",
      "Best Training Loss 6.632263830397278e-05\n",
      "Best Batch Loss 3.1774827220942825e-05\n",
      "Min Test Loss 5.52634010091424e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 278 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.671041046502069e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m6.995712465140969e-05\u001b[0m\n",
      "Current Learning Rate 0.001572125\n",
      "Best Training Loss 6.632263830397278e-05\n",
      "Best Batch Loss 3.1774827220942825e-05\n",
      "Min Test Loss 5.52634010091424e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 279 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.565527291968465e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m7.052211731206626e-05\u001b[0m\n",
      "Current Learning Rate 0.0015597500000000004\n",
      "Best Training Loss 6.565527291968465e-05\n",
      "Best Batch Loss 3.1774827220942825e-05\n",
      "Min Test Loss 5.52634010091424e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 280 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.970905815251172e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.794448225060478e-05\u001b[0m\n",
      "Current Learning Rate 0.001547375\n",
      "Best Training Loss 6.565527291968465e-05\n",
      "Best Batch Loss 3.1774827220942825e-05\n",
      "Min Test Loss 5.52634010091424e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 281 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.51658556307666e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.429850277025253e-05\u001b[0m\n",
      "Current Learning Rate 0.0015350000000000003\n",
      "Best Training Loss 6.51658556307666e-05\n",
      "Best Batch Loss 3.1774827220942825e-05\n",
      "Min Test Loss 5.52634010091424e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 282 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.27159679424949e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m6.859016139060259e-05\u001b[0m\n",
      "Current Learning Rate 0.0015226249999999999\n",
      "Best Training Loss 6.27159679424949e-05\n",
      "Best Batch Loss 3.1774827220942825e-05\n",
      "Min Test Loss 5.52634010091424e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 283 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.420835416065529e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.212454925640486e-05\u001b[0m\n",
      "Current Learning Rate 0.00151025\n",
      "Best Training Loss 6.27159679424949e-05\n",
      "Best Batch Loss 3.1774827220942825e-05\n",
      "Min Test Loss 5.212454925640486e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 284 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.121794285718352e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.316744864103384e-05\u001b[0m\n",
      "Current Learning Rate 0.001497875\n",
      "Best Training Loss 6.121794285718352e-05\n",
      "Best Batch Loss 3.1774827220942825e-05\n",
      "Min Test Loss 5.212454925640486e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 285 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.148354441393167e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m6.807210593251511e-05\u001b[0m\n",
      "Current Learning Rate 0.0014855\n",
      "Best Training Loss 6.121794285718352e-05\n",
      "Best Batch Loss 3.1774827220942825e-05\n",
      "Min Test Loss 5.212454925640486e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 286 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.21555300313048e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.047723607276566e-05\u001b[0m\n",
      "Current Learning Rate 0.001473125\n",
      "Best Training Loss 6.121794285718352e-05\n",
      "Best Batch Loss 3.1774827220942825e-05\n",
      "Min Test Loss 5.047723607276566e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 287 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.388776819221675e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m7.55436485633254e-05\u001b[0m\n",
      "Current Learning Rate 0.0014607500000000002\n",
      "Best Training Loss 6.121794285718352e-05\n",
      "Best Batch Loss 3.1774827220942825e-05\n",
      "Min Test Loss 5.047723607276566e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 288 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.272750033531338e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m8.022854308364913e-05\u001b[0m\n",
      "Current Learning Rate 0.001448375\n",
      "Best Training Loss 6.121794285718352e-05\n",
      "Best Batch Loss 3.1774827220942825e-05\n",
      "Min Test Loss 5.047723607276566e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 289 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.24334134045057e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.235147909843363e-05\u001b[0m\n",
      "Current Learning Rate 0.0014360000000000002\n",
      "Best Training Loss 6.121794285718352e-05\n",
      "Best Batch Loss 3.1774827220942825e-05\n",
      "Min Test Loss 5.047723607276566e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 290 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.71337911626324e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m6.138477328931913e-05\u001b[0m\n",
      "Current Learning Rate 0.0014236249999999998\n",
      "Best Training Loss 6.121794285718352e-05\n",
      "Best Batch Loss 3.1774827220942825e-05\n",
      "Min Test Loss 5.047723607276566e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 291 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.200192729011178e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.224208871368319e-05\u001b[0m\n",
      "Current Learning Rate 0.0014112500000000002\n",
      "Best Training Loss 6.121794285718352e-05\n",
      "Best Batch Loss 3.1774827220942825e-05\n",
      "Min Test Loss 5.047723607276566e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 292 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.9556987253017724e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.688191595254466e-05\u001b[0m\n",
      "Current Learning Rate 0.0013988749999999997\n",
      "Best Training Loss 5.9556987253017724e-05\n",
      "Best Batch Loss 3.1774827220942825e-05\n",
      "Min Test Loss 5.047723607276566e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 293 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.243012467166409e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m7.169786840677261e-05\u001b[0m\n",
      "Current Learning Rate 0.0013865000000000001\n",
      "Best Training Loss 5.9556987253017724e-05\n",
      "Best Batch Loss 3.1774827220942825e-05\n",
      "Min Test Loss 5.047723607276566e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 294 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.816149860038422e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m7.32882836018689e-05\u001b[0m\n",
      "Current Learning Rate 0.001374125\n",
      "Best Training Loss 5.816149860038422e-05\n",
      "Best Batch Loss 3.1774827220942825e-05\n",
      "Min Test Loss 5.047723607276566e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 295 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.46184416837059e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m6.568212120328099e-05\u001b[0m\n",
      "Current Learning Rate 0.0013617500000000001\n",
      "Best Training Loss 5.816149860038422e-05\n",
      "Best Batch Loss 3.1774827220942825e-05\n",
      "Min Test Loss 5.047723607276566e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 296 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.120682519394904e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.3774972911924124e-05\u001b[0m\n",
      "Current Learning Rate 0.0013493749999999999\n",
      "Best Training Loss 5.816149860038422e-05\n",
      "Best Batch Loss 3.1774827220942825e-05\n",
      "Min Test Loss 5.047723607276566e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEpoch:\u001b[0m 297 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.23602099949494e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m6.0013000620529056e-05\u001b[0m\n",
      "Current Learning Rate 0.001337\n",
      "Best Training Loss 5.816149860038422e-05\n",
      "Best Batch Loss 3.1774827220942825e-05\n",
      "Min Test Loss 5.047723607276566e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 298 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.8790985349332914e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.934404180152342e-05\u001b[0m\n",
      "Current Learning Rate 0.0013246249999999998\n",
      "Best Training Loss 5.816149860038422e-05\n",
      "Best Batch Loss 3.1774827220942825e-05\n",
      "Min Test Loss 5.047723607276566e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 299 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.303888221736997e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.8617522881831974e-05\u001b[0m\n",
      "Current Learning Rate 0.00131225\n",
      "Best Training Loss 5.816149860038422e-05\n",
      "Best Batch Loss 3.1774827220942825e-05\n",
      "Min Test Loss 5.047723607276566e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 300 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.9230962506262586e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.440440872916952e-05\u001b[0m\n",
      "Current Learning Rate 0.0012998749999999998\n",
      "Best Training Loss 5.816149860038422e-05\n",
      "Best Batch Loss 3.0694773158757016e-05\n",
      "Min Test Loss 5.047723607276566e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 301 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.587542909779586e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.5290362070081756e-05\u001b[0m\n",
      "Current Learning Rate 0.0012875\n",
      "Best Training Loss 5.587542909779586e-05\n",
      "Best Batch Loss 3.0694773158757016e-05\n",
      "Min Test Loss 5.047723607276566e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 302 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.242482049856335e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m6.0542570281540975e-05\u001b[0m\n",
      "Current Learning Rate 0.0012751250000000002\n",
      "Best Training Loss 5.587542909779586e-05\n",
      "Best Batch Loss 3.0694773158757016e-05\n",
      "Min Test Loss 5.047723607276566e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 303 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.6582306569907814e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.539361518458463e-05\u001b[0m\n",
      "Current Learning Rate 0.00126275\n",
      "Best Training Loss 5.587542909779586e-05\n",
      "Best Batch Loss 3.0694773158757016e-05\n",
      "Min Test Loss 5.047723607276566e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 304 \u001b[31mTraining Loss:\u001b[0m \u001b[31m6.046696216799319e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m6.441251753130928e-05\u001b[0m\n",
      "Current Learning Rate 0.0012503750000000002\n",
      "Best Training Loss 5.587542909779586e-05\n",
      "Best Batch Loss 3.0694773158757016e-05\n",
      "Min Test Loss 5.047723607276566e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 305 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.8465899201110005e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.3976778872311115e-05\u001b[0m\n",
      "Current Learning Rate 0.001238\n",
      "Best Training Loss 5.587542909779586e-05\n",
      "Best Batch Loss 3.0694773158757016e-05\n",
      "Min Test Loss 5.047723607276566e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 306 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.6364537158515304e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m6.255734479054809e-05\u001b[0m\n",
      "Current Learning Rate 0.0012256250000000002\n",
      "Best Training Loss 5.587542909779586e-05\n",
      "Best Batch Loss 3.0694773158757016e-05\n",
      "Min Test Loss 5.047723607276566e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 307 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.7784396631177515e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.557491022045724e-05\u001b[0m\n",
      "Current Learning Rate 0.0012132500000000001\n",
      "Best Training Loss 5.587542909779586e-05\n",
      "Best Batch Loss 3.0481505746138282e-05\n",
      "Min Test Loss 5.047723607276566e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 308 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.8265028201276436e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.324720768840052e-05\u001b[0m\n",
      "Current Learning Rate 0.0012008750000000001\n",
      "Best Training Loss 5.587542909779586e-05\n",
      "Best Batch Loss 3.0481505746138282e-05\n",
      "Min Test Loss 5.047723607276566e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 309 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.6141561799449846e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.299777694744989e-05\u001b[0m\n",
      "Current Learning Rate 0.0011884999999999999\n",
      "Best Training Loss 5.587542909779586e-05\n",
      "Best Batch Loss 3.0481505746138282e-05\n",
      "Min Test Loss 5.047723607276566e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 310 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.599170981440693e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.964368054061197e-05\u001b[0m\n",
      "Current Learning Rate 0.0011761250000000003\n",
      "Best Training Loss 5.587542909779586e-05\n",
      "Best Batch Loss 3.0481505746138282e-05\n",
      "Min Test Loss 4.964368054061197e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 311 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.6142012908821926e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.5509372032247484e-05\u001b[0m\n",
      "Current Learning Rate 0.0011637499999999999\n",
      "Best Training Loss 5.587542909779586e-05\n",
      "Best Batch Loss 3.0481505746138282e-05\n",
      "Min Test Loss 4.964368054061197e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 312 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.514694930752739e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.042706834501587e-05\u001b[0m\n",
      "Current Learning Rate 0.0011513750000000003\n",
      "Best Training Loss 5.514694930752739e-05\n",
      "Best Batch Loss 2.974170092784334e-05\n",
      "Min Test Loss 4.964368054061197e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 313 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.532318391487934e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m6.466318882303312e-05\u001b[0m\n",
      "Current Learning Rate 0.0011389999999999998\n",
      "Best Training Loss 5.514694930752739e-05\n",
      "Best Batch Loss 2.974170092784334e-05\n",
      "Min Test Loss 4.964368054061197e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 314 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.4775842727394775e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.729361510020681e-05\u001b[0m\n",
      "Current Learning Rate 0.0011266250000000002\n",
      "Best Training Loss 5.4775842727394775e-05\n",
      "Best Batch Loss 2.974170092784334e-05\n",
      "Min Test Loss 4.964368054061197e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 315 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.443958070827648e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m6.301028770394623e-05\u001b[0m\n",
      "Current Learning Rate 0.0011142499999999998\n",
      "Best Training Loss 5.443958070827648e-05\n",
      "Best Batch Loss 2.974170092784334e-05\n",
      "Min Test Loss 4.964368054061197e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 316 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.7373363233637065e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m7.025564991636202e-05\u001b[0m\n",
      "Current Learning Rate 0.001101875\n",
      "Best Training Loss 5.443958070827648e-05\n",
      "Best Batch Loss 2.8742066206177697e-05\n",
      "Min Test Loss 4.964368054061197e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 317 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.6413671700283885e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m6.176004535518587e-05\u001b[0m\n",
      "Current Learning Rate 0.0010895\n",
      "Best Training Loss 5.443958070827648e-05\n",
      "Best Batch Loss 2.8742066206177697e-05\n",
      "Min Test Loss 4.964368054061197e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 318 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.297978350427002e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.225825952948071e-05\u001b[0m\n",
      "Current Learning Rate 0.001077125\n",
      "Best Training Loss 5.297978350427002e-05\n",
      "Best Batch Loss 2.8742066206177697e-05\n",
      "Min Test Loss 4.964368054061197e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 319 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.4502295824931934e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m6.454011599998921e-05\u001b[0m\n",
      "Current Learning Rate 0.00106475\n",
      "Best Training Loss 5.297978350427002e-05\n",
      "Best Batch Loss 2.8742066206177697e-05\n",
      "Min Test Loss 4.964368054061197e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 320 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.4553507652599365e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.0700644351309165e-05\u001b[0m\n",
      "Current Learning Rate 0.0010523750000000001\n",
      "Best Training Loss 5.297978350427002e-05\n",
      "Best Batch Loss 2.8742066206177697e-05\n",
      "Min Test Loss 4.964368054061197e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 321 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.544107989408076e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.4243882914306596e-05\u001b[0m\n",
      "Current Learning Rate 0.00104\n",
      "Best Training Loss 5.297978350427002e-05\n",
      "Best Batch Loss 2.8742066206177697e-05\n",
      "Min Test Loss 4.964368054061197e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 322 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.2563049393938854e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m6.015519829816185e-05\u001b[0m\n",
      "Current Learning Rate 0.0010276250000000001\n",
      "Best Training Loss 5.2563049393938854e-05\n",
      "Best Batch Loss 2.8742066206177697e-05\n",
      "Min Test Loss 4.964368054061197e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 323 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.1204569899709895e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.8205009079538286e-05\u001b[0m\n",
      "Current Learning Rate 0.0010152499999999997\n",
      "Best Training Loss 5.1204569899709895e-05\n",
      "Best Batch Loss 2.8742066206177697e-05\n",
      "Min Test Loss 4.8205009079538286e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 324 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.345378667698242e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.0842572818510234e-05\u001b[0m\n",
      "Current Learning Rate 0.001002875\n",
      "Best Training Loss 5.1204569899709895e-05\n",
      "Best Batch Loss 2.8742066206177697e-05\n",
      "Min Test Loss 4.8205009079538286e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 325 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.143671660334803e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.73704055568669e-05\u001b[0m\n",
      "Current Learning Rate 0.0009904999999999998\n",
      "Best Training Loss 5.1204569899709895e-05\n",
      "Best Batch Loss 2.8742066206177697e-05\n",
      "Min Test Loss 4.8205009079538286e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEpoch:\u001b[0m 326 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.3323972679208964e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.3023330110590905e-05\u001b[0m\n",
      "Current Learning Rate 0.000978125\n",
      "Best Training Loss 5.1204569899709895e-05\n",
      "Best Batch Loss 2.8742066206177697e-05\n",
      "Min Test Loss 4.8205009079538286e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 327 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.129577039042488e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.8415949752088636e-05\u001b[0m\n",
      "Current Learning Rate 0.0009657500000000002\n",
      "Best Training Loss 5.1204569899709895e-05\n",
      "Best Batch Loss 2.8742066206177697e-05\n",
      "Min Test Loss 4.8205009079538286e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 328 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.142170994076878e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.222934123594314e-05\u001b[0m\n",
      "Current Learning Rate 0.000953375\n",
      "Best Training Loss 5.1204569899709895e-05\n",
      "Best Batch Loss 2.8742066206177697e-05\n",
      "Min Test Loss 4.8205009079538286e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 329 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.2512699767248705e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.27987394889351e-05\u001b[0m\n",
      "Current Learning Rate 0.0009410000000000003\n",
      "Best Training Loss 5.1204569899709895e-05\n",
      "Best Batch Loss 2.8742066206177697e-05\n",
      "Min Test Loss 4.8205009079538286e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 330 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.361923103919253e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.803660678793676e-05\u001b[0m\n",
      "Current Learning Rate 0.000928625\n",
      "Best Training Loss 5.1204569899709895e-05\n",
      "Best Batch Loss 2.8742066206177697e-05\n",
      "Min Test Loss 4.8205009079538286e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 331 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.1933569920947775e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.997906580683775e-05\u001b[0m\n",
      "Current Learning Rate 0.0009162500000000002\n",
      "Best Training Loss 5.1204569899709895e-05\n",
      "Best Batch Loss 2.8742066206177697e-05\n",
      "Min Test Loss 4.8205009079538286e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 332 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.915663521387614e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.421690730145201e-05\u001b[0m\n",
      "Current Learning Rate 0.000903875\n",
      "Best Training Loss 4.915663521387614e-05\n",
      "Best Batch Loss 2.8742066206177697e-05\n",
      "Min Test Loss 4.8205009079538286e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 333 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.143283124198206e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.0202310376334935e-05\u001b[0m\n",
      "Current Learning Rate 0.0008915000000000001\n",
      "Best Training Loss 4.915663521387614e-05\n",
      "Best Batch Loss 2.8742066206177697e-05\n",
      "Min Test Loss 4.8205009079538286e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 334 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.9893424147740006e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.772883741883561e-05\u001b[0m\n",
      "Current Learning Rate 0.0008791249999999999\n",
      "Best Training Loss 4.915663521387614e-05\n",
      "Best Batch Loss 2.85569803963881e-05\n",
      "Min Test Loss 4.8205009079538286e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 335 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.1486575102899224e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.234694617683999e-05\u001b[0m\n",
      "Current Learning Rate 0.0008667500000000002\n",
      "Best Training Loss 4.915663521387614e-05\n",
      "Best Batch Loss 2.85569803963881e-05\n",
      "Min Test Loss 4.8205009079538286e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 336 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.818764500669204e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.496699693845585e-05\u001b[0m\n",
      "Current Learning Rate 0.0008543749999999999\n",
      "Best Training Loss 4.818764500669204e-05\n",
      "Best Batch Loss 2.85569803963881e-05\n",
      "Min Test Loss 4.496699693845585e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 337 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.081795097794384e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m6.080885577830486e-05\u001b[0m\n",
      "Current Learning Rate 0.0008420000000000002\n",
      "Best Training Loss 4.818764500669204e-05\n",
      "Best Batch Loss 2.644222513481509e-05\n",
      "Min Test Loss 4.496699693845585e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 338 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.1198141591157764e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.5534626881126314e-05\u001b[0m\n",
      "Current Learning Rate 0.0008296249999999999\n",
      "Best Training Loss 4.818764500669204e-05\n",
      "Best Batch Loss 2.644222513481509e-05\n",
      "Min Test Loss 4.496699693845585e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 339 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.834306673728861e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.045242141932249e-05\u001b[0m\n",
      "Current Learning Rate 0.0008172500000000002\n",
      "Best Training Loss 4.818764500669204e-05\n",
      "Best Batch Loss 2.644222513481509e-05\n",
      "Min Test Loss 4.496699693845585e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 340 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.89244339405559e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.3191612096270546e-05\u001b[0m\n",
      "Current Learning Rate 0.0008048749999999998\n",
      "Best Training Loss 4.818764500669204e-05\n",
      "Best Batch Loss 2.6197745683020912e-05\n",
      "Min Test Loss 4.496699693845585e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 341 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.758334398502484e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.597632480203174e-05\u001b[0m\n",
      "Current Learning Rate 0.0007925000000000001\n",
      "Best Training Loss 4.758334398502484e-05\n",
      "Best Batch Loss 2.6197745683020912e-05\n",
      "Min Test Loss 4.496699693845585e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 342 \u001b[31mTraining Loss:\u001b[0m \u001b[31m5.011669418308884e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m7.257911056512967e-05\u001b[0m\n",
      "Current Learning Rate 0.0007801249999999999\n",
      "Best Training Loss 4.758334398502484e-05\n",
      "Best Batch Loss 2.6197745683020912e-05\n",
      "Min Test Loss 4.496699693845585e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 343 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.765435733133927e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.557490683509968e-05\u001b[0m\n",
      "Current Learning Rate 0.0007677500000000001\n",
      "Best Training Loss 4.758334398502484e-05\n",
      "Best Batch Loss 2.6197745683020912e-05\n",
      "Min Test Loss 4.496699693845585e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 344 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.7256187826860696e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.588768908637576e-05\u001b[0m\n",
      "Current Learning Rate 0.0007553749999999999\n",
      "Best Training Loss 4.7256187826860696e-05\n",
      "Best Batch Loss 2.6197745683020912e-05\n",
      "Min Test Loss 4.496699693845585e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 345 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.767327482113615e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.5627817598870024e-05\u001b[0m\n",
      "Current Learning Rate 0.0007430000000000001\n",
      "Best Training Loss 4.7256187826860696e-05\n",
      "Best Batch Loss 2.6197745683020912e-05\n",
      "Min Test Loss 4.496699693845585e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 346 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.6480367018375546e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.24992065038532e-05\u001b[0m\n",
      "Current Learning Rate 0.0007306249999999997\n",
      "Best Training Loss 4.6480367018375546e-05\n",
      "Best Batch Loss 2.6197745683020912e-05\n",
      "Min Test Loss 4.496699693845585e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 347 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.773871478391811e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.476417234400287e-05\u001b[0m\n",
      "Current Learning Rate 0.00071825\n",
      "Best Training Loss 4.6480367018375546e-05\n",
      "Best Batch Loss 2.6197745683020912e-05\n",
      "Min Test Loss 4.476417234400287e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 348 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.739662108477205e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.384581891121343e-05\u001b[0m\n",
      "Current Learning Rate 0.0007058749999999998\n",
      "Best Training Loss 4.6480367018375546e-05\n",
      "Best Batch Loss 2.6197745683020912e-05\n",
      "Min Test Loss 4.476417234400287e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 349 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.7192330384859815e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.830209581996314e-05\u001b[0m\n",
      "Current Learning Rate 0.0006935000000000001\n",
      "Best Training Loss 4.6480367018375546e-05\n",
      "Best Batch Loss 2.6197745683020912e-05\n",
      "Min Test Loss 4.476417234400287e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 350 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.6769389882683754e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.762996104545891e-05\u001b[0m\n",
      "Current Learning Rate 0.0006811249999999998\n",
      "Best Training Loss 4.6480367018375546e-05\n",
      "Best Batch Loss 2.6197745683020912e-05\n",
      "Min Test Loss 4.476417234400287e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 351 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.6860579459462315e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.9379745178157464e-05\u001b[0m\n",
      "Current Learning Rate 0.00066875\n",
      "Best Training Loss 4.6480367018375546e-05\n",
      "Best Batch Loss 2.6197745683020912e-05\n",
      "Min Test Loss 4.476417234400287e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 352 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.5056669478071854e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.6533157728845254e-05\u001b[0m\n",
      "Current Learning Rate 0.0006563750000000003\n",
      "Best Training Loss 4.5056669478071854e-05\n",
      "Best Batch Loss 2.6197745683020912e-05\n",
      "Min Test Loss 4.476417234400287e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 353 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.571730096358806e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.71327657578513e-05\u001b[0m\n",
      "Current Learning Rate 0.0006439999999999999\n",
      "Best Training Loss 4.5056669478071854e-05\n",
      "Best Batch Loss 2.6197745683020912e-05\n",
      "Min Test Loss 4.476417234400287e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 354 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.548956349026412e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.378689845907502e-05\u001b[0m\n",
      "Current Learning Rate 0.0006316250000000002\n",
      "Best Training Loss 4.5056669478071854e-05\n",
      "Best Batch Loss 2.6197745683020912e-05\n",
      "Min Test Loss 4.378689845907502e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEpoch:\u001b[0m 355 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.647449895855971e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.123788650962524e-05\u001b[0m\n",
      "Current Learning Rate 0.00061925\n",
      "Best Training Loss 4.5056669478071854e-05\n",
      "Best Batch Loss 2.6197745683020912e-05\n",
      "Min Test Loss 4.378689845907502e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 356 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.491975414566696e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.5526143367169425e-05\u001b[0m\n",
      "Current Learning Rate 0.0006068750000000003\n",
      "Best Training Loss 4.491975414566696e-05\n",
      "Best Batch Loss 2.6197745683020912e-05\n",
      "Min Test Loss 4.378689845907502e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 357 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.529406942310743e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.847660238738172e-05\u001b[0m\n",
      "Current Learning Rate 0.0005945\n",
      "Best Training Loss 4.491975414566696e-05\n",
      "Best Batch Loss 2.570335527707357e-05\n",
      "Min Test Loss 4.378689845907502e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 358 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.3545267544686794e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.286722469259985e-05\u001b[0m\n",
      "Current Learning Rate 0.0005821250000000002\n",
      "Best Training Loss 4.3545267544686794e-05\n",
      "Best Batch Loss 2.570335527707357e-05\n",
      "Min Test Loss 4.286722469259985e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 359 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.517004708759487e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.285569932311773e-05\u001b[0m\n",
      "Current Learning Rate 0.0005697499999999999\n",
      "Best Training Loss 4.3545267544686794e-05\n",
      "Best Batch Loss 2.570335527707357e-05\n",
      "Min Test Loss 4.286722469259985e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 360 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.4939959479961544e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.5212705774465576e-05\u001b[0m\n",
      "Current Learning Rate 0.0005573750000000001\n",
      "Best Training Loss 4.3545267544686794e-05\n",
      "Best Batch Loss 2.570335527707357e-05\n",
      "Min Test Loss 4.286722469259985e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 361 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.377849109005183e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.367511792224832e-05\u001b[0m\n",
      "Current Learning Rate 0.0005449999999999999\n",
      "Best Training Loss 4.3545267544686794e-05\n",
      "Best Batch Loss 2.508231955289375e-05\n",
      "Min Test Loss 4.286722469259985e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 362 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.332078242441639e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.465357051230967e-05\u001b[0m\n",
      "Current Learning Rate 0.0005326250000000001\n",
      "Best Training Loss 4.332078242441639e-05\n",
      "Best Batch Loss 2.508231955289375e-05\n",
      "Min Test Loss 4.286722469259985e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 363 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.263795563019812e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.333173248800449e-05\u001b[0m\n",
      "Current Learning Rate 0.0005202499999999999\n",
      "Best Training Loss 4.263795563019812e-05\n",
      "Best Batch Loss 2.508231955289375e-05\n",
      "Min Test Loss 4.286722469259985e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 364 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.3946944060735404e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.426082523423247e-05\u001b[0m\n",
      "Current Learning Rate 0.0005078750000000001\n",
      "Best Training Loss 4.263795563019812e-05\n",
      "Best Batch Loss 2.3332224373007193e-05\n",
      "Min Test Loss 4.286722469259985e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 365 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.333518154453486e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.088864443474449e-05\u001b[0m\n",
      "Current Learning Rate 0.0004954999999999998\n",
      "Best Training Loss 4.263795563019812e-05\n",
      "Best Batch Loss 2.3332224373007193e-05\n",
      "Min Test Loss 4.088864443474449e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 366 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.3331961933290586e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.369917951407842e-05\u001b[0m\n",
      "Current Learning Rate 0.0004831250000000001\n",
      "Best Training Loss 4.263795563019812e-05\n",
      "Best Batch Loss 2.3332224373007193e-05\n",
      "Min Test Loss 4.088864443474449e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 367 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.3231117160758004e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.5843808038625866e-05\u001b[0m\n",
      "Current Learning Rate 0.0004707499999999998\n",
      "Best Training Loss 4.263795563019812e-05\n",
      "Best Batch Loss 2.3332224373007193e-05\n",
      "Min Test Loss 4.088864443474449e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 368 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.211753184790723e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.167637598584406e-05\u001b[0m\n",
      "Current Learning Rate 0.0004583750000000001\n",
      "Best Training Loss 4.211753184790723e-05\n",
      "Best Batch Loss 2.3332224373007193e-05\n",
      "Min Test Loss 4.088864443474449e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 369 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.2077936086570844e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.5612603571498767e-05\u001b[0m\n",
      "Current Learning Rate 0.0004459999999999998\n",
      "Best Training Loss 4.2077936086570844e-05\n",
      "Best Batch Loss 2.3332224373007193e-05\n",
      "Min Test Loss 4.088864443474449e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 370 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.3196676415391266e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.4874850573251024e-05\u001b[0m\n",
      "Current Learning Rate 0.0004336250000000001\n",
      "Best Training Loss 4.2077936086570844e-05\n",
      "Best Batch Loss 2.3332224373007193e-05\n",
      "Min Test Loss 4.088864443474449e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 371 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.0934697608463466e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.267936310498044e-05\u001b[0m\n",
      "Current Learning Rate 0.0004212499999999998\n",
      "Best Training Loss 4.0934697608463466e-05\n",
      "Best Batch Loss 2.3332224373007193e-05\n",
      "Min Test Loss 4.088864443474449e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 372 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.114766852580942e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.329137664171867e-05\u001b[0m\n",
      "Current Learning Rate 0.00040887500000000006\n",
      "Best Training Loss 4.0934697608463466e-05\n",
      "Best Batch Loss 2.2563428501598537e-05\n",
      "Min Test Loss 4.088864443474449e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 373 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.154393536737189e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.5203174522612244e-05\u001b[0m\n",
      "Current Learning Rate 0.00039649999999999977\n",
      "Best Training Loss 4.0934697608463466e-05\n",
      "Best Batch Loss 2.2563428501598537e-05\n",
      "Min Test Loss 4.088864443474449e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 374 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.1422648791922256e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.272471051081084e-05\u001b[0m\n",
      "Current Learning Rate 0.000384125\n",
      "Best Training Loss 4.0934697608463466e-05\n",
      "Best Batch Loss 2.2563428501598537e-05\n",
      "Min Test Loss 4.088864443474449e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 375 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.1558399971108884e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.7156256187008694e-05\u001b[0m\n",
      "Current Learning Rate 0.00037174999999999974\n",
      "Best Training Loss 4.0934697608463466e-05\n",
      "Best Batch Loss 2.2563428501598537e-05\n",
      "Min Test Loss 4.088864443474449e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 376 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.071673538419418e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.15610229538288e-05\u001b[0m\n",
      "Current Learning Rate 0.000359375\n",
      "Best Training Loss 4.071673538419418e-05\n",
      "Best Batch Loss 2.2563428501598537e-05\n",
      "Min Test Loss 4.088864443474449e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 377 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.1464354580966756e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.6066168579272926e-05\u001b[0m\n",
      "Current Learning Rate 0.0003470000000000003\n",
      "Best Training Loss 4.071673538419418e-05\n",
      "Best Batch Loss 2.2563428501598537e-05\n",
      "Min Test Loss 4.088864443474449e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 378 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.0222250390797853e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m5.50056247448083e-05\u001b[0m\n",
      "Current Learning Rate 0.000334625\n",
      "Best Training Loss 4.0222250390797853e-05\n",
      "Best Batch Loss 2.2563428501598537e-05\n",
      "Min Test Loss 4.088864443474449e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 379 \u001b[31mTraining Loss:\u001b[0m \u001b[31m3.97169278585352e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.1223767766496167e-05\u001b[0m\n",
      "Current Learning Rate 0.0003222500000000002\n",
      "Best Training Loss 3.97169278585352e-05\n",
      "Best Batch Loss 2.2563428501598537e-05\n",
      "Min Test Loss 4.088864443474449e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 380 \u001b[31mTraining Loss:\u001b[0m \u001b[31m4.112111855647527e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.168500890955329e-05\u001b[0m\n",
      "Current Learning Rate 0.000309875\n",
      "Best Training Loss 3.97169278585352e-05\n",
      "Best Batch Loss 2.2563428501598537e-05\n",
      "Min Test Loss 4.088864443474449e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 381 \u001b[31mTraining Loss:\u001b[0m \u001b[31m3.9651524275541306e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.155428905505687e-05\u001b[0m\n",
      "Current Learning Rate 0.00029750000000000024\n",
      "Best Training Loss 3.9651524275541306e-05\n",
      "Best Batch Loss 2.2563428501598537e-05\n",
      "Min Test Loss 4.088864443474449e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 382 \u001b[31mTraining Loss:\u001b[0m \u001b[31m3.893287794198841e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.2173793190158904e-05\u001b[0m\n",
      "Current Learning Rate 0.00028512499999999995\n",
      "Best Training Loss 3.893287794198841e-05\n",
      "Best Batch Loss 2.2504091248265468e-05\n",
      "Min Test Loss 4.088864443474449e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 383 \u001b[31mTraining Loss:\u001b[0m \u001b[31m3.9425587601726875e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.3068946979474276e-05\u001b[0m\n",
      "Current Learning Rate 0.0002727500000000002\n",
      "Best Training Loss 3.893287794198841e-05\n",
      "Best Batch Loss 2.2504091248265468e-05\n",
      "Min Test Loss 4.088864443474449e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEpoch:\u001b[0m 384 \u001b[31mTraining Loss:\u001b[0m \u001b[31m3.9146900235209614e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m3.942936018574983e-05\u001b[0m\n",
      "Current Learning Rate 0.0002603749999999999\n",
      "Best Training Loss 3.893287794198841e-05\n",
      "Best Batch Loss 2.2504091248265468e-05\n",
      "Min Test Loss 3.942936018574983e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 385 \u001b[31mTraining Loss:\u001b[0m \u001b[31m3.893740358762443e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m3.8896392652532086e-05\u001b[0m\n",
      "Current Learning Rate 0.0002480000000000002\n",
      "Best Training Loss 3.893287794198841e-05\n",
      "Best Batch Loss 2.2504091248265468e-05\n",
      "Min Test Loss 3.8896392652532086e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 386 \u001b[31mTraining Loss:\u001b[0m \u001b[31m3.903924152837135e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m3.9627419027965516e-05\u001b[0m\n",
      "Current Learning Rate 0.00023562499999999989\n",
      "Best Training Loss 3.893287794198841e-05\n",
      "Best Batch Loss 2.2504091248265468e-05\n",
      "Min Test Loss 3.8896392652532086e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 387 \u001b[31mTraining Loss:\u001b[0m \u001b[31m3.843797094305046e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.004547372460365e-05\u001b[0m\n",
      "Current Learning Rate 0.00022325000000000017\n",
      "Best Training Loss 3.843797094305046e-05\n",
      "Best Batch Loss 2.2504091248265468e-05\n",
      "Min Test Loss 3.8896392652532086e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 388 \u001b[31mTraining Loss:\u001b[0m \u001b[31m3.7924830394331366e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m3.947782897739671e-05\u001b[0m\n",
      "Current Learning Rate 0.00021087499999999985\n",
      "Best Training Loss 3.7924830394331366e-05\n",
      "Best Batch Loss 2.0305296857259236e-05\n",
      "Min Test Loss 3.8896392652532086e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 389 \u001b[31mTraining Loss:\u001b[0m \u001b[31m3.79048942704685e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m3.995212318841368e-05\u001b[0m\n",
      "Current Learning Rate 0.00019850000000000016\n",
      "Best Training Loss 3.79048942704685e-05\n",
      "Best Batch Loss 2.0305296857259236e-05\n",
      "Min Test Loss 3.8896392652532086e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 390 \u001b[31mTraining Loss:\u001b[0m \u001b[31m3.8704554754076526e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.0577109757578e-05\u001b[0m\n",
      "Current Learning Rate 0.00018612499999999985\n",
      "Best Training Loss 3.79048942704685e-05\n",
      "Best Batch Loss 2.0305296857259236e-05\n",
      "Min Test Loss 3.8896392652532086e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 391 \u001b[31mTraining Loss:\u001b[0m \u001b[31m3.763759013963863e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.091167284059338e-05\u001b[0m\n",
      "Current Learning Rate 0.0001737500000000001\n",
      "Best Training Loss 3.763759013963863e-05\n",
      "Best Batch Loss 2.0305296857259236e-05\n",
      "Min Test Loss 3.8896392652532086e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 392 \u001b[31mTraining Loss:\u001b[0m \u001b[31m3.7248624721542e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m4.025955422548577e-05\u001b[0m\n",
      "Current Learning Rate 0.00016137499999999982\n",
      "Best Training Loss 3.7248624721542e-05\n",
      "Best Batch Loss 2.0305296857259236e-05\n",
      "Min Test Loss 3.8896392652532086e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 393 \u001b[31mTraining Loss:\u001b[0m \u001b[31m3.67496395483613e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m3.854860915453173e-05\u001b[0m\n",
      "Current Learning Rate 0.0001490000000000001\n",
      "Best Training Loss 3.67496395483613e-05\n",
      "Best Batch Loss 2.0305296857259236e-05\n",
      "Min Test Loss 3.854860915453173e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 394 \u001b[31mTraining Loss:\u001b[0m \u001b[31m3.6669076507678255e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m3.951814869651571e-05\u001b[0m\n",
      "Current Learning Rate 0.0001366249999999998\n",
      "Best Training Loss 3.6669076507678255e-05\n",
      "Best Batch Loss 2.0305296857259236e-05\n",
      "Min Test Loss 3.854860915453173e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 395 \u001b[31mTraining Loss:\u001b[0m \u001b[31m3.668965655378997e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m3.884637771989219e-05\u001b[0m\n",
      "Current Learning Rate 0.00012425000000000007\n",
      "Best Training Loss 3.6669076507678255e-05\n",
      "Best Batch Loss 2.0305296857259236e-05\n",
      "Min Test Loss 3.854860915453173e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 396 \u001b[31mTraining Loss:\u001b[0m \u001b[31m3.6340166843729094e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m3.938763256883249e-05\u001b[0m\n",
      "Current Learning Rate 0.00011187499999999979\n",
      "Best Training Loss 3.6340166843729094e-05\n",
      "Best Batch Loss 2.0222385501256213e-05\n",
      "Min Test Loss 3.854860915453173e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 397 \u001b[31mTraining Loss:\u001b[0m \u001b[31m3.61728343705181e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m3.880985241266899e-05\u001b[0m\n",
      "Current Learning Rate 9.950000000000005e-05\n",
      "Best Training Loss 3.61728343705181e-05\n",
      "Best Batch Loss 2.0222385501256213e-05\n",
      "Min Test Loss 3.854860915453173e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 398 \u001b[31mTraining Loss:\u001b[0m \u001b[31m3.5930857848143205e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m3.804016159847379e-05\u001b[0m\n",
      "Current Learning Rate 8.712499999999977e-05\n",
      "Best Training Loss 3.5930857848143205e-05\n",
      "Best Batch Loss 2.0222385501256213e-05\n",
      "Min Test Loss 3.804016159847379e-05 \n",
      "\n",
      "\u001b[31mEpoch:\u001b[0m 399 \u001b[31mTraining Loss:\u001b[0m \u001b[31m3.574501897674054e-05\u001b[0m\n",
      "\u001b[32mTest Loss \u001b[0m \u001b[32m3.822863436653279e-05\u001b[0m\n",
      "Current Learning Rate 7.475000000000003e-05\n",
      "Best Training Loss 3.574501897674054e-05\n",
      "Best Batch Loss 2.0222385501256213e-05\n",
      "Min Test Loss 3.804016159847379e-05 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "t1 = time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    loss_b = 0\n",
    "    \n",
    "    in_sample_batch, out_sample_batch = shuffle_batch(model_parameters_training, implied_vols_training, batch_size)\n",
    "    \n",
    "    for in_sample, out_sample in zip(in_sample_batch, out_sample_batch) :\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out_sample = torch.reshape( out_sample , (batch_size, 1, 11, 8 ) )\n",
    "\n",
    "        output = NN_cal(out_sample)\n",
    "        \n",
    "        loss_batch = loss(output,in_sample)\n",
    "\n",
    "        loss_b += loss_batch/nb_batch    \n",
    "        \n",
    "        loss_batch.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        min_loss = min(min_loss,loss_batch)\n",
    "      \n",
    "    LOSS_B.append(loss_b.item())\n",
    "         \n",
    "    output = NN_cal( torch.reshape( implied_vols_test , (implied_vols_test.shape[0], 1, 11, 8 ) ) )\n",
    "    \n",
    "    test = loss(output,model_parameters_test)\n",
    "    LOSS_test.append(test)\n",
    "    \n",
    "    if (test<=min(LOSS_test)):\n",
    "        with open(NN_cal_model_File, 'wb') as file:  \n",
    "            pickle.dump(NN_cal, file)      \n",
    "    \n",
    "    print(colored(\"Epoch:\",'red'), epoch,colored(\"Training Loss:\",'red'), colored(loss_b.item(),'red') )\n",
    "    print(colored(\"Test Loss \", 'green'), colored(test.item(),'green'))\n",
    "    print(\"Current Learning Rate\", training_rates[-1] )\n",
    "    print( \"Best Training Loss\", min(LOSS_B))\n",
    "    print('Best Batch Loss',min_loss.item()) \n",
    "    print(\"Min Test Loss\", min(LOSS_test).item(), '\\n')\n",
    "   \n",
    "    if (epoch<tau):\n",
    "        rate = (1-(epoch/tau))*learning_rate + (epoch/tau)*min_rate \n",
    "        optimizer.param_groups[0]['lr'] = rate\n",
    "        training_rates.append(rate)\n",
    "      \n",
    "    else:\n",
    "        rate = min_rate \n",
    "        optimizer.param_groups[0]['lr'] = rate\n",
    "        training_rates.append(rate)\n",
    "    \n",
    "t2 = time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss evolution during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test sample loss (green) is computed at each epoch, however the network's weights are not updated according to this loss. \n",
    "\n",
    "It illustrates how well the network's solution generalizes to unseen data, and allows us to check for overfitting and learning rates adjustment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mFinale Validation Loss\u001b[0m \u001b[31m3.574501897674054e-05\u001b[0m\n",
      "\u001b[32mBest Validation Loss\u001b[0m 3.574501897674054e-05\n",
      "\u001b[31mFinale Test Loss\u001b[0m 3.822863436653279e-05\n",
      "\u001b[32mBest Test Loss\u001b[0m 3.804016159847379e-05\n",
      "Data Size torch.Size([33792, 4])\n",
      "Training Time (h) 0.14192170249091254 \n",
      "\n",
      "Evolution during training\n",
      "Quadratic loss evolution during training\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rates evolution during\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print( colored('Finale Validation Loss','red') , colored(loss_b.item(),'red'))\n",
    "print( colored('Best Validation Loss', 'green'), min(LOSS_B))\n",
    "print(colored('Finale Test Loss', 'red'), LOSS_test[-1].item()) \n",
    "print(colored('Best Test Loss', 'green' ), min(LOSS_test).item() )\n",
    "print('Data Size', model_parameters_training.shape)\n",
    "print(\"Training Time (h)\", (t2-t1)/3600 ,'\\n') \n",
    "\n",
    "print( \"Evolution during training\")\n",
    "\n",
    "\n",
    "print( \"Quadratic loss evolution during training\")\n",
    "plt.plot(LOSS_test[1:],color='green')\n",
    "plt.plot(LOSS_B[1:],color='red')\n",
    "plt.show()\n",
    "\n",
    "print(\"Learning Rates evolution during\")\n",
    "plt.plot(training_rates)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of the inverse map as learned by the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative error here is computed as \n",
    "$$\\frac{\\left|\\theta^{N N}-\\theta \\right|}{\\left|\\theta\\right|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_RE(pred, true):\n",
    "    return (torch.mean( abs(pred-true)/abs(true) , dim=0))\n",
    "\n",
    "def RE(pred,true):\n",
    "    return( abs(pred-true)/abs(true) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = NN_cal( torch.reshape( implied_vols_test , (implied_vols_test.shape[0], 1, 11, 8 ) ) )\n",
    "\n",
    "Av_RE = average_RE(output, model_parameters_test)\n",
    "RE_ = RE(output, model_parameters_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average relative error for all parameters  0.011722007766366005\n"
     ]
    }
   ],
   "source": [
    "XI = model_parameters_test[:,0]\n",
    "NU = model_parameters_test[:,1]\n",
    "RHO = model_parameters_test[:,2]\n",
    "H = model_parameters_test[:,3]\n",
    "\n",
    "\n",
    "RE_xi = RE_[:,0]\n",
    "RE_nu = RE_[:,1]\n",
    "RE_rho = RE_[:,2]\n",
    "RE_H = RE_[:,3]\n",
    "\n",
    "print( 'Average relative error for all parameters ', torch.mean(Av_RE).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEPCAYAAABShj9RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXzU9Z348dd7JvfFmXAEQlABBVpBI1gVBa2KRxHUtT5Auv7qAdtVt9Wu0sKvXXfR1nb159LahXq0XqBuFUs5VpS1bqDKIaAcopyGO+FIwpF73r8/5mAymclMkkkmM7yfjwcPMvP9zsx7Jvm+5/t9fy5RVYwxxsQ/R6wDMMYYEx2W0I0xJkFYQjfGmARhCd0YYxKEJXRjjEkQltCNMSZBWEI3xpgEYQndGGMShCV0YzxE5BkR0Wb+DY51jMY0JynWARjTiawH3gD+DnAC24DPgQNAhap+FcPYjAnLEroxZ2QCtwPvAD9V1R0xjseYFhGby8UY8JRTtgDvqOp3Yx2PMa1hNXRj3K7HfcU6L9aBGNNaltCNcRPP/zeLiDOmkRjTSlZyMQYQkV7AR8AQ4BCwCigBjqrqE7GMzZhIWUI3xkNEvg38P2C4391rVXVUjEIypkWs5GLOeiKSKSILgfdxd1H8HnABkA1cGsvYjGkJ67ZojLshdCJwr6q+GOtgjGktK7mYs5qI5OOulf+Pql4b63iMaQsruZizXRHu42B9rAMxpq0soZuzXYXn/5tFpHtMIzGmjazkYs5qIuLA3V3xCtzJ/X1gF3AEKFbVT2IYnjEtYgndnPVEJBWYDkwChgJdgWRAgTtU9U8xDM+YiFlCNyYIEbkG+AB4U1XvjHU8xkTCaujGBHfA839JTKMwpgUsoRsTQETuA5YBx4HfxTgcYyJmJRdjAojIetzzufyzqm6JdTzGRMoSujHGJIiYDf3v2bOnFhYWxurljTEmLn366adHVDU32LaYJfTCwkLWrVsXq5c3xpi4JCJfh9pmjaLGGJMgLKEbY0yCsIRujDEJwhK6McYkCEvoxhiTIOI+oZdWVnPHvI8pPVEd61CMMSam4j6hz1mxnbV7jjHng+2xDsUYY2IqbtcUHTJrGTX1Lt/t11aX8NrqElKTHHw5+4YYRmaMMbERt2foxY+OY8KIvqQlu99CWrKDW0b0pfixcTGOzBhjYiNuE3peThrZqUnU1LtITXJQU+8iOzWJvOy0WIdmjDExEbclF4AjJ2uYMnoAk0cVMH9NCWXWMGqMOYvFbLbFoqIitblcjDGmZUTkU1UtCrYtbksuxhhjGrOEbowxCcISujHGJAhL6MYYkyAsoRtjTIKwhG6MMQnCEroxxiQIS+jGGJMgLKEbY0yCsIRujDEJwhK6McYkCEvoxhiTICyhG2NMgrCEbowxCcISujHGJAhL6MYYkyAiSugiMl5EvhSRHSIyI8j2sSJSISIbPf9+Fv1QjTHGNCfsEnQi4gSeA64F9gFrRWSRqm4N2LVYVW9uhxiNMcZEIJIz9FHADlXdpaq1wBvALe0bljHGmJaKJKHnA3v9bu/z3BfoWyLymYgsE5FhwZ5IRO4XkXUisq6srKwV4RpjjAklkoQuQe4LXFl6PTBAVS8EfgO8G+yJVPX3qlqkqkW5ubkti9QYY0yzIkno+4D+frf7AQf8d1DVSlU96fl5KZAsIj2jFqUxxpiwIknoa4FBIjJQRFKAO4FF/juISG8REc/PozzPezTawRpjjAktbC8XVa0XkQeA9wAn8JKqbhGR6Z7tc4HbgX8QkXqgCrhTVQPLMsYYY9qRxCrvFhUV6bp162Ly2sYYE69E5FNVLQq2zUaKGmNMgrCEbowxCcISujHGJAhL6MYYkyAsoRtjTIKwhG6MMQnCEroxxiSIuEzopZXV3DHvY0pPVMc6FGOM6TTiMqHPWbGdtXuOMeeD7bEOxRhjOo2wQ/87kyGzllFT7/Ldfm11Ca+tLiE1ycGXs2+IYWTGGBN7cXWGXvzoOCaM6EtasjvstGQHt4zoS/Fj42IcmTHGxF5cJfS8nDSyU5OoqXeRmuSgpt5FdmoSedlpsQ7NGGNiLq5KLgBHTtYwZfQAJo8qYP6aEsqsYdQYYwCbbdEYY+KKzbZojDFnAUvoxhiTICyhG2NMgrCEbowxCcISujHGJAhL6MYYkyAsoRtjTIKwhG7axdixY+nWrRs1NTWxDqVdHDx4kAkTJtC3b19EhD179jS7f2FhIenp6WRlZZGVlcV1113n26aqPPHEExQUFJCTk8Odd95JZWWlb/uvf/1revbsyfDhw9m8ebPv/lWrVjFx4sSovzcTvyyhm6jbs2cPxcXFiAiLFi2K6nPX19dH9flay+FwMH78eN5+++2IH/OXv/yFkydPcvLkSZYvX+67/5VXXuHVV19l1apVHDhwgKqqKh588EHA/cXx4osvsmvXLqZPn86MGTMA9+fwyCOP8Oyzz0b3jZm4ZgndRN0rr7zCpZdeyt13383LL79MTU0NXbt2bXR2WVZWRnp6OqWlpQAsXryYESNG0LVrVy677DI+//xz376FhYU89dRTfPOb3yQzM5P6+np++ctfcu6555Kdnc3QoUNZuHChb/+GhgYeeeQRevbsycCBA/ntb3+LiPi+DCoqKrjnnnvo06cP+fn5zJo1i4aGhha9x169evGDH/yASy65pC0fFeBO9Pfccw/9+/cnKyuLxx57jDfffJPTp09TUlLCyJEjycnJ4dvf/ja7du0C4Nlnn2XChAkUFha2+fVN4rCEbqLulVdeYcqUKUyZMoX33nuP8vJybr31VhYsWODb56233uKqq64iLy+P9evX8/3vf5958+Zx9OhRpk2bxoQJExqVaxYsWMCSJUsoLy8nKSmJc889l+LiYioqKvj5z3/OXXfdxcGDBwF4/vnnWbZsGRs3bmT9+vW8++67jeL7+7//e5KSktixYwcbNmxg+fLlvPDCCwCsXLmSrl27hvy3cuXKVn8uU6ZMITc3l+uuu47PPvvMd7+q4j8Fh6pSU1PD9u3bOe+889i0aRPl5eV88MEHDBs2jL179/LGG2/w4x//uNWxmATl/WPq6H8XX3yxmsRTXFysSUlJWlZWpqqqQ4YM0WeeeUbff/99HThwoG+/yy67TF9++WVVVZ0+fbrOmjWr0fMMHjxY//rXv6qq6oABA/TFF19s9nUvvPBCfffdd1VVddy4cTp37lzftvfff18Braur00OHDmlKSoqePn3at33+/Pk6duzYVr3furo6BXT37t3N7rdy5Uo9ffq0njp1Sp988knt1auXHj9+XFVVn3/+eR00aJDu3r1by8vL9Tvf+Y4C+re//c0X38iRI3X8+PG6Z88enTRpkn7wwQf6xhtv6JVXXqkTJkzQvXv3tip+E3+AdRoir0aUfIHxwJfADmBGM/tdAjQAt4d7Tkvoienee+/VG2+80Xf78ccf1wsvvFAbGhq0d+/e+sknn+iePXs0IyNDKysrVVX1hhtu0PT0dO3SpYvvX3p6us6fP19V3Ql9+fLljV7n5Zdf1gsvvNC3v9Pp1BdeeEFV3V8iS5Ys8e27bds2X0JfvXq1ikij18rOztahQ4e26v1GmtADDRkyRBctWqSqqg0NDfqzn/1MBwwYoPn5+frMM88ooCUlJU0et3jxYr3zzjv16NGj2q9fP62oqNBXX31Vv/vd77YqfhN/mkvoYafPFREn8BxwLbAPWCsii1R1a5D9ngLei8KFQ4uVVlbzwIIN/HbySJsfPUaqqqp46623aGhooHfv3gDU1NRQXl7Opk2buOOOO1iwYAG9evXi5ptvJjs7G4D+/fszc+ZMZs6cGfK5RcT389dff819993HihUr+Na3voXT6WTEiBG+skWfPn3Yt2+fb/+9e/f6fu7fvz+pqakcOXKEpKSmf/7FxcXccEPo1a+WLVvGmDFjIvxEQhMRX7wOh4PHH3+cxx9/HIDly5eTn59Pfn5+o8dUVVXx05/+lGXLlrF9+3b69+9PTk4Ol1xyCU8++WSbYzLxL5Ia+ihgh6ruUtVa4A3gliD7PQi8DZRGMb6I2Tqjsffuu+/idDrZunUrGzduZOPGjXzxxReMGTOGV155hcmTJ/Pmm2/y+uuvM3nyZN/j7rvvPubOncvq1atRVU6dOsWSJUs4ceJE0Nc5deoUIkJubi4Af/jDHxo1uN5xxx38x3/8B/v376e8vJynnnrKt61Pnz5cd911PPLII1RWVuJyudi5cycfffQRAGPGjPH1RAn2zz+ZV1dX++r8NTU1VFcHn5u/pKSEVatWUVtbS3V1Nb/+9a85cuQIl19+OQDHjh1j586dqCpbt27l4Ycf5mc/+xkOR+PDc/bs2dx999307duXgoICvvzySw4fPsyHH37IOeecE/HvySSwUKfueqaMcjvwgt/tqcBvA/bJBz4CnMAfCVFyAe4H1gHrCgoKonL5MXjmUh3w2OIm/wbPXBqV5zeRu/766/Xhhx9ucv+bb76pvXr10rq6Oj333HO1W7duWlNT02ifZcuWaVFRkXbp0kV79+6tt99+u68kM2DAAH3//fcb7f/Tn/5Uu3Xrpj169NAf/ehHeuWVV+rzzz+vqu4yyA9/+EPt3r27FhYW6jPPPKNJSUnqcrlUVbW8vFynT5+u+fn5mpOToyNGjNAFCxa0+P0CTf55TZs2TadNm6aqqps3b9ZvfOMbmpGRod27d9err75a165d69v3yy+/1MGDB2t6eroWFBTo008/3eS1tm3bpkVFRVpXV+e771e/+pX26NFDL7jgAv38889bHL+JTzRTcgm7wIWI/B1wvare67k9FRilqg/67fNfwNOq+omI/BFYrKp/au55o7XARWllNbOXfsHyLYeornORluzg+mG9mXnTBVZ6MYC7TDJ9+nS+/vrrWIdiTJu1dYGLfUB/v9v9gAMB+xQBb4jIHtxn9L8TkQ4ZwmbrjJpAVVVVLF26lPr6evbv38/jjz/OpEmTYh2WMe0ukoS+FhgkIgNFJAW4E2g0/E9VB6pqoaoWAn8CfqCq7zZ9qvbhXWd04Q8uZ8roAZSdTMzh5iYyqsrPf/5zunXrxsiRI7ngggv413/911iHZUy7C9vLRVXrReQB3L1XnMBLqrpFRKZ7ts9t5xjDmjf1zNXH7InDYxiJ6QwyMjJYu3ZtrMMwpsOFTegAqroUWBpwX9BErqp3tz0sY4wxLRWXQ/9LK6u5Y97HlJ4I3k3MGGPORnGZ0K3PuTHGNBVRyaWzGDJrGTX1Lt/t11aX8NrqElKTHHw5O/ToPmOMORvE1Rl68aPjmDCiL2nJ7rDTkh3cMqIvxY+Ni3FkxhgTe3GV0K3PuTHGhBZXCR2a73NujaXGmLNZ2KH/7SVaQ//9zVq4idfXlDBlVAGzJ30jqs9tjDGdQXND/+OqUTQUayw1xpg4LLkEY42lxhiTIAndGkuNMSZOE3qwxk+boMsYc7aLy0bRR97ayNvr93PbRfk8fceIKEdmjDGdV8I0igY2fr69fj9vr99vjZ/GGEOclVxCXU3E6irDGGM6k7hK6Csfu5oBPTIa3VfYI4OVM66OUUTGGNN5xFVCz8tJo8HlPhtPdgoADS613izGGEOc1dABhvXNYeyQPCaPKmD+mhLKbJi/McYAcdrLxRhjzlbN9XKJq5KLMcaY0CyhG2NMgrCEbowxCSKhErrNh26MOZslVEK3xaONMWezuOu2GIzNh26MMQlyhm7zoRtjTIIkdJsP3RhjIkzoIjJeRL4UkR0iMiPI9ltE5HMR2Sgi60TkiuiHGpy3IXR/eZXNh26MOauFraGLiBN4DrgW2AesFZFFqrrVb7cVwCJVVRH5JvAWcH57BOxVWlnNAws20L9rOmv3HHMvDD1xOIDvf2OMOZtE0ig6CtihqrsAROQN4BbAl9BV9aTf/plAu88ncOkvVuBSWOO5bQ2hxpizXSQJPR/Y63d7HzA6cCcRmQT8AsgDbgr2RCJyP3A/QEFBQUtjBZr2aPF3y4i+zLzpglY9rzHGxLtIaugS5L4mZ+CqulBVzwcmAv8W7IlU9feqWqSqRbm5uS2L1COwRwuAwxNhuIbQ0spqJj23ionPrbLBR8aYhBNJQt8H9Pe73Q84EGpnVf1f4FwR6dnG2ILy79HiTeTjh/fhrkvDN4TOWbGdDXvL2bi33AYfGWMSTiQll7XAIBEZCOwH7gQm++8gIucBOz2NohcBKcDRaAfrdeRkDVNGD2g0J3pzDaHByjRWczfGJJqI5kMXkRuBZwEn8JKqPiEi0wFUda6IPAZ8D6gDqoB/VtWVzT1nR86HXlpZzf/982Y+2HqYBs/bdQhcO7QX/zZxuPVXN8bEjebmQ49o6L+qLgWWBtw31+/np4Cn2hJke8rLSaNnVqovmQO4FHKzUi2ZG2MSRkLM5RKJIydr6N8tnW/26wrA5/vKbfCRMSahxGVC9w4q+u3kkaD4fm7ubHve1KBXKMYYkzDici4X/2lybcpcY4xxi6tFopsbVORlvVaMMYksYRaJDhxU5JQzg4raOmWurXZkjIl3cZXQA6fJbVB3b5VoTJlrpRtjTLyLu0ZR/0FF015dR4NL6Z6ZwpDeOa3qtWKrHRljEkVcnaGDu7fK7InDGdo3h+LHrubq8/PYcrCS9GRHq3qy2GpHxphEEXdn6F6Rnln7d3EMVo6x1Y6MMYki7s7Qvbxn1t5GUYcQ9Mw6ktq4t4xjqx0ZY+JZXHVb9BeqC6P3DD3UdgFWz7zGzsCNMXEpYbotepVWVofsj+79ggpWGy/skQGC9WQxxiSkuKyhz1nhTshZqU5O1jT47i/skcFb078FNK6NA1TXudhz9DRgPVmMMYkprhJ6YBnFP5kDNLi0USnFWxsfP6w3s97dRMmx07jUfbZ+/bDetlydMSahxFVCL350HLOXfsHyLYeornOvWFTQPYPZk4bz35sPUxYwytO/G+Pl5/Xk6zUl1pPFGJOw4iqhB3YxrG1wccV5PbnivFyuOK/5NUqDrXJkjDGJJK4SOjROzL/5cDsL1u5l8qUFDO3TBQjd79z/bL255eqMMSZexV0vF/+RojsOn6TBpfzTgo2+7TYnizHmbBWX/dALZyyJeF/ryWKMSSQJ1w996UNXkN81vdF9fXLSGDsk1+ZkMcacteIyoQ/t24WMFGej+7LSksjvmm5zshhjzlpxmdC3Hqhge+lJUpzCE5OGM7hXFhVVdY3mZLl1ZD5LNh2k9ES1LV5hjDkrxF0vF4B/esPdCFrboHxxoJLlP7qqyT7pyU7Kq+p8jaPehtLZk77RobEaY0xHiatG0eYaQ8NNyhVsX2OMiTcJ0yi69KEr6N0ltcn9Ywf3ZOEPLuOOeR+z8AeXtdu6o/6sjGOM6WziKqFP+t3fOFTRdK7y/91+hPmrS1i75xjzV5e027qj/qy/uzGms4mo5CIi44H/AJzAC6r6y4DtU4DHPDdPAv+gqp8195ytKbmUVlYz5lcfhi2pgPusfPGDY5j2qvs15k0t8g35b81SdV7h5mE3xpj21FzJJWyjqIg4geeAa4F9wFoRWaSqW/122w1cparHReQG4PfA6LaH3lheThq3X9yP11eXhNzHO5PitCvP4V/+soW3f3CZ74w8GkP+AycIs5kbjTGdRSQll1HADlXdpaq1wBvALf47qOrfVPW45+YnQL/ohnnGkTDLw3nLKvNXl7Bm9zFunrMyqnVuW4PUGNNZRdJtMR/Y63d7H82ffd8DLAu2QUTuB+4HKCgoiDDExuZNLWLrgQpunLMy6HZV9wIWXqUnahj1xApSnMKIgm4hF4tuCZu50RjTGYWtoYvI3wHXq+q9nttTgVGq+mCQfccBvwOuUNWjzT1va+dyCdct8aZv9GbZ5kO4Qrytu0YXWF90Y0zcamu3xX1Af7/b/YADQV7km8ALwC3hknlbFD86zr02aAhLNoVO5uA+ey+csYQhs4JeRBhjTNyKpOSyFhgkIgOB/cCdwGT/HUSkAHgHmKqqX0U9So9IBg2lJTvolpFCWpKD3Z41RAGcDqHBpdaIaYxJWGHP0FW1HngAeA/4AnhLVbeIyHQRme7Z7WdAD+B3IrJRRFo3L24YxY+O47phvZrc7/QMHPI2UpZWVjdK5uBebxSwRkxjTMKKaC4XVV0KLA24b67fz/cC90Y3tKbyctLIzWo6UrTBU2JxqTJl9AD2HTtFTkZKwNqjmcyeOJz/3nLIGjGNMQkp7ibnWrAmeB90h8CqGVf7zrxnLtwUsPZoD64Y1JMrBvX0DduPRo8XY4zpLOJq6D/AJz+5hgkj+vrmZ/G6YVjvRsnZfyrdKaMHUObXf72lw/Zt3hZjTDyIuzN078CewJ4sn++vaHT7324ZzgMLNtAzO8U3QjSwUfW11SW8trqEFKeDEQVdQ56x+38BWJdHY0xnFVfT50L4ni7eOVVmLdzE62tKmDLqTL/z0srqoMP2k0R4Z+P+Rvs291o2b4sxJlaa64cedwm9tLKaUU+uCLpNgGSnUNvQ9D15R4r265bOwg37SXE6Qn4xeBN2qC+AmTddYLV3Y0xMJMx86OAuuQzsGXxgUWGPDFY+dnWj+dBF4PphvfjOhX1Zu+cYa3cf89XWb7son95d0kIuLG3zthhj4klc1dDDlVt2Hz3NqCdX4BBQ3D1fXArvbTns22fv8Spe++Rr/mvdXr6cfQMzF25i/pqSkAk7GvO2lFZW88CCDdarxhjTruIqoQdOXdscVXdSDxQ4UjRcwvafO7210+9ao6oxpiPEVUL3L4GkJDmoDXG27lJ3PR1xJ3anuAcfCU1HikYjYYcSqleNNaoaY9pD3NXQj5ysQSBkMvdS3Mnc4UnmXdKSUGBQblajPuntqfjRcY3q+dFc09QYYwLFXUKfN7WIT35yTZOBRaF4+6tXVNcD8FXpSd7bcpjBHTDbojWqGmM6UtwldHAnyokj8iPa1xmQ+J2eb4LvfKNPtMMKqrkRq8YYE01x1w/d6+6X1vDxrqNNer0IkJrsCNto6mX1bGNMPEmofuhe3TNTqKl3kZXqbHS/At0zUrjtony+dW73kI+3erYxJtHEVS8XaNpz5GRNg+/ngu4ZlBw7jdMhLP78YMg+6wJU11k92xiTWOLuDL25ClHJMfeiFnuPV/mSubeE7t+IqsCgvDO9XWw2RWNMIoi7hL7ysXH065Ye0b4De2T4Mnrg7IzbPb1dhsxa1uLpdI0xpjOKu4Sel5PG8VO1Ee27++hpVN0TdgV+CTjEPc9LTb2L11aXoNp0AWk7czfGxJO4SuhDZi2jcMYSTtU2hN8Zd5fF8cN6sWrG1Vw1OLfRNpfCrSPzGw38SU0SemSmsPAfLwNavhCGMcbEUlx1WwyczjYSt43M5+nvjuCcnyxpUnYBfBN5JTvOTLvrnSogkHVxNMbEWsJ0W8zLSWPxZwciTuYAa/YcA2Dxg1fQPTOF1KTGw/CvHNQTB43nUA9M5tbF0RgTD+IqoQNcOahnxI2i4O7xUjhjCd/5zUqOnaptMgz/413HaAhylSK4a+w2ZP/sYW0mJt7FXUL/eNcx9h2vCrrNO8y/sEeGb4i/l/9Zd029CwHKTtb4JtAKnCKgT5e0RkP29x0/HdOD3ZJN+7M2ExPv4i6hFz86jt5dzpwpC+46eIpTePmeUTgE9hw9TUOwgrln/27pySx+6ArmTS3yTaDln/DTkh0M7pXF7InDGdo3h9kTh9OvW0bIg70jkq0lm/bjbWwP1dvJmHgRV42iXjMXbuL11SW+24PystheepIemSn06ZLG4RM1lJ04MwlWerKDas9ZuTfP3zoyn33lVWz4+hjBSvIOgV2/uCmihaKDLUgdLbZQdfuztWNNPGmuUTTuh/6De5AQwNFTtRwN0ke9ypOx/b+63tmwv9nXcSkUzlhCitPBhBF9gx7sHbGAReAqTYErLpm2s2mOTaKIqOQiIuNF5EsR2SEiM4JsP19EPhaRGhH5cfTDPCNw0Yhw2l5T0pAHe0csYGHJpmPYNMcmEYQ9QxcRJ/AccC2wD1grIotUdavfbseAh4CJ7RJlgE92Ho2466ILd3nlvS2HIh6Q5E9VQ6472lHJNhoLVZvmtedShMZ0lEhKLqOAHaq6C0BE3gBuAXwJXVVLgVIRualdovQzZ8V2Sk+07OwpXHmlORNG5PP0HSN8twMP9o5ItpZsjDGRiCSh5wN7/W7vA0a35sVE5H7gfoCCgoIWPTZU42B7e3v9ft5ev79RXby0spoHFmzgt5NHWrI1xnQakZSYg63e2aquMar6e1UtUtWi3Nzc8A/w09LaeWv1yk5l7OBcX7/0FGfj+V3AuhAa42XjIzqXSLLjPqC/3+1+wIH2CSc0/3p1e+iVkwrAxYXd6J6Z4uuXXtugHD1Vy/xPSlrVX9n+4E0is5ObziWSkstaYJCIDAT2A3cCk9s1qhC89erxw3px9x/WUh9i8FBrHK501+WXbjoUdPtrnn7vDnFPx1tTr6QmCeOH92m2C6H/H3y0+6gbEysd0WXXtFxEA4tE5EbgWcAJvKSqT4jIdABVnSsivYF1QA7ujiUngaGqWhnqOVs7sChWtfTUJAfjh/fGKdKokfWu0WcGE/nX1sc89aENCDIJywZjxU6bZ1tU1aWqOlhVz1XVJzz3zVXVuZ6fD6lqP1XNUdWunp9DJvO2KH50HNcN6xWV58pMcYbfyaOm3sXizw406THz2uoSX8nF/2y8I/qoGxMrNj6ic4q7kaJ5OWnsLjsVdJtToH/3DPYcPR32eZIELhnYnXuvGMhdL64Ju78AWWlJXHpODz76qqzRWcmyzYconLHEt6/38tM717r9wZtEZOMjOp+4Sujhyi0NSkTJHKBeITPZyfdeCp/MwZ2YK6rqeW/L4SbT6q70DM9f8vlBGlyK0yHc/M0+VJyupV/3TPuDNwnJuux2PnE126K3jOFdpKKtlmw+FHQVo3BUwaXKrSPzWbLpIFc89T8s2njAN8Njg0v588YDfLzrGLMnDqdnVgpfHT7Bv7XDH731ojHGeMVVQveuWBSLRlF/1w/rxfD8LgCUV9WRnZZMXnaqr++6U6BnVgpJTmHrwYp27dpl3cZiw75ITWcUd9Pn3lWXOMcAABHPSURBVP3SGv76VVk7RBQd3lJMqHVJvfu0pqdLW3vR+D/eavlt055TJpvEE81jL2HWFAX44/dHxTqEZnmTbLBkHtjTpaVneW3tRWNn821ni2EkjmhfZTX3fB117MVdQgcoGtAt1iG0WJK4k31tvYtr/v0jth6s4Kn/3saa3cd4atk2SiurmfTcKiY+t6rJH8TgmU2TyKgnV/jKT+F60VgSip5gX6TXD+vF0L45rUoMiVy66ezvLdIkG+n7CPZ8oY69we107MVdySVWA4vaSoAplw5g4fp9YafxvWt0AQ9dM4j7X/0UEeiTk8bSzYdwOoQGl/q6SwbrRePf88DLBoFE18yFm5i/poQUp4PaBhfn5Waxo+xkq8oviVy66azvraWrgIV7H4NnLqO2IfjzFQf0gBPcPeZuG5nP098d0eQxkWiu5BJ3Cd2bnBZt7PDpZDqVHpkpFPTIYN7UiyNKyoFJKBYHWaLU8Ke9uo7c7DTeXFtCXZDaWiRtJPG0tGBLf2+d/b1FeoIT6ft4+M2NvLNhf5MTrpk3XRCyrSvUc0UioWroY3714VmdzMcOySW/axpHT9WyoaScm+esjOiSNlor8rTlMrotdcS2Xr5H8/J/3tQiZk8czqrHrm5UfnEIjB/WK6LRwN7SjX/PqM46krilv7fAslRqUtMZS2Mp0lGu4dqpvOUU7+hxb7fl6rrGq5r17pKGI2DO2tSk9hk5HncJ3f0BpeIM/IQ6gezUyKcSaK2/flnG/vIzSan0RA2jnljRpB5eWlnNxOdWMel3q9h6oILjp+t46JrzGNo3h9kThwctzYRTWlnNzb9ZyZrdLUvK0ajht7VRqT0apfwTg1Pc69DuLDsV0Vms98TEe4LfoPDnjQcY89SHnab23NrfW9OEeWbG0s4ikhOccIk/MOE7xX3CddvF+b7ny8tJ45rz81DwJXWnQG1D+4wcj7uSC8A5P1nSqgFBic7/8s17GQgwKC/yGm+oy+tILj9LK6uZ9uqnKPD7750pBUVyiRvsdUsrqxn95Iqgk+9Heqna3pf/of4Wwz1/aWU1E55bSdmJWt/o4h6ZyfTtmsG5uZm8s2F/s7+vjihftaXtZdqr6/hgaykNQfJLZym9RMJbXgvVThVJKfPul1az9eAJCrpnUFldR4+sFM7NzQ7Z5hVOQpVcAK4clEv/bunxGXw7UppeBgJsLz0Z9gzLe1b4L4u2uHveLN3m2xYqKTqERpeMc1ZsZ8PecjbuLeeXS7f5zjIDz3Sq61x8vPNoo+cKPIP2Xg0oUNgjo9numc2d0bb3JGmLH7yC7pkpvtHLkT6/+8ytFy5VUpMcuFQpPVHLxr3lvL1+f9jfV0d0g4vk9xZMaWU1x0/X8ZcHL4/7Ceq85bVQo72PnKzh1pH5DMrL4taL8oOe6ffrlkHZyRrO753N8h9dxYL7vtXqq+Rw4jIn/vH7o9hfXkX89XVpP4U9Mlj52DhczVxxJTuFrhnJFPbMYNLv3N0jvclw9JMrWLP7GEs3u+eDf3vDfl8yKX50XNBlq1wKY576sFG3Sq93Nuxnze5jXPrkCqDxJe6gvCxKT9Qw54PtIS/rRz25wrd27J6jp32LggerdzaX3CKtl4Yrc4TaPn91CcdO1UY86+DWAxV84+fvsfVghe8zcakS6tcW2CUykjJINEs2oX5vzb3OL5e5u+O+WLw7Kp99JPu0dxtLqL+xeVOLSE92suVgJelJzkZJOhbdheOy5ALuy5iPvjrSurXwElCKU3j3Hy/nxjkrI37MXaPdl5HNla9Skhx8NfsGxv36r+w+emaWSwFys1Po2zWDHpnJrNjW/Ohd7xlsqDP9FM8ZYCgCvHrPaN7ZsI8Pt5Xy+r2juWnOyojKMXe/tIZN+yvomZ3K8PwcTlbXNzk7Ctc1LXB7c1ctVw7qyek6V9ByyLhff8juo6cZ2CODD/95nK9MlZuTyv96ZvH0Po9Cky6RwcogYwfncqiyxlfmCvZe2lKiCfVeneJe/GDSiHz2lVexseQ4tUF6/TgEFj84JmTX2ki6N7b099OS9+y9Giw94T7b3lde5XtMcyU7CP737P37K62sZtafN/P+1sOoEvR31RoJ1W0RaDRV7dlOcB8woaYZ6AxSkxws/MfL6JmZyq3/+Tf2Ha8C3Anh5gv78pfPDoRtE+mRmcKyH45hzgfbeX1NiTvRlZ4kxe/ASk1ykJWaxKv3jmJony6+x85auMl39XDbRfnsLDuFCMybenHYKRRCHdDJDiEnPZmTNXXU1DfuquaN0T+5NPc3K3ImcQvuKx+Hp5E1WFy3X9wvaD94B8H/DlKTHPzdxf0axdSSBB/4JdJSAqyeeU2T14mkfSPcPs1tD3zPwd5XqDYah8CuX9zUbDsCiruP+Wfuxm3v37N/G8O1z3zE9tKTIb+gWyPhEvrWAxXc98qn7C+vinJUJtCNw3v7yjBe3sER0ZTshLrmx1tFzLuK1OCZS4OeMXpdc34uR0/V0Ssn1TfHvUPguqG9+NeJw8nLTmtylgXu8tbFA7rx9np3O0VKkoO6BlfIhJqS5EBVg/ZZD5TsFL57SQH7jp0iJyMlaCL5v+9uJjM1iYXr9zf7e0hLdlBb7wr6xeBNMLlZqSx+6IqQSd2b+HtmprB08yHfF01mijPsADlw13QVmDK66ReJNyE21+i69UAFU19c0+SLc9qV5/Avf9nKv3xnKHP/d1ej5wj1ngOv3M6ZsSRs2VaASRfls3DD/kYNnw9dM4jRv1gRtFTW3Nl7uJgi0VxCj6v50P0dsGTeIQKTOUQ/mUP0kjn4LTASZj//MpEIvgnVdpadAsXdSPydoSzfcrjR4/YcPd1o3v3aehdOEcYM6tFom3egybUX5LG/vJrth080SoJJDsHpEN+B379bOj2yUnnomvNA4abfFFNT7/KVo5IcQl52GvOmFjFr4SYQKOyewaHKaqrrXIhAftd03xVQdZ2LW0fmU6/qG6no5f3R2+3Vv0zgf+burR17E5f3cZEkc8CXMAMXfbnh2WIKemTQJyfNHTvB20fmry7h6KlagEafw/zVJazdc4wXV+7m451HGtXpJ41wv+f3Nh/y3T9uiLvUUXqiOuxgH3B/GfbOSePrY6f5ZOdRcrNSeeaOEfz3lkOUnahmzortqEJGspOaBpevp1Jedip/fuDyJl9WqUkOemalUnaiitoG9xfFdcN6RX1K7bg8Qw+s5xqTSBwCk0cV8NrqEtKSHVx+bk9WbCulb5c0DlZWh2xABfeZs0PgRE0DmSkOTtVGXiLxL1N4Sz8d6ZwemRT0yOB0XQMb95ZT24IpPgRY8tCZOn3PrFRe92uk93bdnTTCfbbdmrfmXiDeEfLLQIRGpZTALo2pAe1Eg/KyeP/hq1ocR8KUXKx2btqL0wFBpuNoNRGaTbydTaiafazcMLw3n+w6yvHTdS16XDTKgRkpDrJTkzl8IvxoaodAQfcMZk8azsL1+/ngi8P06ZLO3mOnubB/V87JzWLyqAJunFMc8jlaWnZJmJJLskOo60x/dSZhRDOZQ3wlc4AB3TNQIl/CMZQUpzTbbhGpZUFKfZGIxsd+utbF6drIpsZweZe9VPhg62EqquupqDoBwLo9R/nbzqO89snXIR8fOJajreKqH/qqGVeT1QHD64052+wOaBdoreaSedf0uDp/bJG7XlxDRXV9o/siaWbwjuWIlrhK6Hk5adTbGboxcam8qj78TmeZPl3Szt4zdICrBudy20X5sQ7DGGPa7GBFdVTn4om7hD5vahFP39G6ieGNMaaziWZnj7hL6IAtnWaMSRjJ3knxoyCihC4i40XkSxHZISIzgmwXEZnj2f65iFwUtQiDKH50HNcN69WeL2GMMR0ikhHEkQrb7CwiTuA54FpgH7BWRBap6la/3W4ABnn+jQb+0/N/uxjzq/AjvYwxJh4kR7FOEslTjQJ2qOouVa0F3gBuCdjnFuAVdfsE6CoifaIXZmPeVYv8pUTxssUYYzrKqp9cE7XniiSh5wN7/W7v89zX0n0QkftFZJ2IrCsra3661eZ4FwcQcY+yEoE7ivozdnBPslIbX3R0wpXqjDEGgFSndHgvl2ApMbDoE8k+qOrvVbVIVYtyc3MjiS+kYGsC/vH7o+makQycOWNPSXLQJa3lAxq8b8gh0CU9iRnjh9hVgDEmqsaenxfV54sk0+0D+vvd7gccaMU+UeU/Sf5svxnLhvXNYeyQvEZrAG4oKSfTpaQlO6mpb6CuwYWIIMDV57sbV1dsO0xuVir9u2dw5GRN0HX/NuwtJzc7jZ1lJ/h8bwX1Lg1ay/fOUe5SyEx1UlPnIiPVSUVVPQ7c83y41P2N5xRIcjp8VxLZaclcN6w3Sz4/gEuVwh6ZVNU1UH66jjUzv820V9ex9UCle8UmhSSHe5i5AtcO7cWGknKOnqwhyemgrt6FyxNLerKTlCQHudmplJ2oobbeRb1LfTPKeeNWv/fgXTrsysHuL9/3tx4OOd+HdxY9pwiZKc4mo+b8p8cd0D2DEzV1HDtV1y5T8UZDerJ7EqZojGNzej4b73u1sXGx4RT3DJh1DdphawgIkJHi5ML+XdlQchyHCE/d9k3m/M92yk/XRX0ZurCTc4lIEvAVcA2wH1gLTFbVLX773AQ8ANyIuzF0jqqOau5527pikTHGnI3aNDmXqtaLyAPAe4ATeElVt4jIdM/2ucBS3Ml8B3Aa+D/RCt4YY0xkIiouq+pS3Enb/765fj8r8I/RDc0YY0xLxOVIUWOMMU1ZQjfGmARhCd0YYxKEJXRjjEkQMVtTVETKgNBrM3WsnsCRWAcRhsXYdp09PrAYo6Gzxwdti3GAqgYdmRmzhN6ZiMi6UP06OwuLse06e3xgMUZDZ48P2i9GK7kYY0yCsIRujDEJwhK62+9jHUAELMa26+zxgcUYDZ09PminGK2GbowxCcLO0I0xJkFYQjfGmASR8Am9tQtci0h/EflQRL4QkS0i8k+dKT6/7U4R2SAii9sjvrbGKCJdReRPIrLN81l+qxPG+CPP73iziCwQkegtIRN5fOeLyMciUiMiP27JY2MdY0cdK22J0W97ux4vbfw9t/1YUdWE/Yd7ut+dwDlACvAZMDRgnxuBZbjnor8UWO25vw9wkefnbNxzwg/tLPH5bX8YmA8s7myfoWfby8C9np9TgK6dKUbcSyXuBtI9t98C7o5BfHnAJcATwI9b8thOEGO7HyttjdFve7sdL22NLxrHSqKfobd6gWtVPaiq6wFU9QTwBUHWSY1VfAAi0g+4CXghynFFJUYRyQGuBF4EUNVaVS3vTDF6tiUB6eJezCWD6K+2FTY+VS1V1bVAXUsfG+sYO+hYaVOM0CHHS6vji9axkugJPSoLXItIITASWN3J4nsWeBRoug5e9LQlxnOAMuAPnsvcF0QkszPFqKr7gX8HSoCDQIWqLo9BfO3x2JaIyuu047ECbY+xvY+XtsQXlWMl0RN6mxe4FpEs4G3gh6paGcXYwr52c/uIyM1Aqap+GuWYArXlM0wCLgL+U1VHAqeA9qgBt+Vz7Ib7LGog0BfIFJG7YhBfezy2Jdr8Ou18rEAbYuyg46Utn2FUjpVET+htWuBaRJJx/4G+rqrvdLL4LgcmiMge3Jd2V4vIa50sxn3APlX1nq39CfcfbWeK8dvAblUtU9U64B3gshjE1x6PbYk2vU4HHCvQthg74nhp6++5zcdKoif0tcAgERkoIinAncCigH0WAd/z9IK4FPcl90EREdz1rC9U9ZnOFp+q/kRV+6lqoedx/6Oq0T6zbGuMh4C9IjLEs981wNbOFCPuUsulIpLh+Z1fg7sG3NHxtcdjOyTGDjpW2hRjBx0vbYkvOsdKtFt6O9s/3L0bvsLd+jzTc990YLrnZwGe82zfBBR57r8C9+XS58BGz78bO0t8Ac8xlnbq5dLWGIERwDrP5/gu0K0Txvg4sA3YDLwKpMYgvt64z9IqgXLPzzmhHhujzzBojB11rLT1c+yI46WNv+c2Hys29N8YYxJEopdcjDHmrGEJ3RhjEoQldGOMSRCW0I0xJkFYQjfGmARhCd0YYxKEJXRjjEkQ/x/uxik8oYqhqQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAENCAYAAAABh67pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3xU1fno/88zM8kkkEQCJFzCtQpq8AJKEa9VtBZB8dJ+1Yq2tj1VjqXWUz0WlbZ+W9va8/3qz3JqvXvqDTl+K1hUqBeqLfTITcByEYQikAQw4RJCyD15fn/MhZnJTGZPLmSyed6vV17JzF577zU7ybPXfvbaa4mqYowxxr083V0BY4wxXcsCvTHGuJwFemOMcTkL9MYY43IW6I0xxuUs0BtjjMtZoDfGGJezQG+MMS5ngd4YY1zOAr0xxricBXpj2iAixSKiIvKPOMvWiEiliOR1R92MccoCvTFtKwl+Hxpn2TPACcAVx646xqTOAr0xbVDVw0AVMFhEvDGL1we/n3lsa2VMaizQG5NcKeAFBiZY3usY1sWYlFmgNya50uD32PTNpcHv/zyGdTEmZRbojUkulKcfEnpDRPoDPwAOAX/qjkoZ45SvuytgTA8Q1aIXEQGeBQqA21W1qrsqZowT1qI3JrnY1M3DwNXAH1T16e6pkjHOWaA3Jrlw6kZEfgbcCzwPzOy+KhnjnNicsca0TUTGABuAesAPPAH8QO2fx/QQ1qI3JrlQi94PPKiqd1iQNz2JteiNMcblrEVvjDEuZ4HeGGNczgK9Mca4nAV6Y4xxubR8MrZ///46YsSI7q6GMcb0GB9//PE+VS2ItywtA/2IESNYvXp1d1fDGGN6DBHZmWiZpW6MMcblLNAbY4zLWaA3xhiXcxToRWSyiGwRkW0iMivO8lNE5CMRqReRe+Is94rIWhF5qzMqbYwxxrmkgT44T+bjBCZALga+KSLFMcUOAHcC/5lgMz8CPu1APY0xxrSTkxb9BGCbqm5X1QZgHoGxuMNUtVxVVwGNsSuLyBBgKoGJGrpUeVUd1z/1EeWH67p6V8YY02M4CfRFHB29DwKTMBSlsI/HCIzf3ZLCOu0yZ8lWVu04wJz3t3b1rowxpsdw0o9e4rznaMhLEbkSKFfVj0Xk4iRlbwNuAxg2bJiTzYedPHsx9U1HzyMvr9jFyyt24fd52PLQFSltyxhj3MZJi76Uo1OoQWCC5N0Ot38+ME1EdhBI+UwSkZfjFVTVp1V1vKqOLyiI+3BXQkvvvYRpYweTlRH4OFkZHq4eO5ilP7kkpe0YY4wbOQn0q4BRIjJSRDKBG4GFTjauqvep6hBVHRFc76+qenO7a5tAYV4WuX4f9U0t+H0e6ptayPX7KMzN6uxdGWNMj5M0daOqTSIyE3gH8ALPq+pGEZkRXP6kiAwEVgN5QIuI3AUUq2pVF9Y9yr7qeqafM5ybJgxj7spdVNgNWWOMAdJ0hqnx48erjXVjjDHOicjHqjo+3jJ7MtYYY1zOAr0xxricBXpjjHE5C/TGGONyFuiNMcblLNAbY4zLWaA3xhiXs0BvjDEuZ4HeGGNczgK9Mca4nAV6Y4xxOQv0xhjjchbojTHG5SzQG2OMy1mgN8YYl7NAb4wxLmeB3hhjXM4CvTHGuJwFemOMcTkL9MYY43KOAr2ITBaRLSKyTURmxVl+ioh8JCL1InJPxPtDReQDEflURDaKyI86s/LGGGOS8yUrICJe4HHgq0ApsEpEFqrqpohiB4A7gWtiVm8C7lbVNSKSC3wsIu/FrGuMMaYLOWnRTwC2qep2VW0A5gFXRxZQ1XJVXQU0xry/R1XXBH8+DHwKFHVKzY0xxjjiJNAXASURr0tpR7AWkRHAOGBFguW3ichqEVldUVGR6uaNMcYk4CTQS5z3NJWdiEgO8Dpwl6pWxSujqk+r6nhVHV9QUJDK5o0xxrTBSaAvBYZGvB4C7Ha6AxHJIBDkX1HV+alVzxhjTEc5CfSrgFEiMlJEMoEbgYVONi4iAjwHfKqqj7a/msYYY9oraa8bVW0SkZnAO4AXeF5VN4rIjODyJ0VkILAayANaROQuoBg4A7gFWC8i64KbvF9VF3XBZzHGGBNH0kAPEAzMi2LeezLi570EUjqxlhE/x2+MMeYYsSdjjTHG5SzQG2OMy1mgN8YYl7NAb4wxLmeB3hhjXM4CvTHGuJwFemOMcTkL9MYY43IW6I0xxuUs0BtjjMtZoDfGGJezQG+MMS5ngd4YY1zOAr0xxricBXrTLhdffDH5+fnU19d3d1W6zNy5cxk+fDi9e/fmmmuu4cCBA0nX+dvf/oaIMHv27PB7e/bsYdq0aQwePBgRYceOHVHr/Md//Af9+/fntNNOY8OGDeH3//GPf3DNNdd02ucxxy8L9CZlO3bsYOnSpYgICxc6mmzMsaampk7dXntt3LiR22+/nZdeeokvvviCXr16cccdd7S5TmNjIz/60Y8455xzot73eDxMnjyZ119/vdU6e/bs4bnnnmP79u3MmDGDWbNmAYHjcPfdd/PYY4913ocyxy0L9CZlL774IhMnTuTWW2/lhRdeoL6+nj59+kS1RisqKsjOzqa8vByAt956i7Fjx9KnTx/OO+88/vnPf4bLjhgxgt/+9recccYZ9O7dm6amJh5++GFOPPFEcnNzKS4uZsGCBeHyzc3N3H333fTv35+RI0fy+9//HhEJnyQOHTrE9773PQYNGkRRURGzZ8+mubk5pc/4yiuvcNVVV3HRRReRk5PDL3/5S+bPn8/hw4cTrvPII49w+eWXc8opp0S9P2DAAO644w6+/OUvt1pn165djBs3jry8PC677DK2b98OwGOPPca0adMYMWJESvU2Jh4L9CZlL774ItOnT2f69Om88847VFZWct111/Hqq6+Gy7z22mt85StfobCwkDVr1vDd736Xp556iv3793P77bczbdq0qLTPq6++yttvv01lZSU+n48TTzyRpUuXcujQIX7+859z8803s2fPHgCeeeYZFi9ezLp161izZg1vvPFGVP2+/e1v4/P52LZtG2vXruXdd9/l2WefBWDZsmX06dMn4deyZcuAQIv+zDPPDG/zxBNPJDMzk88++yzuMdm5cyfPP/88P/vZz1I6lieddBLr16+nsrKS999/nzFjxlBSUsK8efO45557UtqWMQmpatp9nX322WrS09KlS9Xn82lFRYWqqp588sn66KOP6nvvvacjR44MlzvvvPP0hRdeUFXVGTNm6OzZs6O2M3r0aP3www9VVXX48OH63HPPtbnfM888U9944w1VVb3kkkv0ySefDC977733FNDGxkbdu3evZmZmak1NTXj53Llz9eKLL07pc06aNEmfeOKJqPcGDx6sH3zwQdzy06ZN03nz5qmq6re//W194IEHWpVpbGxUQD///POo9+fOnavjxo3TyZMn644dO/Taa6/V999/X+fNm6cXXXSRTps2TUtKSlKqvzn+AKs1QUx11KIXkckiskVEtonIrDjLTxGRj0SkXkTuSWVd07O88MILXH755fTv3x+Am266iRdeeIFJkyZRW1vLihUr2LlzJ+vWrePaa68FAq3dRx55JKrlXFJSwu7du8PbHTp0aNR+XnzxxXCqJ5QW2rdvHwC7d++OKh/5886dO2lsbGTQoEHhdW+//fZwCsmpnJwcqqqqot6rqqoiNze3Vdk333yTw4cPc8MNN6S0j5BvfvObrFmzhsWLF7Nhwwb8fj/jxo3jnnvu4c033+Tf/u3frHVvOiTp5OAi4gUeB74KlAKrRGShqm6KKHYAuBO4ph3rmh6itraW1157jebmZgYOHAhAfX09lZWVrF+/nuuvv55XX32VAQMGcOWVV4aD4tChQ3nggQd44IEHEm5b5Ogc8jt37uT73/8+S5Ys4dxzz8Xr9TJ27FgCjRYYNGgQpaWl4fIlJSXhn4cOHYrf72ffvn34fK3/vJcuXcoVV1yRsB6LFy/mwgsvZMyYMXzyySfh97dv3059fT2jR49utc6SJUtYvXp1+JgcOnQIr9fL+vXr+fOf/5xwX7Fqa2u5//77Wbx4MVu3bmXo0KHk5eXx5S9/mV//+teOt2NMK4ma+qEv4FzgnYjX9wH3JSj7IHBPe9aN/LLUTXqaO3eu5ufn686dO3XPnj3hrwsvvFB//OMf6/Lly3XgwIE6ZsyYcJpFVXXVqlU6ZMgQXb58uba0tGh1dbW+9dZbWlVVpaqB1M17770XLr9x40b1+/26efNmbWpq0ueff169Xq8+88wzqqr6hz/8QYuLi7W0tFQPHjyol112WTh1oxpIo9x555166NAhbW5u1m3btoXTRE5t2LBBc3Nz9e9//7tWV1fr9OnT9YYbbohbtqqqKup4XH/99XrXXXfp/v37w2Vqa2u1urpaAd28ebPW1ta22s7999+vjz76qKqq7t69W/v27at79+7VJ554QqdOnZpS/c3xhw6mboqAkojXpcH3nOjIuibNvPDCC3znO99h2LBhDBw4MPw1c+ZMXnnlFc4++2x69+7N7t27o1rN48eP55lnnmHmzJnk5+dz0kkn8cc//jHhfoqLi7n77rs599xzGTBgAOvXr+f8888PL//+97/P5ZdfzhlnnMG4ceOYMmUKPp8Pr9cLBNI+DQ0NFBcXk5+fzze+8Y3wjVynxowZw5NPPsn06dMpLCzk8OHD/OEPfwgvnzFjBjNmzAAgNzc36nhkZ2fTu3dv+vbtGy6fnZ1NTk4OAKeccgrZ2dlR+9uyZQvvvvsuP/zhD4HAVcusWbMYM2YMc+bM4Te/+U1K9TcmkmjwcjhhAZF/A76mqv8t+PoWYIKq/jBO2QeBalX9z3asextwG8CwYcPO3rlzZ0c+lzmOLF68mBkzZmB/M+Z4JiIfq+r4eMuctOhLgcg7ZUOA3QnKtntdVX1aVcer6viCggKHmzfHo9raWhYtWkRTUxNlZWX8+7//e/jGrzGmNSeBfhUwSkRGikgmcCPg9HHIjqxrTFyqys9//nPy8/MZN24cp556Kr/4xS+6u1rGpK2kvW5UtUlEZgLvAF7geVXdKCIzgsufFJGBwGogD2gRkbuAYlWtirduV30Yc3zo1asXq1at6u5qGNNjJM3Rd4fx48fr6tWru7saxhjTY3Q0R2+MMaYHs0BvjDEuZ4HeGGNczgK9Mca4nAV6Y4xxOQv0xhjjchbojTHG5SzQG2OMy1mgN8YYl7NAb4wxLmeB3hhjXM4CvTHGuJwFemOMcbnjKtCXV9Vx/VMfUX64rrurYowxx4xrA328oD5nyVZW7TjAnPe3dmPNjDHm2Eo68UhPFRnU/+vjUuqbWsLLXl6xi5dX7MLv87DloSva2IoxxvR8rgv0J89e3CqoA3gEMn0e6hpbyMrw8LUxA3lg6qndVU1jjDlmXJe6WXrvJUwbO5isjMBHy8rwcPXYwVwztoj6phb8Pg/1TS3k+n0U5mZ1c22NMabrua5FX5iXRa7f1yqoV1TXM/2c4dw0YRhzV+6iwm7IGmOOE64L9AD74gT1p245OpXiQ9ec1o21M8aYY8vR5OAiMhn4HeAFnlXVh2OWS3D5FKAGuFVV1wSX/Q/gvwEKrAe+o6ptNqdtcnBjjElNhyYHFxEv8DhwBVAMfFNEimOKXQGMCn7dBjwRXLcIuBMYr6qnEThR3NjOz2GMMaYdnNyMnQBsU9XtqtoAzAOujilzNfCiBiwH+ojIoOAyH5AtIj6gF7C7k+pujDHGASeBvggoiXhdGnwvaRlVLQP+E9gF7AEOqeq78XYiIreJyGoRWV1RUeG0/sYYY5JwEuglznuxif24ZUQkn0BrfyQwGOgtIjfH24mqPq2q41V1fEFBgYNqGWOMccJJoC8Fhka8HkLr9EuiMpcBn6tqhao2AvOB89pfXWOMMalyEuhXAaNEZKSIZBK4mbowpsxC4FsSMJFAimYPgZTNRBHpFeyZcynwaSfW3xhjTBJJ+9GrapOIzATeIdBr5nlV3SgiM4LLnwQWEehauY1A98rvBJetEJE/AWuAJmAt8HRXfBBjjDHxOepHf6xZP3pjjElNh/rRG2OM6dks0BtjjMtZoDfGGJezQG+MMS5ngd4YY1zOAr0xxricBXpjjHE5C/TGGONyFuiNMcblXBvoy6vquP6pjyi3uWGNMcc51wb6OUu2smrHAea8v7W7q2KMMd3KdZODnzx7MfVNLeHXL6/YxcsrduH3edjy0BXdWDNjjOkermvRL733EqaNHUxWRuCjZWV4uHrsYBbccZ6lcowxxyXXBfrCvCxy/T7qm1rw+zzUN7WQ6/cxd8UuS+UYY45Lrgv0AGUHaynI8fPcreMRjqZvVAM/j5j1NifPXpx0O3ZD1xjjBq4M9EPys6morucv6/ey/L5L46Zylv7kkqTbsRu6xhg3cNXN2EQ3Yj0SmM08MpVTmJuV8nbshq4xpidyVYs+0Y3Yi0YXMP2c4Sy443ymnzOciur6dm3HyVWAMcakG1e16BPdiP3djePCZR665rR2b6etqwBjjElXjlr0IjJZRLaIyDYRmRVnuYjInODyf4rIWRHL+ojIn0Rks4h8KiLnduYHiLWvut5R6z3ZjVan2zHGmHSXdHJwEfECnwFfBUqBVcA3VXVTRJkpwA+BKcA5wO9U9ZzgsheApar6rIhkAr1UtbKtfR6LycFnL1jPKyt3MX3CMB669vSU1i2vqmPmq2v5/U3jrJVvjEkLbU0O7iR1MwHYpqrbgxubB1wNbIooczXwogbOGsuDrfhBwBHgIuBWAFVtABra+0E6Q2fcaI3sjZPqScIYY441J6mbIqAk4nVp8D0nZb4EVAD/R0TWisizItK7A/Vtl8g0TUdutJ48ezEjZr3drj75xhjTXZwEeonzXmy+J1EZH3AW8ISqjiPQwm+V4wcQkdtEZLWIrK6oqHBQLeciW+AdudFqvXGMMT2Rk9RNKTA04vUQYLfDMgqUquqK4Pt/IkGgV9WngachkKN3UK+k2upXP/2c4dw0YRhzV+6iwuGTr9YbxxjTEzkJ9KuAUSIyEigDbgRuiimzEJgZzN+fAxxS1T0AIlIiIier6hbgUqJz+11qwR3ncctzK6mub6S+KXDuGNGvF6/NODccnJ10t4SjN2B7ZXjadZIwxpjukjTQq2qTiMwE3gG8wPOqulFEZgSXPwksItDjZhtQA3wnYhM/BF4J9rjZHrOsS4SC8tD8bPYfib73u2N/DRN+tSTlp1xD6Z/pE4aFTw5OTxLGGNOdknav7A4d7V554n1v05zgY3kErjpzMA9MPTUq5ZKoy2Rs+ifEhkMwxqSTtrpXumoIhFCvmERBPtMrKMTNqycawMxuwBpjejpXDYGw9N5LeGjRp7y7cS91jYFWuEegRWFUYQ6/u3FcOK8easGvK6mkoY1+9XYD1hjT07kq0EcG5VCA75OdwSWnDKC6vpHiwXnhvPrsBetZteMA144tokk1fHLIyvDwtTEDeWDqqeHthoZDsBuwxpieyFWBHqKD8o/mrWVreTXZGR4euT6QuorNuc9fWxb+OVGL/albjqa97AasMaanceXN2LZuoMamd7IyPOT3yuS8k/rxvfO/FG6xRwZ3Y4xJdx0d66bHiZer9/s8LPjBeXFz7peeUhges8Za7MYYt3FloC/My+KtT3bTEnGxUt/UwpTfLcPv83DxyQWWczfGHDdcGegBLhpVwIeftR4zp76phQ+3VIT7wFsL3hjjdq7qRx/pj9+dwBWnDYx6z+uRbukDn2ySE2OM6UquCvTlVXVc+/g/uObxf1B+uI51JQfDy7wCzS3aqkfNsQjCiR7GMsaYY8FVgX7Okq2sLalkXUklE361hD2Hjk7/16zgFaH0YE1UYE8lCKd6UrDx640x6cAV3SsTdaeMlekVrh8/lFdW7sIDcYdKaGsMm1SnHyyvqmvVlTP0MJY9WWuM6Uyu71659N5L+OmfN/D+pi8SjnMD0NCsvLxiFwDNMcviPREb0t7pB234BGNMOnBF6qYwL4v+Of42gzwExr2JHJxsRL9eiCR+IjakIwObhZ7UXXDH+Uw/ZzgV1fVJ1zHGmM7kihY9BALq0PxszhjSh7rGZj7YUo5qYIorr0cozPVz7on9WLC2jEyfh7rGFhqaWhz1p+9Iyzz0hG15VR2ffXGY3980rjM/tjHGJOWaQB87ZMEDC9Yzd+Uu/F4PDc2Bp18rgq3rPZW1LNlcDuB4EpGODmwWedPXSX7fGGM6iytuxoZETh7y0zc2UJCbFRWYP9xSccwnEbGJS4wxx8JxM/FIqNX88KLNHKxp5KYJQ3nwzY3ceelJPHXLeBKd1LryZGcTlxhjupsrAn1sf/X5a8tY+fkBpsxZFtVHftlPJjG8X6+odUf068WyWZNS2l8q/emt540xpru5ItCHWs3xRD6odOH/+oDm4EhnGV4BAk/Lphp0U33S1XreGGO6k6ObsSIyGfgd4AWeVdWHY5ZLcPkUoAa4VVXXRCz3AquBMlW9spPqHhZqNcPRqQMj+X0eJp8W6CP/0zc2cPHJhe26qdre/vQ2cYkxpjslDfTBIP048FWgFFglIgtVdVNEsSuAUcGvc4Angt9DfgR8CuR1Ur1b2Vddz80Th3Ogup5FG/aG3/cKNDQfTZe0N+iWV9Vx6qA8BuT5+dtnFQmnHTTGmHTjJHUzAdimqttVtQGYB1wdU+Zq4EUNWA70EZFBACIyBJgKPNuJ9W7lqVvG89A1p9Gsys0Th3Peif0YPSCHCV/qG5Uuae8gZnOWbOWT0kq2Vxw5Zvl2G/XSGNMZnKRuioCSiNelRLfWE5UpAvYAjwH3Arlt7UREbgNuAxg2bJiDasWXbArAVPuzx6ZrtpZXA9CiGjiBdEIQjuwWGnnSsL73xpjO4CTQS5z3Yvsjxi0jIlcC5ar6sYhc3NZOVPVp4GkI9KN3UK+UJMqvZ3o9jB3Wp1WQDYk3x2xnD0wWG9Dbey/AGGPicRLoS4GhEa+HALsdlvkGME1EpgBZQJ6IvKyqN7e/yqlrK7/uE2H+urKEreau7B7Z1sln2tjBcU8uxhiTKic5+lXAKBEZKSKZwI3AwpgyC4FvScBE4JCq7lHV+1R1iKqOCK7312Md5CF+fr2usYU/r9vN62vLko4V31XdIxM9TLVs1iXW994Y02mStuhVtUlEZgLvEOhe+byqbhSRGcHlTwKLCHSt3Eage+V3uq7KzrWVX//6WUX841/7qaxpSNpq7qrukW1dLXR0bB1jjAlx1Vg3sZJN/BEa+CwzOPDZ9AnDuPPSUa1ujJZX1XHbSx8jAk/dcnantqxvf2l1qzF5kt1QNsaYWK6feCRSVA+WJPn1eK3meD1d5izZyrqSysDPndwDxh6mMsZ0Nde16O9+bR2vrynj62cV8ZPJp/C1x/7OpFMG8L0LRrbZYnY6HWFIqAdMoq6RxhhzLB0Xo1eGBjZ7fU0ZAK+vKWPCr5dwsKaR7AwPxYPzeOia0xKmRZbeewkjIgY8y8rwMHnMAL4yuj+eiM6jXoHJYwaER590Mu6NPfhkjOlOrkndtHVlkqwferzWfF1jC+9t+oIbJwyLGjunWaF/jp8Lf/uB477u3fHgk11pGGNCXNOiX/aTSRTlZ8dd5vcFWuenDsqL26pecMd59O2dGW65+30eRvTrzYWj+oenKJx6+iCmnj6IofnZVFTXOxpnPnb45La6cHa2VEfYNMa4l2ta9IV5WZQdrI27rL6phb9s/AKA3y7azCM3jA23eB+8qphbnlvJgSMNQCDINzS3cMFJ/ZK2vpP1dW/rqdquYk/VGmNiuSLQp3Ij9fW1Zby+tiz8esqcZVHL65ta8AiOHopK1te9OyYd6Y6TizEmvbki0C+99xJ++ucNvBNstXeUz+tx1JfdSdfIY/3gU2eeXCzPb4w7uCJHX5iXRf8cf6dtr6Gppd159NgeNqHhk5P1+ulMnTVkg+X5jXEHV7TogfBN033V9dQ2Hk3j+H1CfVOg24zPIzTFTj8VhwDFgwM3bjsyzWB3DS3c0YewLM9vjLu47oGp219aTY7fF+5Pn4jQeqzlWDefM8xxsE50n6AnBsdkQ0cYY9LPcfHAFAQC1MGaxuQRnKNFsjMSHwIn3SFDqZoFd5yXtLtlT9EdN5GNMV3HNakbgIm/WUKLwsoU1olM88Ry0mMllKqZu2KXq4Jj2cEaCnL8PHr9WP6yca+NnmlMD+aKQJ/qODWRIlM4mT4h0+uhur45abCOl8cG8Iqw4I7ze/zQwkPye/G3rfv4y4Y9No1hO1iPJZNOXJG6CT2l6vel/nFCQd7v89DYrOT3yuS6s4oY2b83fXtlUnqwptU6oRmrvjZmQKtUzUf3T+pwD5tjOTZO7L6682leN7EeSyaduCLQF+Zl8dYnu9vdqs/O8IS7IhYPzqNXhpfNew+z/0gD/Xq37rYZb8aqzkzVHMsgEbsvJ0M7mMTsRGnSkSt63XQkdeNU6Goh3n4EuO6sIVTXN3aon/yx7LnT1r6+cfaQVhOyWPrGGeuxZLqL63vdhFqhXmm9rFemN+q1J06ZTK/Qr3cmL39vAnEWA4GHqOL1rBnRrxdI4KogMsi3J/1yLFvTbe2rq+bIPR5YjyWTjlxxMzb0z9Uc5+KkpqE56nW856UampX9Rxr4y4a9XDuuiPlro/vg5/i9HGlojupZA4GhjHfsD+TwYx8q+u1fNrPy8wP8dvFmHrl+bEqf41gEibb2ZbNedYzN92vSjaPUjYhMBn5HYHLwZ1X14ZjlElw+hcDk4Leq6hoRGQq8CAwEWoCnVfV3yfbXngemQnOvTh4zkNlvrGfXgZpwUPcA2ZmBYA3g9QjNDp6QTcQrwmM3juWe//qExuYWWvRoV8zFG/bS0IH0y7GcQ9bmqzXGPTo0Z6yIeIHHga8CpcAqEVmoqpsiil0BjAp+nQM8EfzeBNwdDPq5wMci8l7Mup0iMkCdf1J/dq7cFW6ptkA4yANRQT40vnxkPrW8qo5/VRzhZ1cW85/vbgmfNCLzrXPe3xpu2Ue2iElw4ow9oSbqfpdKa7qjXfiOh5a7dXM0xlmOfgKwTVW3q2oDMA+4OqbM1cCLGrAc6CMig1R1j1vVdfgAABYPSURBVKquAVDVw8CnQFEn1j+ufdX1CPFvnIZkBBP6+480UNfYQmZEsD6xIIeK6np+8dYmigfl0aKEl7/5yW4m/GpJuN88wf2owvZ9Rxgz+ASGxEyAMqJfL5bNmhT1Xmf0rOnpXfiS3cfojG6mPf0YGdMZnOToi4CSiNelBFrrycoUAXtCb4jICGAcsCLeTkTkNuA2gGHDhjmoVnyhYRDOO7Efy7btT1iuMZjQD+XwG5ta8IhEBfDyw/Us2rAXgK+eUkh+jp/SAzXk9cqI6lUxMC+LnQdqqDhcz7aKanplBG4AZ3iFxmYNXEEoXP/UR6zbdZCGiJsJody+CKy4/1JHrc72DjqWbq3bRAPAheo5ND+73QPE2cBsx5fyqjpuf+ljFHj6W2enxd93qrry/9NJiz5eR5TY/ESbZUQkB3gduEtVq+LtRFWfVtXxqjq+oKDAQbVa27T7EBN+vYSVnx9oM8jH3T/Q3Mb9irc37OXl5Tv5aPv+uDdkVWFreTWq0WmimycG+uaHgtpVZw6O23NHFcetzvb2zkmX1m2yvubn/ibwO3x9TVm7+6IvvfcSvjZmQLiXlT0P4G5zlmxlbUkl60oqu/3vOxWRV61d+f/ppEVfCgyNeD0E2O20jIhkEAjyr6jq/PZXtW2d0Zf+6rGDaWxqCbfiI/l9wsUnF1JWWctb/9zNdWcVcfHoQu6atzZub5+LTy7gf33jjFaTiMeOqtlWz51IUWf7JL1zYlsG7WnddmXrItEsWIs37GXErLfjrnP12MEpzZJVmJfF9oojtCh4Bevm6FLx/u970tXbnCVbWfn5ASb8akn4va6ov5MW/SpglIiMFJFM4EZgYUyZhcC3JGAicEhV9wR74zwHfKqqj3ZKjeMYMevtDgd5EXjzk91xgzwEumBurzjChrIqDtU28caaMn74avwgD/Dhlgou/O0HR/v4B5uWXo8w6IQspp4+kJMH5DI0Ip/vFRK2OmPP9m31dXfytGtbk6XH20ZnCLVekPjz7S6LqScEnnuQYHmnQTp0xbC1vBqAZg3cI5+7cleSNU1PE7pyi3yGxiPwtTED0vrqLfKqNlZXXH0mbdGrapOIzATeIdC98nlV3SgiM4LLnwQWEehauY1A98rvBFc/H7gFWC8i64Lv3a+qizrtEwCL7ryA77/4MWWV8ScHT2bqaQPJz/Hz10+/YPeh6MDXO9gtM5SaCUkU4CPVN7Uw4ddLot5rblH2HKrjLxv2ttpGs8Kf1+3mLxv2hs/kyVrj5VV1fPbFYX5/07g2y37j7CFRgfVfFUfYVlHdKv/dWbnteFcEkSePeH3NI69UPBJ45uGKMYHfTSp90WOvGETg8uIB/LKH9ixKt3sr6SQ0u1zk/1KLQkGOP62PVezfqFcC//+ZXfT8jKMHpoKBeVHMe09G/KzAD+Kst4z4+ftOVTz4hFZPwDox5fRBZGV4mN/GJCX9cjL5StEJLFofv6XfliF9snn466dzz5/+SUVVXdQfY7wThdcjFOb6+fPM84Gjg6cNyPPzt88q4k72HRk825oY/KdvbOC6cUXh1FHopBUbyJ1MLu4k8ETW678+Lo070qcI3HnpSVFdO+OdAGK7fibbf7wTxvaKI2n9j9+W2JvWFvijhWaXO2NIHwD+WVqZ9k9zx0u/ji7M4bEbx3XJQ3aueDK2vfn5RevDnYIY0a8XJQdqWgXgXQdq2XWgfVcKFdX1jB6Yy6WnFPLKil3hIZFH9OvF3qq6VmfyxuYWLj2lMKoF/ElpJScV5LRKc8Tm/kMB2yOBfWT6PNQ1tuDzSPhp19kL1iMCw/se3b8n2Nr9RTCYJsr/h3oN/f6mcW1Ol5ho+GaPHK2TR6CoTzYlB2v57eLNlBysDQetUN/+yCuVWE4C36srAzd6Q7/OreXVjJj1dtwrk027D3HDU8v5vzMmUjzohDZ/p7H76sqgm+jqyiuBpw+7c7rKrpTqMe2pD/nFa9SERr7tbK4Y1Gz0A4tpaO7aQc064uLR/dm5v4bP99cwsl8vGluUsspaMr3xz+QfbqmIe+LK8Ao3fHkYFYfr+OXVp7VqeX9ldAHLt+/nsuIB1NU38/aGvQzI9fPF4bZbN6MKc3jvx18Jv771+ZVs2lPFozecyV82fEHF4ToKcvxx84lAVBrp9pc+piDPz99jrkC8HmHB2rJEz5PhFfjXb6aGX89esJ5XVu5i+oRh3HnpKGa+upZ1JZVxnzoOBb7IwddSGVzsq4/+ja3l1a2OQzyR9Xro2tNbve5MsZ8hkXS/6eg0cIe71fbJZv66sk49psfDVVBbT8a6ItCXV9Vx3RP/j9KD7Wt5dwevCG/+8ALmrtxF6YEj1DS2hP8IN+0+xC3PraS6vjE8sfmIfr14bca5UX+kDyxYHzXK5EkFOVH3EWJlZXgcBYxQ8LpuXBHz15QlnJkxNniG1vMHW+5+39HRL+eu3BV3nKFUXDeuiCZVx4Ev9vjEBo5EPXwAdjw8Neq106vGzg66sZ8h8mqsu0fGdBo8nZ4Mv3Tf23H/RiKPaaoBu6Mnj550gnD96JWFeVk9KsgDeL0Svkwbkt+LVTsO8PCizVz/1Ec8t+xz9h9pCAd5gB37a5jwqyWMjuhLHrr0a1FtdbM4nsjgmOkV/L7A7ZNAwBhAQ1NLVP/219sI8qHthdJIkeuF9tOiGu4RtPy+S1v1qIkU6mmw6M4LAiOCxjF/bRkL1+0On0REAifARM8TJBuFc9GdF1DUJ/op5iF9sln0owta7Tt2chu/z0NRn+yoY9gV/fRjP0NTi3bpoHepPI0cSqFdOWdZ3PJOx+YPlYsX5GOPaaq9wSaGnslY275nMtraXyrHKlR20+5DUd+PxeRC4JIWPcBNzyxn+fb9HW41Hgu9M7188D8v5oKHP3Cccgrl978+rohHbgiMhhlqbTx4VTFP/n07C9fFPt4QLXQ/IFKo1R26GhjRr1e4X39bzj2xH4NPyGZ/dR2VtU18UloZNy2T6RXGDsvnwauKufX/rKL8cH34BumwvtnsOlAbvq8gxB9dNCQrw0OO30dlTQOP3TiW5dsP8uGWcsoqa8nwCA3NGnV8nAilbULaSt88sGA9r0Skr0YV5rCtojrqiiGUZuqqFmBXD0QXan1fO7aI0srAvROUNp/LCIm9mnGaPouXogr9jdx8TqAFnmifmT4Pn8W5gmrrCiz0TEZbv5+E+wv+Pf/+pnE8vGgz89eWOfqb+/H/Xcf8tWWM7NeLHQdqOKkg8LfTmekp16duQmL/aTtDrt/H4fomPIDHI0w6tZB3N37RqftojwyvcMP4oby8YheFuX4uOKk/89eWhf9BupoAnz88ldkL1od77cT+Y0weM4Defh+vrykLp41GFebwu+D9iHc37uXyMQPDQSs0vMSb63ajQIYX/D4v1fXNZHoDgTzHH3gdCsihwHfgSAOL1u9haH42S39ydFyhyEvvyIAV+nl7RTV9e2dy56RRzPnrVg4caeBLBTmtAnWif/zIFFzoXsYrK3dRkOPnrTsvSOuUSqS2AuPXxxVFpT3Kq+qY+JslSdMs0Dr1lCiwhcqFTvZTTh9E396Z4RNZovsVXx9XxE+uOIXbXvoYEfjVNafx4Jubwo2feGm+0MmjLYlOUj6PtHroMdFnB+cpv1SGQEm8jeMg0B+LWaa6g9/n4azh+azcvt9R3/1LTy5gyZYKx9svyMmksVmpaWgKj8EzJD+b0YW5/HVLeXurnVS81t/MV9cypE921HwAg0/IoqlFKU9yQzlWhlf4x08mceX/XkZFdT3TJwyjtrGZ19eU8fWzisjO8IZPkpEBOVE+OVnr9FjODpaM05x47EnwoUWfJr0qhMBnmnr6oKjfk1fgyjNbt5Qjr0CeW7adv24u553/cVGrgObkSiX2iiqe0FXW9AnDUIg6eYSWf6mgd8KroMhjEmqxZ/o8cTsBxBJgxQPRwTpZRxEh8Lfa0KyOTkBt7v94CPTp3vOmPUKt81GFbd9k7WyjCnPYVl7dZn6+IzwCxYPy8Hk94QGoQlcGqRjSJ5uTB+awZLPzE1uqRODtH17Ag29uYkh+NgvWloVbp5Gt9lRauR3RVms91fRG5E33koO1DMnPbvOZEjia9vjpGxvYXnGEreXVrdIsieo9Zc5S9lU38PWzihxPxhMp0c3a9kjU0BiaH2hoTJ8wjL99VkHJwVqmnjYQf6aX//evfVTWNLbZESD2GJRX1THpkb9RXd/kuG6Jfl/JHBeBvryqrtVTqCY9Dc0P9KFPR6EUVGhU0h37axjZvzef7zuCAFPOGMgPLh7FXfPW8ll5ddQ/digPG+IVuKx4AOWH63nqlrNBSXmExdjAfvdr68JXJbHBMll645bnVoQDc6IGar/emVTVNYZHdw2JF8yd3i9werWT7HmGTbsPcfNzKzlU05D06jbTCyKB32XkcysQvwfbife97eiKOXSvLJnM4Mi17Ymuqd5nCjkuAn1bXeWM6WoCfGV0fz77ojpqGI3IVAIcfYAsXus3MqjvO1zPDU8t56LR/Xm7jaeyMzyw9ddHu4I6SW/k+r1ccuoAR2makEtPKWRQn2y2V1SzvvRQOBgnux/Q1pW2T+CsEX3D6yZ7nqE9V31tyfR6aGxpSfhsR0igZ5U4Tg2P6NeLs4fnM39tGb0yvFGj2aYi1SvB4yLQb9p9iClzlnVRjYxbOG2ROZWV4aF/jp/Sg7V8fVwRb6wrc9QyjDRuWB+euuVs5ry/lZdX7KJ/Tib7qhscr78yIi88ctbbXZJyC93kvuQ/PuDz/TUM7ZvNoBOyw6mOUPonMuAnu8oe2a83Ow4caTPQ7nh4aqfdf8vwCI3B3E+/3plMHNmXtzfsbdXi724egavi3O9I5rgI9G69GWu6ht8nUc8pdIfOPuksuvOCbm/shJ5w7uj/owCjB+bw/10/lvsXbIga76k9snxCXTf/vlNlLfo4LEdvTM/k5KTb2SfFniDTK3z2qymOy7v+yVgIPB1rjOl5nFxZdXWQT/DAdrcZmp/dap7pjnDF6JV2I9YY0xFpkp4PKzlY26kP26XZeax9MkITgxpjjEukMkdyMq4I9P+YNYne7Zh4xBhj0lFnT4XoikBfmJdFcxreVDbGmPZ4Z+MXUROGd5QrcvTWtdIYYxJzRYt+6b2XcPmYAd1dDWOM6TS+TozOjjYlIpNFZIuIbBORWXGWi4jMCS7/p4ic5XTdzlCYl8X7m7p/6GBjjOksHk/nRfqkWxIRL/A4cAVQDHxTRIpjil0BjAp+3QY8kcK6neKiUQVkp1tnWGOMaScnQyM75SQyTgC2qep2VW0A5gFXx5S5GnhRA5YDfURkkMN1O8UfvzuB684a0hWbNsaYHs1JoC8CSiJelwbfc1LGyboAiMhtIrJaRFZXVLRvfPF91fXcPHE4U04b2K71jTEmHQjEnbu4vZwE+nhPI8X2ZUxUxsm6gTdVn1bV8ao6vqCgwEG1WnvqlvE8dM1pNKvGDfihymR47QErY0z6CkzO03pM/vZy0r2yFBga8XoIEDuQdaIymQ7W7XShyQ9uf2k1N08cHndihLYmTQgt+1fFYdaXHqK338f44X1ZvGEPItAnO5P6pmaq6wPjTPt9gUkGQrPfeIAWAr+s7AwvZw7tQ8mBGvZV15OXncEXValNizcg18/BmgZaFMYMzuOT0kNA4MQlEfv6avGAVhM/TPjV+xypb8Kf4eHAkcbweqGzsEjg67wv9WP1zoPUNrbgkcA+vzhcT4vCgDx/uM4+T+JJKyJFzl3rkcBIfH2yM6msDQy/W9cUGAc8IzhBg1MeCdS7f44fBI7UN3H6kBM4sSCX11aXkOER/BmBqd9qGprDdejfO5ODNQ0o4PN6AkPTHqrjcF1g5h+PB9qaoMzJXLzZGR5q4zxLHzkgVyqDc0XuMzvDw/jhfVm6bR8AfXtlcKCm0eGWWsvwQmNwmPR+vTPZf6T1sMiRdfV7BY9Hoj5f6O88tmzoOHgFTuiVwaGaRhS44rRBrN5xgC9SnBYyngtP6sfSbfuB6GPRt3dgf6kMFe3zCM0tis8r+IKf0QNkRAy2Fnm8PAJeT/RrcDZXc+TcyqHj7hXI8fuobWzGn+Fl0AlZVHbgdxtP0tErRcQHfAZcCpQBq4CbVHVjRJmpwExgCnAOMEdVJzhZN572Tg5ujDHHq7ZGr0zaolfVJhGZCbwDeIHnVXWjiMwILn8SWEQgyG8DaoDvtLVuJ3wmY4wxDrlmPHpjjDmeHRfj0RtjjInPAr0xxricBXpjjHE5C/TGGONyaXkzVkQqgJ3tXL0/sK8Tq9OVelJdoWfVtyfVFXpWfXtSXaFn1bcjdR2uqnGfNk3LQN8RIrI60Z3ndNOT6go9q749qa7Qs+rbk+oKPau+XVVXS90YY4zLWaA3xhiXc2Ogf7q7K5CCnlRX6Fn17Ul1hZ5V355UV+hZ9e2SurouR2+MMSaaG1v0xhhjIligN8YYl+uRgd7BZOUXi8ghEVkX/PpZd9QzWJfnRaRcRDYkWJ5wYvXu4KC+6XRsh4rIByLyqYhsFJEfxSmTFsfXYV3T6dhmichKEfkkWN9/j1MmXY6tk7qmzbGNqJNXRNaKyFtxlnXusVXVHvVFYLjjfwFfIjCxySdAcUyZi4G3uruuwbpcBJwFbEiwfAqwmMDcDROBFWle33Q6toOAs4I/5xKY+yD2byEtjq/DuqbTsRUgJ/hzBrACmJimx9ZJXdPm2EbU6cfA3Hj16uxj2xNb9MdswvHOoKp/Bw60USTRxOrdwkF904aq7lHVNcGfDwOf0npO4rQ4vg7rmjaCx6s6+DIj+BXbcyNdjq2TuqYVERkCTAWeTVCkU49tTwz0TiccPzd4KbdYRMYcm6q1i+MJ1NNI2h1bERkBjCPQmouUdse3jbpCGh3bYGphHVAOvKeqaXtsHdQV0ujYAo8B93J0NsZYnXpse2KgdzLh+BoC4z6cCfxv4I0ur1X7OZ5APU2k3bEVkRzgdeAuVa2KXRxnlW47vknqmlbHVlWbVXUsgbmeJ4jIaTFF0ubYOqhr2hxbEbkSKFfVj9sqFue9dh/bnhjok05WrqpVoUs5VV0EZIhI/2NXxZQ4mXw9baTbsRWRDAKB8xVVnR+nSNoc32R1TbdjG6KqlcCHwOSYRWlzbEMS1TXNju35wDQR2UEg9TxJRF6OKdOpx7YnBvpVwCgRGSkimcCNwMLIAiIyUEQk+PMEAp9z/zGvqTMLgW8F77JPBA6p6p7urlQi6XRsg/V4DvhUVR9NUCwtjq+TuqbZsS0QkT7Bn7OBy4DNMcXS5dgmrWs6HVtVvU9Vh6jqCALx66+qenNMsU49tkknB0836myy8m8A/11EmoBa4EYN3so+1kTkVQJ3/PuLSCnwcwI3i0J1jTuxendxUN+0ObYEWka3AOuD+VmA+4FhkHbH10ld0+nYDgJeEBEvgaD4mqq+FfN/li7H1kld0+nYxtWVx9aGQDDGGJfriakbY4wxKbBAb4wxLmeB3hhjXM4CvTHGuJwFemOMcTkL9MYY43IW6I0xxuX+f7KhWGUoQ+QkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAELCAYAAADX3k30AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZxU5ZXw8d+pru7qHZre2BcRUEAFQwANLoiJKAbQJCYqThwziu/E0Sx+jInMO5nRGM1MfCPZ1KgTFcGYREwiICpZBA2bQAQUBEGaZrGbfe39vH/UYlV1Lbe6a2mK8/18Grqq7r311O2qc586zyaqijHGmOzlynQBjDHGpJYFemOMyXIW6I0xJstZoDfGmCxngd4YY7KcBXpjjMlyFuiNMSbLWaA3xpgsZ4HeGIdEZIKI/FVEjohIi4jsEJFZImKfI9Ol2RvUGAdE5AHgTaA78BjwK6AUuB+4L4NFMyYusSkQjIlNRL4JPAL8CLhXfR8aERkFrAUOAeWq2pa5UhoTnQV6Y2IQkd7AB8C7wITwYC4i7wNnAQNVdUcGimhMXJa6MSa264Ei4JEoNfbjvv9b01ckYxJjNXpjYhCRN4HPACWqeiLC43uAEqDUUjemq7IavTFRiIgbGAvsixLkPw30BP5iQd50ZRbojYluBOAB8qJ0ofy27/8n0lckYxJngd6Y6D7l+787MDn4ARH5OvBl4FVV/VO6C2ZMItyZLoAxXZg/0P8J+J2IzAXqgYvw5u1XAzdkqGzGOGaNscZEISLLgdFAGfDvwD8BFcB24Dngx6rakLkSGuOMBXpjIhCRHOAo8L6qfire9sZ0ZZajNyay4UABsCbTBTGmsyzQGxOZvxa/NqOlMCYJLNAbE5k/0FuN3pzyLEdvjDFZrkt2r6yoqNCBAwdmuhjGGHPKeOedd/apamWkx7pkoB84cCCrV6/OdDGMMeaUISJRZ0+1HL0xxmQ5C/TGGJPlLNAbY0yWs0BvjDFZzlGgF5HJIrJZRLaKyL0RHp8mIu+KyDoRWS0iE5zua4wxJrXiBnrfnB8/B67EOyz8ehEZHrbZEuA8VR0F3AI8mcC+xhhz2qo70sB1j/+duqOpmx/PSY1+LLBVVbepahPwAjAteANVPaafjLwqAtTpvsYYczqbvWQLqz46wOw3tqTsOZz0o+8D7Ay6XQuMC99IRK4BfghUAVMS2de3/23AbQD9+/d3UCxjjDl1DZu1iMaWT1agnLOihjkravC4XWx+4MqkPpeTGr1EuK/dvAmqOl9VzwKmA/cnsq9v/ydUdYyqjqmsjDi4yxhjssbSeyYydVRv8nO9YTg/18W0Ub1Z+p2JSX8uJ4G+FugXdLsvsDvaxqr6JjBYRCoS3dcYY04XVaX5lHjcNLa04XG7aGxpo8TjpqokP+nP5SR1swoYIiKDgF3AVwhbPk1EzgQ+VFUVkfOBPGA/cCjevsYYc7rad6yRG8cN4Iax/Zm7sob6FDXIxg30qtoiIncAi4Ec4GlV3Sgit/sefwz4AvBPItIMnAS+7GucjbhvSl6JMcacYh6/aUzg9wemj0zZ83TJaYrHjBmjNqmZMcY4JyLvqOqYSI/ZyFhjjMlyFuiNMSbLWaA3xpgsZ4HeGGOynAV6Y4zJchbojTEmy1mgN8aYLGeB3hhjspwFemOMyXIW6I0xJstZoDfGmCxngd4YY7KcBXpjjMlyFuiNMSbLWaA3xpgsZ4HeGGO6gPd2H+ac/1jMe3sOJ/3YFuiNMaYLuOuFdRxtbOGueeuSfmwna8YaY4xJkYH3Lgi5vaXuWOC+jx6akpTnsBq9McZk0MI7J9Cne0HIfX27F7DwrglJew4L9MYYk0HDe3ejMC8n5L6CvByG9+qWtOewQG+MMRl2+GQzQ6uL+dn1oxlaXczhk81JPb7l6I0xJsNW3nd54Perz+ud9ONbjd4YY7KcBXpjjMlyjgK9iEwWkc0islVE7o3w+I0i8q7v520ROS/osY9EZL2IrBOR1cksvDHGmPji5uhFJAf4OfBZoBZYJSJ/VNX3gjbbDlyiqgdF5ErgCWBc0OMTVXVfEsttjDHGISc1+rHAVlXdpqpNwAvAtOANVPVtVT3ou7kc6JvcYhpjTHarO9LAdY//nbqjDUk/tpNA3wfYGXS71ndfNF8DFgXdVuA1EXlHRG6LtpOI3CYiq0VkdX19vYNiGWNM9pi9ZAurPjrA7De2JP3YTrpXSoT7NOKGIhPxBvrgIV2fUdXdIlIFvC4im1T1zXYHVH0Cb8qHMWPGRDy+McZkm2GzFtHY0ha4PWdFDXNW1OBxu9j8wJVJeQ4nNfpaoF/Q7b7A7vCNRORc4Elgmqru99+vqrt9/9cB8/GmgowxxgBL75nI1FG9yc/1huP8XBfTRvVm6XcmJu05nAT6VcAQERkkInnAV4A/Bm8gIv2Bl4CbVPWDoPuLRKTE/zvwOWBDsgpvjDGnuqrSfEo8bhpb2vC4XTS2tFHicVNVkp+054ibulHVFhG5A1gM5ABPq+pGEbnd9/hjwP8FyoFfiAhAi6qOAaqB+b773MBcVX01aaU3xpgssO9YIzeOG8ANY/szd2UN9UlukBXVrpcOHzNmjK5ebV3ujTHGKRF5x1fBbsdGxhpjTJazQG+MMVnOAr0xxmQ5C/TGGJPlLNAbY0yWs0BvjDFZzgK9McZkOQv0xhiT5SzQG2NMlrNAb4wxWc4CvTHGZDkL9MYYk+Us0BtjTJazQG+MMVnOAr0xxmQ5C/TGGJPlLNAbY0yWs0BvjDFZzgK9McZkOQv0plMuvfRSysrKaGxszHRRUmbu3LkMGDCAoqIipk+fzoEDB6Ju+/bbbzN27FhKSko499xzWbZsWeCxBQsWMGHCBLp3707Pnj259dZbOXr0aODx//7v/6aiooKRI0eyYcOGwP1vvfUW06dPT82LM6cHVe1yP5/61KfUdH3bt29Xl8ulZWVl+uKLLyb12M3NzUk9Xkdt2LBBi4uL9W9/+5sePXpUr7/+ev3yl78ccdv9+/dreXm5vvjii9rS0qLPPfecdu/eXQ8cOKCqqs8//7wuWrRIjx8/rgcOHNDJkyfrzJkzVVV19+7dOmzYMD18+LD+9Kc/1SlTpqiq9zyMGzdOt2/fnpbXa05dwGqNElOtRm867Nlnn2X8+PHcfPPNPPPMMzQ2NtK9e/eQ2mh9fT0FBQXU1dUB8MorrzBq1Ci6d+/OhRdeyLvvvhvYduDAgTz88MOce+65FBUV0dLSwkMPPcTgwYMpKSlh+PDhzJ8/P7B9a2sr3/72t6moqGDQoEH87Gc/Q0RoaWkB4PDhw3zta1+jV69e9OnTh1mzZtHa2prQa3z++ef5/Oc/z8UXX0xxcTH3338/L730UkhN3O/tt9+murqaL33pS+Tk5DBjxgwqKyt56aWXALjhhhuYPHkyhYWFlJWVceutt/LWW28BUFNTw+jRoyktLeXyyy9n27ZtAPzkJz9h6tSpDBw4MKFyGxPMAr3psGeffZYbb7yRG2+8kcWLF3Po0CGuvfZa5s2bF9jmxRdf5JJLLqGqqoo1a9Zwyy238Pjjj7N//35mzpzJ1KlTQ9I+8+bNY8GCBRw6dAi3283gwYNZunQphw8f5j/+4z+YMWMGe/bsAeBXv/oVixYtYt26daxZs4aXX345pHxf/epXcbvdbN26lbVr1/Laa6/x5JNPArBs2TK6d+8e9cefctm4cSPnnXde4JiDBw8mLy+PDz74oN358Neewu8LvvAFe/PNNxkxYgQAZ555JuvXr+fQoUO88cYbjBgxgp07d/LCCy9w9913O/6bGBNRtKp+Jn8sddP1LV26VN1ut9bX16uq6rBhw/SRRx7R119/XQcNGhTY7sILL9RnnnlGVVVvv/12nTVrVshxhg4dqn/9619VVXXAgAH61FNPxXze8847T19++WVVVZ04caI+9thjgcdef/11BbS5uVn37t2reXl5euLEicDjc+fO1UsvvTSh13nZZZfpL3/5y5D7evfurX/5y1/abbtv3z7t1q2bzp07V5uamvTXv/61iojedttt7bZ97bXXtHv37rp58+aQ8o0ePVonT56sH330kV5zzTX6xhtv6AsvvKAXX3yxTp06VXfu3JlQ+c3pg86mbkRksohsFpGtInJvhMdvFJF3fT9vi8h5Tvc1p6ZnnnmGz33uc1RUVADetMQzzzzDZZddxsmTJ1mxYgU7duxg3bp1XHPNNQDs2LGDH//4xyE15507d7J79+7Acfv16xfyPM8++2wg1eNPC+3btw+A3bt3h2wf/PuOHTtobm6mV69egX1nzpwZSCE5VVxczJEjR0LuO3LkCCUlJe22LS8v5w9/+AOPPPII1dXVvPrqq1x++eX07ds3ZLvly5dzww038Lvf/Y6hQ4cG7r/++utZs2YNixYtYsOGDXg8HkaPHs3dd9/Nn/70J770pS9Z7d50iDveBiKSA/wc+CxQC6wSkT+q6ntBm20HLlHVgyJyJfAEMM7hvuYUc/LkSV588UVaW1vp2bMnAI2NjRw6dIj169dz3XXXMW/ePKqrq7n66qsDQbFfv37cd9993HfffVGPLSKB33fs2MGtt97KkiVLuOCCC8jJyWHUqFGB9EivXr2ora0NbL9z587A7/369cPj8bBv3z7c7vZv86VLl3LllVdGLceiRYu46KKLGDFiBP/4xz8C92/bto3GxsaQAB3skksuYdWqVQC0tLQwePBgvv3tbwceX7t2LVOnTuXpp59m0qRJEY9x8uRJvve977Fo0SK2bNlCv379KC0t5dOf/jQPPvhg1DIbE1W0qr7/B7gAWBx0+7vAd2NsXwbs6si+/h9L3XRtc+fO1bKyMt2xY4fu2bMn8HPRRRfpt771LV2+fLn27NlTR4wYEUizqKquWrVK+/btq8uXL9e2tjY9duyYvvLKK3rkyBFV9aZuXn/99cD2GzduVI/Ho5s2bdKWlhZ9+umnNScnR3/1q1+pquovfvELHT58uNbW1urBgwf18ssvD6RuVFWnTp2qd955px4+fFhbW1t169atgTSRUxs2bNCSkhJ988039dixY3rjjTdG7XWjqrpmzRptamrSw4cP61133aUXXnhh4LH169drVVWVvvDCCzGf83vf+54+8sgjqurtjdOjRw/du3ev/vKXvwz0xjEmHDFSN04C/ReBJ4Nu3wT8LMb2d/u3T2Rf4DZgNbC6f//+6TgvpoOuuOIK/da3vtXu/t/85jdaXV2tzc3NOnjwYC0rK9PGxsaQbRYtWqRjxozRbt26ac+ePfWLX/xi1ECv6g16ZWVlWl5ert/85jf14osvDgT65uZm/cY3vqE9evTQgQMH6iOPPKJut1vb2tpUVfXQoUN6++23a58+fbS0tFRHjRql8+bNS/j1Pv/889qvXz8tLCzUqVOn6v79+wOPzZw5M9BFUlX1K1/5ipaWlmppaaled911+vHHHwceu/nmm1VEtKioKPAzfPjwkOfatGmTjhkzJqR76Y9+9CMtLy/Xs88+W999992Ey29OD7ECvWhYL4FwIvIl4ApV/Rff7ZuAsar6bxG2nQj8ApigqvsT2TfYmDFjdPXq1THLZUy4RYsWcfvtt7Njx45MF8WYhNQdaeCOeWv52Q2jqSrJ79AxROQdVR0T6TEnjbG1QHALWV9gd/hGInIu8CQwTVX3J7KvMR1x8uRJFi5cSEtLC7t27eI///M/Aw2/xpxKZi/ZwqqPDjD7jS0pOb6TGr0b+ACYBOwCVgE3qOrGoG36A38G/klV305k30isRm+cOHHiBJdccgmbNm2ioKCAKVOm8Oijj1JaWprpohnjyLBZi2hsaWt3v8ftYvMD0TsLRBKrRh+3142qtojIHcBiIAd4WlU3isjtvscfA/4vUA78wtdrokVVx0TbN6HSGxNFYWFhoIeLMaeipfdM5IGF7/Paxr00NLeRn+viihE9uW/K2Ul9nriBHkBVFwILw+57LOj3fwH+xem+xhhjoKo0nxKPm8aWNjxuF40tbZR43B3O00fjKNAbY4xJjX3HGrlx3ABuGNufuStrqD/akPTniJujzwTL0RtjslUyethE0tleN8YYY5Ik1T1sIrHUjTHGpEF4D5s5K2qYs6KmQz1sEmU1emOMSYOl90xk6qje5Od6w25+rotpo3qz9DsTU/7cFuiNMSYN0tXDJhJL3RhjTJqko4dNJNbrxhhjsoD1ujHGmNOYBXpjjMlyFuiNMSYD6o40cN3jf6cuDXl6C/TGGJMB6Rw4Zb1ujDEmjTIxcMpq9MYYk0aZGDhlgd4YY9IoEwOnLHVjjDFplu6BUzZgyhhjsoANmDLGmC4qHd0sLdAbY0wGpaObpeXojTEmA9LZzdJq9MYYkwHp7GZpgd4YYzIgnd0sLdAbY0waRGp09XeznP+vn+HGcQOoP9aYkue2HL0xxqRBcKPrnZOGcMe8tfzshtGBGvwD00em7LkdBXoRmQw8CuQAT6rqQ2GPnwX8L3A+cJ+q/k/QYx8BR4FWoCVaP09jjMlG0RpdAWa/sYUHrjkn5WWIO2BKRHKAD4DPArXAKuB6VX0vaJsqYAAwHTgYIdCPUdV9TgtlA6aMMdmi7kgDDyx8n9c27qWhuS3iNsnoadPZAVNjga2quk1Vm4AXgGnBG6hqnaquApo7VVJjjMkywY2ueTkCQI7L+386JjQDZ4G+D7Az6Hat7z6nFHhNRN4RkduibSQit4nIahFZXV9fn8DhjTEms+KNbvU3ur789QkMqSqmtU3TNqEZOMvRS4T7Epkg5zOqutuX3nldRDap6pvtDqj6BPAEeFM3CRzfGGMyKrihNVLO/fGbPsmonFFZxLgzytM2oRk4C/S1QL+g232B3U6fQFV3+/6vE5H5eFNB7QK9McacajoyujU46N952ZncMW8tdUcbUlqrd5K6WQUMEZFBIpIHfAX4o5ODi0iRiJT4fwc+B2zoaGGNMaYr6ezo1nQtJxi3Rq+qLSJyB7AYb/fKp1V1o4jc7nv8MRHpCawGSoE2EfkGMByoAOaLiP+55qrqq6l5KcYYk14dHd2a7uUEHfWjV9WFwMKw+x4L+n0v3pROuCPAeZ0poDHGdGUdWURk6T0TQ7pc5ue6uGJET+6bcnZKymgjY40xphOCc+5OR7emezlBm+vGGGMSlIzFQtI1zw3YUoLGGJOwWfPX8/zKGm4c2z/qFAZ1RxrazWeTSrFGxlrqxhhjHEqkETVe3/p0shq9McY4FD5vTXAjqr/WHn4x8EtVjxo/WxzcGGOSwEkjajpXjnLKUjfGGJOAeN0p092jxgkL9MYYkwAn3SkjXQzS3TgbzHL0xhiTBk566nSG9boxxpg0Ca+5p3u6g0isMdYYY5IofKKyrtA4azV6Y4xJglg19y9+qm9GG2etRm+MMUkQq+aezukOIrEavTHGJEGsbpX3TxvJHfPWUlGS53jis2SyGn0KJWPiI2PMqSNazT1dC4xEY90rUyjV3amMMV1bOqdDiNW90gJ9CmRqrgtjTPrFGgjlZG6cZLG5btKsK3SnMsakR6y0jJPpENKR4rXG2BToinNdGGMi6+jUBE4HQsWbGycd0xlb6iZFZj63msqS/JA/bvAcGcaYrqGjbWn+tMziDXtobFE8bmHyyF6O0zLJTvHaFAgZ0JF1JI0x6dPZqQk++eburSw3tmhC39zTuUC45eiNMaelzralDZu1iOdX1ITcN2dFDcNmLXK0f3CKVwQam9v4+4f7E3sRDlmgN8acljrblpaMThf7jjUypLIYVSjNd1N3tDElfe0tdWOMOW3FayiNJdELRbxZLQ83tACpmd3SUY1eRCaLyGYR2Soi90Z4/CwR+buINIrI3Ynsa4wxmfL4TWN4YPpIhvcu5YHpIxPuMJHIHDbRZrX0uCVkO487+d2x49boRSQH+DnwWaAWWCUif1TV94I2OwDcCUzvwL7GGHNKctLpIt6slk2tikugzdcBMhXdsZ3U6McCW1V1m6o2AS8A04I3UNU6VV0FNCe6rzHGdGWdHdDkZFbL8WeU063AW+8eUlWc9NktnQT6PsDOoNu1vvuc6My+xhiTcZ2dkCxaLh+FgyeaeXFVDW9/uJ/DJ705+i11x1i88WPHvXeccNIYKxHuczrKyvG+InIbcBtA//79HR7eGGNSI5lLAEZq9PVfQK4d3YdjjS28/t7HtCkp6U/vJNDXAv2CbvcFdjs8vuN9VfUJ4Anwjox1eHxjjEmJzg5oCu5lE5zL/+3qnSEXkN+v2RX4PUcyl6NfBQwRkUEikgd8Bfijw+N3Zl9jjMmYzk5IFi3ls/SeiQwsL4z4nK0KqjB3ZU3Exzsqbo1eVVtE5A5gMZADPK2qG0Xkdt/jj4lIT2A1UAq0icg3gOGqeiTSvkl9BcYYkyIdmZAsVsoHiDi/jV+qpkGwSc2MMSZBsSYk86d8Fry7h1Zfn0mP28X8r19IRZGHBxa+zyv/2E2bQp5byMtxcayxFY/bRVNrW4cXKrJJzYwxJoli5e8vevgv7S4CjS1tXPXoskDfeYVAYK8uyWP66KoOjc51ygK9McYkKFb+fuk9E7n6p8uoO9q+L3xjSxvzVta0Swf5B1ulaqZbC/TGGNMB0fL3Ex7+C02tkfPwk0dU81/TRwYadNM1hbkFemOM6YBo0x9cfW4vXlq7K9IufFh/PGK3yY6ucuWUBXpjjHEgWjD237+u5iBNrbE7t2ypO8bAexe0G3SV6uUELdAbY4wD0YJx8AjX5jYNNNDmCIw9owe1B05Se/AkCuS4hKvP7RXoPpnM0bexWPdKY4yJIVpXylj8PWqET2alDH988wNXBtadjdR7J9EUTqzulbbClDHGxBBt9smFd05od3+vbvl84fy+gfnpLx5SSc9u+eT4Zv3KEejVLT8w13xnV7lyygK9Mea0Fm8a4mjBeHjvbu3un3RWFT++7rzAQia/vmUsk86qog1vLb4NmHRWVUgg33XwBJXFHp766qfjLl7SURbojTGnNSfTEEdbScrJClORtgm+uPQtK6T+WCOvbtjToVWunLAcvTHmtBRrGoNkNoT6+XvnfP/zw7n5f1dFHFDVmee3HL0xxoSJtfJTKsxesoWV2w9w1ezIo2ZT+fzWvdIYc1pKdUNoIv3r81LYEAtWozfGnMac5NijideI68/9f/683kwd1RuPO9KCe9C/RwFDq4q59vw+KWmIBcvRG2NMTMG59e//6b3AyNhZ89fz/MqadtMKx+p37xJvv3p/yD+zqpiG5lZqD53s8PTEfrFy9BbojTGnnFTMDRMvoJ9ZWczW+mMdGgRVVpjHhYMr2HP4JHsPn6T+WBPHG1qIdDmwxlhjjMFZl0iIn16JdMy7XljHqo8OcMGDSxh47wLmrKhB1TtPjWr7IB/eiBop9+/vXz/31vGM6lfG0YYWrjynZ0hjMMDA8sKUNMZajd4Yc8pItEukvzZ+7eg+7Dx4MuI3gI5McRAuR+DDH04J3J753GqKPG4+2HuUYT1LOdbYzF831zt+no7U6q1Gb4zJCk67RA6btSikNv77NbtYud1bS492TP+arokQoG9ZARcNqQi5//GbxlCYm8PGPUcoyHXx+E1jaG2LHuT9z+0SUtLF8rQO9Il8rTvd2LkxXZHTLpH+4B2uVWHgvQsYNmtRu2M2tbYF5qSJ3D8mlEtAgYLcHH59y7jA/eEXmTkrahh47wIirUUysLyQa0f3oanV+3oUbK6bZHOa5zsd2bkxXZWTLpH+4C3iDch+0b4B7Dp4kspiD6P7lzG0upgLBpczqLwwZjn8uXr/HPODvruAuqMN7b51+EVKkje2tHG8qaXDXTydOi1z9PFWcE/lSi9dXbqHhRuTKjOfW01lST4HjjWycMPeQA08UjfGSF0lZz63msUbP456/PxcV6BXTc/SfD7af4JrR/eh9tBJ+pYVMH/tLlx4v0XkuAip0fu/jVSXehhQXpSUeGPdK8PEmgN69htbIvaNPV0kc35sY7oCf8APXtvVP3FYvIrNzU+v5KP9x9l9+CRNLRryeFNrG8kKny5g20NT4m4XizXGhomU53vlH7sZ+4Ml7fJqwbm800G65sc2Jl3unzaSDz4+SkVJXrvZIaM17s7/1wu57vG/86MvnctnzqwICfLgTbmoOsvl+xV7cqIG3Dbatx0k02kZ6KF9nu+iIRUJTXDUFRsrk1WmzgwLT5XOvLau+LcyqRP+9/a3N109e1m790B4xaahuY2/f7ifp5ZuD7RR7TvWyBfO7xMxqEer0Ofnuhjoy/F73C5EYPqoPlxzfp+YZU9VfsVR6kZEJgOPAjnAk6r6UNjj4nv8KuAEcLOqrvE99hFwFGgFWqJ9tQiWqX70981fz9yVNeTleL+WxUrfRBv+nEldsUzJ0pnXls3nxbTn/3vHG8HqF5zauWr20ojH9LhdTDmnFy+t3RVyf474cvDivWjsOdwQaAvo272AS4ZVccPY/nz+p8todZjnyXO7+CDJI2PjBnoRyQE+AD4L1AKrgOtV9b2gba4C/g1voB8HPKqq43yPfQSMUdV9TgucqUAfK5fn1xUbK7timZKlM68tm8+LaS+RgU/h74FY+04b1Zv7ppzNv7+8gW31x9lSdywwZ43/WE2tbSGBPTx+1B1pYPwPl0S88IT7wug+/PjLoxy9jmCdDfQXAN9X1St8t78LoKo/DNrmceCvqjrPd3szcKmq7jmVAr0TXbGxsiuWKVk689qy+bycTpzOaxPp711Z7GHnwZOBbXIErj6vd8h7oO5IA7c99w7VpR7+urmOxrB8/IxxoT1xKkvyOXiskQUb9lKYl8MZFUUM61XCsYaWmKtDfes369p9I4gl0QpJZxtj+wA7g27X+u5zuo0Cr4nIOyJyW4xC3iYiq0VkdX19vYNiZUZXbKzsimVKls68tmw+L11NKttBHlq0iZXbD/Dwwk0xt4v09wYYUlUMePvTtyr8/cP9IfvNXrKFf9QeYlv98XZBHrwdM/yNpH/dXM+c5TtYsGEvACeaWtmw+wi/f2cX908fGdgn0vk43tQSKEssqViAxEmgd9IGEWubz6jq+cCVwNdF5OJIT6KqT6jqGFUdU1lZ6aBYmdMVGyvjlSkZH6xlAkcAABdUSURBVMRMNWp25Hz7y7rr0Mku97fKRqkYYOcfYeqvBf9+7a64PVP2HWvkmtF9GFJVzLWj+zK8dylnVBYxY/wAnr1lHPm5LuqONjL7jS3tRrBuqTsW8ZguIRB05//rhZQX5UXcbuwPlgTK9vCrvovTok2B9+L900dyRmURvbvl07tbfmAgl0sIabhNRYUk5ambsGN9Hzimqv8T6zm7cuoGOjZFaiqmVU1EZxok/WXv172Al9bt6tKNmoGylhXw0tquXdZgyXh/RDpGqt93qWwHGXrfIpoizBsQ3lgZ/hojvdejlVN8/6gSkncPdtU5PfnFjZ8CvJ+jOStqOvR6glNA981fz/NBx+lXVsCnB/UImQQt0UXCO5ujd+NtjJ0E7MLbGHuDqm4M2mYKcAefNMbOVtWxIlIEuFT1qO/314H/UtVXYz1nRwN9st/U0Y7XkaCZqZ4fyfggnvHdBY56L3QFg7+7gEirtnXFsgZL1YXYyXE787lJZTuIvyb80f4TgfsGlhfy4u0XRPw8+kehOuUSGNCjiO37j0cN8gC9u+Wz/3hTp2e4jFseQCXyyF0nOj0y1ter5id4u1c+rao/EJHbAVT1MV/3yp8Bk/F2r/xnVV0tImcA832HcQNzVfUH8Z6vo4E+2cE0/HgdCZqZ7vnRmQ+ik54IyV70oaMX6WSWNZ3fvlJ5IY4k0nE7+7lJpFtyoiY8/GdqD54kL0doalX6lRWw9DuXAfF72XjcLiaP7MnMi8/gsTe3seDdPbQ6PVExjtmZgJ/jEi44owfLtu6Pu226G2NR1YWqOlRVB/sDtao+pqqP+X5XVf267/FzVHW17/5tqnqe72eEkyDfEdFmi3MyyixS3jna8VRJeNX4dK80Hy6RBsnwcxFpciZ/XjHRHKLT9TU7muONVlaRxMuazgndOvP+8L9PI8WuK0ZUc8WI6pjH7cznJlhn2lDqjjbEfG+M6F3KjPEDePnrE5gxfgDDe5cGHlt6z8RAbtv/GoNvN7Z4Bz9VlHgo8bhpbVNyxJuuGVRRRM9unpDzc+Hg8rjl7mytvrVNHQX5TDTGdnlOPizR3kzf/9NGVm4/wPf/uDHu8ZbdO5ESj5uG5jYEaGiO32jSFXp+OP0ghge44LL7A/zkkb2YMT7xRs1owTNZwSZSWa8c0TOhBlinZelIo3S0fTrz/oh1Ia4s9lBR7Il53GRVQh6/aQwPTB/J8N6l7aYYiCb4/RDrwhrt2MNmLWLsg0tC0joNzW0htwHqjjYy/sEl7DvWyIzxA3jmlrFUlngYUF7IpLOqQ0bEvv1h/ACcDh2pnMTjTtqRMijShyVHhDvmfvL1O/jNFCkNs3D9XgbeuyDwdSn8eG6X93iFuS6GVBWzte4Yg8oLWbB+D5NH9uT/zFnDb24fz/Be3dp99fcH2uCBFOkU/MF7IKgLmF/4uZizooY5K2rwuF1cOqyyXdkjHQMipzxiHXvzA1ey9J6JUVNLidp18ASVxR4euW4Ur27cG1JWJ+kYp2UJfy854d/n4UWb2q101NH3R/jFrU29F+IeRXmBY1w7ug+b9x5lWK+Sdhe8dFZC/H3V/7HzUEiXveCGzfD3Riz+v9Ur/9hNm3rTHFUlHhpaWqk/2hSybZvC4o0fexf3UKX+WCN9uxdQ7zvvc5bvcPw6/P3wl3+4j4/DnidZVGHuypqktuVlRaCH9h+Wv22uo/bQSS54cElIA43/zRSNPyj556d+5Mvn8eqGj/nz+x+z+3DoB3C7r/Yw46mVANw1bx2vf+uSkEBw56QhHDzRzP3TR1JVkh81SGZSrAAX/KGPV/bwAFh3pIGze5VSXerhbx/URwyeyQw2fcsK+duWfby6YU+7D4mT4ByvLPEuWpGE7/P7Nd6ughc8uCSw9Fy8C3EskS4SwceYNX89G/ccYWh1CQdPNFN3tCHk3KarEjJ7yRbW7TwEeBtU9x5poKHZe4ESvI2oiVzk/X8rxVsDbmpt45KhlZxoao06KKmxpS3w2ff/n5cjTDqriiWb6tptX5Dr4mRzaKqmVeEP63Yn8MoTI3jTwx2p6MQ8brZNUxyvgcb/Zvrs2VXcOW9dyCrshbk5nFFVxNM3fzowXbEoEVdqT1Rw1yq/REb8pbpxsDMNatHOuX/OjzMri9lafyzk2HdOGhJ4Tf/+8oa4U0905Pn9y7Ml0tgZaxoMJw3b4X8r/z5/jBIcUtUoH+tvsu2HHZsOtyPvQ6fTEvinEUjkfTfzudVsrz/OB3XHGFpVzKDKIt7bfYSdB09SmJfDiabWwLZFeTkcD7rtn6MmlqoSDxcNqWDBu3toCHoNeTlCq0Jbm6ZkErJIscKJ02qa4kh5x4HlhYiEDkZYvu1AuwDe1NrGhl1HQqYrTjTIF+TmkJfT/n5/vnfofQsD9zlt9EtV42Bw3jg8j1978KTjPHS0FXXalMBAFFVoUw3kzINfU0dyvLGePzjXnGgeOlZZotX4UQLnKlo7h9OVjoJ1ZoBatKX02iIspRfref2/v7f7MFf/dBkrt0d+H4aX1X97/r9eyOdGVIe8dvD2G5/ztXH0KyugX1lB3Paj4HJc9/jfGTprEYs3fswHvkFOH9QdY/HGjwPTHQQHeSAkyIOzbpj1Rxv5/ZpdIUEeoKlVaU1RkIfQkbjJkjWpG79IH8bWNm+AmTyiJzOeWhE1ddPisOuVS+CCweW8FaH1PDdHaGgh0B3Mz1+D+Px5vR2nADqSKkhEeLD1e2D6yECXOyd56EjnPPjreXDN96KH/+L4NTmtQcZLuSQzDx0p1TF7yRZWbj/A2B98svB0pHaO4JWOnJTDaVtApPPkPydA2ARcQrEnl+f+Zayj5wVYuf0AV81eFvG1+f9m4WX1335q2XbW1Rxq1zPokqGVTBhSEegqCbHTVv7jfX3uWrbvO85V5/TkzQ/2cayxpd1r9N/u36OIuz83hDvmrYt63GgqivLYd7yJvmUF1AbNlZMuye6zn3WBHkI/jE8u28ZfNtVx56Qzmf3GFkS804fujPHH8wdl8Y2YC9em8NbW/cwYP4CF7+6mVZUB5YXeVv99xwO5/X+ds4YjDd43oj/m+3O0LvGO8IvV6JfMhspg0S4geTkumlvb2jWWObm4hAdA7+RQbeTlCA3NbbhFqCrJj/iaLhlaSd3Rxnb542iBLlJgi5VrTmYeOviC+NvVO6N+IP0Bvu5oY6B95uanV1BVEtpYHEmiF/ho58nf0yT0AqM0tjQxd/knjX3v7T7Mlx9fTkNLK81BlZNYbVn+aQGildXP/34HmHJOLxqaW1m2pZ7ag6G9Y6IJP/72fccBb+eJYOEXkjaFE00tfOu37zp6nnD7jnsbWjMR5AFyk5xrybocfbiODlkeWlXMmVXFLNywN+Z2wR++uiMNXP3TZdQdjd6dL0ege2Ee487owaINe+PmxDs7GCXasPjgYOsS+Nzwaory3Ly0bhcDekSujSdSC/bnuncfOsmfN9XRu1s+b393UsTX5M/hOx2Y5p8F8Auj+/CdK8/K2NQS4ecxxyUhA3KGVIW+LqcDk8KPC5FHhEY7T+FTBMx8bjVvvFcXcT70vBwXLpe3a+Kg8kKG9Czh9fc+jrtEnn8qXX9ZF2/YG7iwV5bks+tQ9AApCYz+jNfGkY1cAsu/Nynh9/NpuWZstHkyYsl1Cf/7z2NDalvBiwuH83/4UBj34JKEc3Yzxg9o1+gXHphvfnoF7+05yiPXjWL+2lr+vKmOxd+82HHjbbQ1cP3BNt6w8WgfynhplVhzi1wytIK+PYr4zaqakBqkX16Oi8nn9Gz3TWbR+j0h6bBgM8aFNvB2JOh3pLHRfx478jGK9S0p2mjXPLeLZb4F7L//+eE89ua2dheaq0b2ZN/xppgXd//C1p0xun93Hr/pUzz6xpaQeVv8Fzgn58RJGrIz5/hU1ZH07GnTGBvcYFOS781K5YS3AsXQ3Kbc8utVfPDxUe6fPpL7p41kw67DUWv1H+0/wdgfLGFsB4I8wJzlO5j+i7dCGv2CZ70Db5fB+mONvLphDwW5ORw62dyuMSzS0mn+nHG0wT9zffdHipsu8TaWDasu5trz+wQayIKfJ14DcbQPpQLrdx3hzkln8tZ3LmPqqN7k+P5EOULIwLR2jZ4xVuics6KGsQ8uidpY6IT/3E/+f28y/edvBRr+whtDIzVi/+z6UXjcrpBGR//vHreLPt0L8Li9d+Tnupg8opqze5W2a7z03754SCUDywvJc4e+5s+f0ytw7ueuqAkM4AMC3yYWbtjLyu0HGP/gJ20GkZbM66y1NYcY+4MlIUEePml8jyd4VshIlm2pZ/B3F/D8itMryIM3Rz/w3gVJO15W1eg7M7Oc3xdG9wlMDAWf5CkHlhdSc+CE4zlFnPLXvtbVHIxaW43G/xU93tJpQLsUzLdfXBeSP40kvDYf6/yG10AiTUjllD+/Hd7N8f5pIxM6psftYmlQ7ff7f3ovUMv156V/edP53PTkyogXav/Mhv7uoLc99w4icEZFUbuZMYPPTaTVh/wXrGjpquB0lH91ofAZDqNxARcNq+RvmyOv4+B/n8x8bjXFHjeb9x5lQHkh79Qc4uPDDUntPZIj0DtOG5hfeXEei+66KOTbk39glQhs/fgoRxtbYxwhu+W6YMuDzrvCZn3qJpElxFLBSZ/cZDyH4g0g/q/oQuzFhP3lynN7G1lvHNuf375Tm9RzFS2H35HaSI5LqCrx8Ic7PgNKxDSK0+P2KyugvNjD4EpvUPYH1h6FefQvL+TQ8Sa27z+Rlr+d39CqYrbvPx4xXRVOBC4ZUkGhx81bW/dz+GRzxO0uHVbJoRPNnFFZxEtrdoVcZPzvk+CLR3A7gQLPr6iJ+z5KpRyBD384JWQGzt8nsApTtir25PDnuy9NKAWZ9YH+kwahPRFXiElEMnKXyeJ2QUub8wtJfq6LimJPyGx/Q6uK+clXRgdqxXdNGsJNT63kwPEmxx/uS4dW8I/aw5xoagk5v/4LSGWxh1funBDypnxv9+GQLnnx5Ih3zEKPwjz69Sikd/d8Fq7fy1Xn9GTPIW+t84l/+hTjftCxNFmqTB5RzcnmVt7csi+QXog15a3/8TwH6ZMZ4/ozb2WNo7+9AFed0wtVjduBwHR9Bbku3r/fcvQh/PnHRFMf4Xp3z+9wkK+IsuqMU5GaEvwV7+CX5Q76i3ncEhj96U8NHPR1C7t8eE+uPb8Pe46c5EuPv83kkdUcPNHMlNnL2O8wyPuLtGP/SQ6eaG53EW1qaUOVwIo9wYb37sagoJkE42n1Da7af7yJdTsPBbrPLVy/l7U7D7Fu5yFmv7GFBXdOCLzmcMWeCCPVUsR/blwirNh+ICSHHCvITxvVm+mj+3jTOFFeh9+cFc6CPHhr5AvW7wl0oxwzoHvIh7u6xBOyfU6E95vpOi4emtxV9rKiRg+Jzcl9qgnvthduUEURtQdPOEoJpFpwrj6ZjUmR+FMOlcV51B9LzQRTyVZWkEsbyoQzK3j7w/0cPBE5JdNZpfnuwBgOc+r66KHk5OizZsCU2+VKuDvlqSLeYgn+QSSZFtyLItlDuCNRvF9Jo+Wvu6KDvrIuWJ/a9IoF+VNbVYmHX9/y6aQdL2sCvcm8a0b1AU19TT5YG3Q6ZWdMV9OtIJfhvbol7XhZE+iztTZ/Kpm/blesru7GGIe2+CZrS5asaIw1XUObErdvvjEmtitGVLPyvklJPWZWBPp05IONMSYdKos9SZ+3KSsCfSYHSxljTDKlYj76rAj0cbojG2PMKaMjC7THkxUh0ir0xphskYoF2rMi0BtjTLbwzzSbTI4CvYhMFpHNIrJVRO6N8LiIyGzf4++KyPlO902GXBvPbYzJIgvvmpDU48UN9CKSA/wcuBIYDlwvIsPDNrsSGOL7uQ34ZQL7dlpXGPpvjDHJkszBUuCsRj8W2Kqq21S1CXgBmBa2zTTgWfVaDnQXkV4O9+20hXcm9+pnjDGZUlHcuQkSI3ES6PsAO4Nu1/ruc7KNk30BEJHbRGS1iKyur4+8gEI0w3t3oygvfTMXGmNMKvQrK2D1rM8m/bhOAn2kBHh4riTaNk729d6p+oSqjlHVMZWViU/RWeTJmtkcjDGnEKdNhMWeHDxuF0V5Oe2mjR5QVkC/sgKG9y5NQQmdBfpaoF/Q7b5A+JLs0bZxsm9SrLzvcj56aApXjKimX1lB4P7gPva5LsHt8v7v53G7KMj1rvXpEhjQo5AeRbnAJ7eD5z/PVLNv+Hqk6SZEnjM/Fc/j5xKizj0fiSdofdVYRe1RmEuOy7u4QyTBH9xoxynJdwe+RQ7oURDSISB8n8LcT75tDuhRyEVnVgSeY0CPwpD3Yzj/+7Mg18WUc3ox5Zxegb+D9/xI2Paf3PbvU13qoSgvh7LCXIZWF3Ph4HJmjB/AFSOquWJENS7xBqHR/boF/s4XDO4ROE7wa/P/PQQYVl0ceK0ClBflBc5pQa6LCweXM7S6mFyX0K3Azbl9S8lxfVL28AApYf/7dct3I777/efc7YJuBZ9U7i46s4IBPQoDn+OCXBe9Sj2B8rhdwr2Th9GtwE2e2xUSK/znKEe88/b7f/e4hQE9Cqku9ZCf66JfWQE9CnMDr2VodTFVJR4+/OEU8twuuhW4GVJVFChrdYn3vPvP92fOrGDzA1ey8b8mM6p/d2aMH8DCOy9ixvgBnNW7lKXfuSywdnSyxZ2PXkTcwAfAJGAXsAq4QVU3Bm0zBbgDuAoYB8xW1bFO9o2ko2vGGmPM6apT89GraouI3AEsBnKAp1V1o4jc7nv8MWAh3iC/FTgB/HOsfZPwmowxxjiUNStMGWPM6Szr14w1xhgTnQV6Y4zJchbojTEmy1mgN8aYLNclG2NFpB7YkelydCEVwL5MF6ILs/MTm52f2LLl/AxQ1YijTbtkoDehRGR1tNZ0Y+cnHjs/sZ0O58dSN8YYk+Us0BtjTJazQH9qeCLTBeji7PzEZucntqw/P5ajN8aYLGc1emOMyXIW6I0xJstZoO+CRKSHiLwuIlt8/5dF2e6bIrJRRDaIyDwRyU93WTMhgfPTXUR+JyKbROR9Ebkg3WXNBKfnx7dtjoisFZFX0lnGTHFybkSkn4j8xfee2Sgid2WirMlkgb5ruhdYoqpDgCW+2yFEpA9wJzBGVUfinQb6K2ktZebEPT8+jwKvqupZwHnA+2kqX6Y5PT8Ad3H6nBdwdm5agG+r6tnAeODrIjI8jWVMOgv0XdM04Bnf788A06Ns5wYKfAu8FJKi1bu6oLjnR0RKgYuBpwBUtUlVD6WthJnl6P0jIn2BKcCTaSpXVxD33KjqHlVd4/v9KN4LYcS1rk8VFui7pmpV3QPeNx1QFb6Bqu4C/geoAfYAh1X1tbSWMnPinh/gDKAe+F9fauJJESlKZyEzyMn5AfgJcA/Qlq6CdQFOzw0AIjIQGA2sSHnJUshW1M4QEXkD6Bnhofsc7l+Gt3YyCDgE/FZEZqjqnOSVMnM6e37wvrfPB/5NVVeIyKN4v6b/e5KKmFFJeP9cDdSp6jsicmkyy5ZpSXjv+I9TDPwe+IaqHklG2TLFAn2GqOrl0R4TkY9FpJeq7hGRXkBdhM0uB7arar1vn5eAC4GsCPRJOD+1QK2q+mtivyN2rvqUkoTz8xlgqohcBeQDpSIyR1VnpKjIaZOEc4OI5OIN8s+r6kspKmraWOqma/oj8FXf718F/hBhmxpgvIgUiojgXYD9dGlUi3t+VHUvsFNEhvnumgS8l57iZZyT8/NdVe2rqgPxNuL/ORuCvANxz43v8/QU8L6qPpLGsqWOqtpPF/sByvH2CNji+7+H7/7ewMKg7f4T2ARsAJ4DPJkuexc7P6OA1cC7wMtAWabL3pXOT9D2lwKvZLrcXeXcABMA9b1v1vl+rsp02TvzY1MgGGNMlrPUjTHGZDkL9MYYk+Us0BtjTJazQG+MMVnOAr0xxmQ5C/TGGJPlLNAbY0yW+//kStkmSFMkZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAENCAYAAAAfTp5aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3jU5Znw8e89MzkSwjHhEI5aQEElaABrhULpKh4qUK2rqG23B2Wttd3aVynw7na3SLW79VLULbrVFkuVdl9FXSFrlVpFq0AUFLEqyCGEU8IxBHKe+/1jMsPMZCbzSzKTyQz357pyXZn5/TLz/JLJPc/cz/Pcj6gqxhhjUp8r2Q0wxhgTHxbQjTEmTVhAN8aYNGEB3Rhj0oQFdGOMSRMW0I0xJk1YQDfGmDRhAd2c0URERaTNxRgisqvlvBFd0ypjOsYCujHGpAkL6MYYkyYsoBtjTJqwgG6MMWnCk+wGGNMdiMhP2zjcu6vaYUxniFVbNGeyWDNcwoxU1V2JaosxnWUpF2MAVZVoX8DuZLfPGCcsoBtjTJqwgG6MMWnCAroxxqQJC+jGGJMmLKAbY0yasIBujDFpwuahG2NMmrAeujHGpAkL6MYYkyYsoBtjTJqwgG6MMWkiadUW+/fvryNGjEjW0xtjTEp69913D6lqQaRjSQvoI0aMoKysLFlPb4wxKUlEohaLs5SLMcakCQvoxhiTJiygG2NMmrCAbowxacJRQBeRmSLyiYhsF5H5EY7/HxHZ3PL1oYg0i0jf+DfXGGNMNDEDuoi4gUeBK4CxwI0iMjb4HFX9d1UtVtVi4CfA66p6JBEN7ozK6jquf+xtKk/UJbspxhgTd0566JOA7aq6Q1UbgJXArDbOvxF4Jh6Ni7ela7excdcRlr66LdlNMcaYuHMyD70I2BN0uwKYHOlEEckFZgJ3RDl+K3ArwLBhw9rV0M4Ys6iU+iZv4PaK9eWsWF9OlsfFJ4uv6LJ2GGNMIjnpoUuE+6LV3P0K8Fa0dIuqPq6qJapaUlAQcaFTQqy7ezrXFA8mO8N3udkZLmYVD2bdPdO7rA3GGJNoTgJ6BTA06PYQYF+Uc2+gG6ZbCvOz6Znlob7JS5bHRX2Tl55ZHgp7Zie7acYYEzdOUi4bgVEiMhLYiy9ozw0/SUR6AV8Ebo5rC+PkUE09N00eztxJw3h6QzlVNjBqjEkzMQO6qjaJyB3Ay4AbeFJVt4rIvJbjy1pOnQP8SVVPJqy1nfDYLSWB7xfPPi+JLTHGmMRI2hZ0JSUlasW5jDGmfUTkXVUtiXTMVooaY0yaSOmAbguFjDHmtJQO6LZQyBhjTkvaBhedYQuFjDGmtZTsodtCIWOMaS0lA7otFDLGmNZSMuUCtlDIGGPC2Tx0Y4xJITYP3RhjzgAW0I0xJk1YQDfGmDRhAd0YY9KEBXRjjEkTFtCNMSZNWEA3xpg0YQHdGGPShAV0Y4xJExbQjTEmTVhAN8aYNOEooIvITBH5RES2i8j8KOdME5HNIrJVRF6PbzONMcbEErPaooi4gUeBvwMqgI0i8qKqfhR0Tm/gP4GZqlouIoWJarAxxpjInPTQJwHbVXWHqjYAK4FZYefMBZ5T1XIAVa2MbzONMcbE4iSgFwF7gm5XtNwXbDTQR0T+IiLvisjXIz2QiNwqImUiUlZVVdWxFhtjjInISUCXCPeFF1H3ABcBVwGXA/9XREa3+iHVx1W1RFVLCgoK2t1YY4wx0TnZsagCGBp0ewiwL8I5h1T1JHBSRN4AxgOfxqWVxhhjYnLSQ98IjBKRkSKSCdwAvBh2zgvAFBHxiEguMBn4W3ybaowxpi0xe+iq2iQidwAvA27gSVXdKiLzWo4vU9W/icj/Ah8AXuDXqvphIhtujDEmlO0paowxKSQt9xStrK7j+sfepvJEXbKbYowx3ULKBvSla7excdcRlr66LdlNMcaYbsHJLJduZcyiUuqbvIHbK9aXs2J9OVkeF58sviKJLTPGmORKuR76urunc03xYLIzfE3PznAxq3gw6+6ZnuSWGWNMcqVcQC/Mz6Znlof6Ji9ZHhf1TV56Znko7Jmd7KYZY0xSpVzKBeBQTT03TR7O3EnDeHpDOVU2MGqMMTZt0RhjUklaTls0xhgTygK6McakCQvoxhiTJiygG2NMmrCAbowxacICujHGpAkL6MYYkyYsoBtjTJqwgG6Sbtq0afTp04f6+vpkNyUh9u/fzzXXXMPgwYMREXbt2hX13PLycvLy8kK+RIRf/vKXACxZsiTkWE5ODi6Xi0OHDgHw7//+7/Tv35/zzjuPDz88vcfMW2+9xezZsxN6nSb5LKCbpNq1axfr1q1DRHjxxfCdDTunqakpro/XUS6Xi5kzZ/Lss8/GPHfYsGHU1NQEvrZs2YLL5eLaa68FYMGCBSHH77nnHqZNm0b//v3Zv38/TzzxBDt27GDevHnMnz8f8P0e7rrrLh588MGEXqdJPgvoJqmeeuopLr74Yr75zW+yfPly6uvr6d27d0jvsqqqipycHCorKwF46aWXKC4upnfv3lxyySV88MEHgXNHjBjB/fffzwUXXECPHj1oamrivvvu4+yzz6Znz56MHTuWVatWBc5vbm7mrrvuon///owcOZJHHnkEEQm8GRw/fpxvf/vbDBo0iKKiIhYtWkRzc3O7rnHAgAHcfvvtTJw4sUO/n6lTpzJixIhWx1SV3/3ud3zjG98AfL37CRMmkJ+fz5e//GV27NgBwIMPPsg111wT8TFMmlHVpHxddNFFaszZZ5+tjz76qJaVlanH49EDBw7oP/zDP+iCBQsC5zzyyCN6+eWXq6rqu+++qwUFBfrOO+9oU1OT/va3v9Xhw4drXV2dqqoOHz5cx48fr+Xl5Xrq1ClVVf3jH/+oe/fu1ebmZl25cqXm5ubqvn37VFX1V7/6lZ577rm6Z88ePXLkiM6YMUMBbWxsVFXVWbNm6a233qo1NTV68OBBnThxoi5btkxVVdetW6e9evWK+rVu3bqQa21sbFRAd+7c6fj3c9ZZZ+lvfvObiMdef/117dGjh544cUJVVQ8dOqTjxo3To0eP6sMPP6zXXXedlpeX60UXXaT19fWOn9N0b0CZRomrFtBN0qxbt049Ho9WVVWpquqYMWP0gQce0FdeeUVHjhwZOO+SSy7R5cuXq6rqvHnzdNGiRSGPM3r0aP3LX/6iqr6A/sQTT7T5vOPHj9fnn39eVVWnT58eCNCqqq+88kogoB84cEAzMzMDbwyqqk8//bROmzatQ9fb3oD+xhtvhATscN/61rf0G9/4Rsh9Tz/9tE6YMEFnzpypu3bt0jlz5uirr76qK1eu1KlTp+o111yje/bs6VD7TffQVkB3VD5XRGYCDwFu4Neqel/Y8WnAC8DOlrueU9V/i8tHCJO2li9fzmWXXUb//v0BmDt3LsuXL+e9996jtraW9evXM3DgQDZv3sycOXMA2L17N8uXL+fhhx8OPE5DQwP79u0L3B46dGjI8zz11FM88MADgcHImpqawCDivn37Qs4P/n737t00NjYyaNCgwH1er7fV4yfK8uXLufbaa8nLy2t1rLa2lv/+7//mhRdeCLn/xhtv5MYbbwRg9erVZGVlMWHCBMaPH8/WrVt58cUX+fGPf8zKlSu75BpM14oZ0EXEDTwK/B1QAWwUkRdV9aOwU9ep6tUJaKNJQ7W1tfzxj3+kubmZgQMHAlBfX8+xY8fYsmUL119/Pc888wwDBgzg6quvpmfPnoAv4C5cuJCFCxdGfWwRCXy/e/duvvvd77J27Vo+//nP43a7KS4u9n08BQYNGkRFRUXg/D179gS+Hzp0KFlZWRw6dAiPp/W/yrp167jiiujbHpaWljJlyhSHv5FQ/oAdnO8P9txzz9G3b1+mTZsW9ecXLFhAaWkp27ZtY+jQoeTn5zNx4kSWLFnSoTaZ7s9JD30SsF1VdwCIyEpgFhAe0I1x7Pnnn8ftdrNlyxYyMzMD919//fU89dRTzJ07l9mzZ9OvXz/uvffewPHvfve7zJkzhy9/+ctMmjSJU6dO8Ze//IWpU6cGgn6wkydPIiIUFBQA8Jvf/CZkwPX666/noYce4qqrrqJHjx7cf//9gWODBg3isssu46677uJnP/sZeXl57Ny5k4qKCr74xS8yZcoUampqHF1vXV1dYDC1vr6euro6srOj77K1atUqevfuzfTpkbdWXL58OV//+tdD3ryCLV68mG9+85uBqZKffPIJBw8e5LXXXuOss85y1GaTgqLlYvxfwHX40iz+27cAj4SdMw04DLwPlALjojzWrUAZUDZs2LCuSDeZburyyy/XH/3oR63u/8Mf/qADBgzQxsZGPfvss7VPnz6tBvRKS0u1pKREe/XqpQMHDtTrrrtOq6urVdWXQ3/llVdCzl+wYIH26dNH+/Xrp//0T/+kU6dO1f/6r/9SVV9e+4c//KH27dtXR4wYoQ888IB6PB71er2qqnrs2DGdN2+eFhUVaX5+vhYXF+szzzzT7usFWn353XbbbXrbbbeFnH/ZZZe1Givwq6ioULfbrdu2bYt4/OOPP9aSkpLAwK6q6i9+8Qvt16+fnnvuufrBBx+0u/2m+6CNHHrMHYtE5GvA5ar6nZbbtwCTVPX7QefkA15VrRGRK4GHVHVUW49rOxaZ7qi0tJR58+axe/fuZDfFmIg6u2NRBRA8CjQE2Bd8gqpWq2pNy/drgAwR6d/B9hrTZWpra1mzZg1NTU3s3buXf/3Xfw0MwBqTapwE9I3AKBEZKSKZwA1AyJI+ERkoLck8EZnU8riH491YY+JNVfmXf/kX+vTpw4QJEzj33HP5t3+zCVomNcUcFFXVJhG5A3gZ37TFJ1V1q4jMazm+DF+e/R9FpAmoBW7QWLkcY7qB3NxcNm7cmOxmGBMXMXPoidKZHHpldR13PLOJR+ZOoLBn9JkCxhiTbjqbQ+92lq7dxsZdR1j66rZkN8UYY7oNRytFu4sxi0qpb/IGbq9YX86K9eVkeVx8sjj6Ag9jjDkTpFQPfd3d07mmeDDulrUUboFZxYNZd0/kxRfGGHMmSamAPuUXr/Hi5n00t6T9mxVe2LyPyUvWUnmiLrmNM8aYJEupgL7u7ukM7JWF2+XrortdQm6GG1Usn26MOeOlVEAvzM9mxjkD8LbMzGn2KqcaffUxVqwvZ8T81YxZVJrMJhpjTNKkVEAHOFRTz02Th7Pi25MY0S+Xls462Rkuy6cbY85oKTXLBeCxW05Pv/zC5/qze4Nvlkt9k5eeWR6bl26MOWOlXEAP5u+tz500jKc3lFNlA6PGmDNYSq4UNcaYM1XarRQ1xhjTmgV0Y4xJExbQjTEmTVhAN8aYNJGSAb2yuo7rH3vblvsbY0yQlAzo/vK5Vy9904K6Mca0SKl56OHlcytP1DPp3rVWPtcYY0ixHvq6u6cHlvoHq2/yWg0XY8wZL6UCemF+NrOLi0Lus5roxhjj4yigi8hMEflERLaLyPw2zpsoIs0icl38mhjqZEMTowrzAHCJrya61XAxxhgHAV1E3MCjwBXAWOBGERkb5bz7gZfj3chgj91SwlkFPbj54uG89P0p3HzxcKpq6hP5lMYYkxKcDIpOArar6g4AEVkJzAI+Cjvv+8CzwMS4tjCC4IqLi2efl+inM8aYlOAk5VIE7Am6XdFyX4CIFAFzgGVtPZCI3CoiZSJSVlVV1d62GmOMaYOTgB5hXgnhJRofBO5R1ea2HkhVH1fVElUtKSgocNpGY4wxDjhJuVQAQ4NuDwH2hZ1TAqwUEYD+wJUi0qSqz8ellcYYY2JyEtA3AqNEZCSwF7gBmBt8gqqO9H8vIr8FXrJgbowxXStmQFfVJhG5A9/sFTfwpKpuFZF5LcfbzJsbY4zpGo6W/qvqGmBN2H0RA7mqfrPzzTLGGNNeKbVS1M+qLRpjTGspGdD91RaXvrrNgrsxxrRI6WqLK9aXs2J9OQBLX93G4jnnJ6tpxhiTdCnVQ19393SuKR5MdkbrZq9YX86I+aut6qIx5oyVUj30wvxsemZ5qG/ykukWGpoVt0to9irZGS4uHzeQhVedm+xmGmNMUqRUDx3gUE09N00ezvPfu5RRhXk0e5Usj4v6Jq9VXTTGnNFSqocOpwtzVVbXcaimnmsvHMK3Lx3J0xvKqbKBUWPMGSzlArrf0rXbOFbbSE6Gi7GD863qojHmjJdyAT3aTBfbV9QYc6ZLuRx6+EyX7AyXbUFnjDGkYEAPnulig6HGGHNayqVc4PRMl7mThtlgqDHGtBDV8L0qukZJSYmWlZUl5bmNMSZVici7qloS6VjKpVyMMcZEZgHdGGPShAV0Y4xJExbQjTEmTVhAN8aYNGEB3Rhj0oSjgC4iM0XkExHZLiLzIxyfJSIfiMhmESkTkUvj39TTbJciY4xpLWZAFxE38ChwBTAWuFFExoadthYYr6rFwLeAX8e7ocGCt6ALFu9Ab28cxphU4qSHPgnYrqo7VLUBWAnMCj5BVWv09AqlHkBCViuNWVTKiPmrWbG+HNXWuxRFC/QdFe/HM8aYRIq5UlRErgNmqup3Wm7fAkxW1TvCzpsD/BwoBK5S1bfbetyOrBStrK5j8Zq/8aetB6hr9AZ2KSr98AANQRUY/TpagTG8omNnH88YY+KlsytFJcJ9rd4FVHWVqp4DzAZ+FqUht7bk2MuqqqocPHWoaIW53oxQgXHmuAGcOyi/Q+kSq+hojElFTgJ6BTA06PYQYF+0k1X1DeBsEekf4djjqlqiqiUFBQXtbiycLsy16vYvcNPk4VTV1EcM9J9VneT9imMdSpdYRUdjTCpyknLxAJ8CM4C9wEZgrqpuDTrnc8BnqqoiciHwP8AQbePB412c67bflVHQM5s/bCynsbn107Y3XeJ/vOCKjv7t74wxJlnaSrk4qrYoIlcCDwJu4ElVvVdE5gGo6jIRuQf4OtAI1AL/R1XfbOsxE1VtMVqefeFV51oP2xiT8toK6I7qoavqGmBN2H3Lgr6/H7i/M42MF0uXGGPOVCm/UjTSXPHwPHvF0VM2n9wYk/ZSPqBHmiv+2C0lLJ59HmMH57N49nkM6ZNr88mNMWkvJXcsqqyuY/LP1xKp6cGDnzaf3BiTbtJux6Kla7eh6gvMWZ7QueKrbr8kkF7pzvPJrayAMSbeUmqT6PAed/j3b392GI9IIL2yeM753XaANDhVtHjO+clujjEmDaRUyqWyuo6Lf74Wbzua7BKYO3m4o/nkldV13PHMJh6ZOyFhQd/SQMaYzkiblEthfjazi4scnz+reDDvLJgRMkDa1uKgrijG1Z3TQMaY1JZSKReAkw1NjCrMY3tlTaCgzIh+uRyorqOu0dfzdQl4FcfplfBe84r15axYX56QXrPNkzfGJEpK9dDBNyXxrIIejCrMA2B0YR5NXqW+yYurpYzYzPMGcfPFvjovTnR1rzlSPRpjjOmslOuhh/emP62sAXy98pe+PyWQJ188+7zAObFy413daw5O+wS30xhjOiPleujRetPvLJgRNU/uJDduvWZjTKpLuR56e3rT7cmNW6/ZGJPqUq6HDr7e9JwJRYwqzOOrE4ZErdViM0qMMWeSlAzoj91SQm6Gm637q8nJcEWt1RKpN+9xCXc8vclWaBpj0k5KLSyC6AtzggWnVMI3qnj9k0oqjtVy06RhtkLTGJNyOr3BRSJ0NKD7a6DsOnwK8M1uEaBZfSmVaaMLOFBdz+Nfvygkr24rNI0x6SBtVoqOWVTKpCVrA8EcfAuImlsKdbW1l6jl040x6S6lAro/KPsXEPnlZLjwqqIK2yprUPXNaBkxfzVjFpUCtkLTGJP+Uiqg+4NyeHGu2kYvjc2KSwj0wEXg8nEDQnrg/rnmT3xjIgV5WVQcPUWiWHlcY0xXS6mADvDMhvKox7xKoASAKuyoOhnSA/fvZPS/H+6nqqaeIX1yE9bOrij0ZYwxwRwNiorITOAhwA38WlXvCzt+E3BPy80a4B9V9f22HrOjg6KjF66hobl9A7n+gc+uGBi1wVdjTCJ1alBURNzAo8AVwFjgRhEZG3baTuCLqnoB8DPg8c41ObIxi0rbFczDBz67YmDUBl+NMcniJOUyCdiuqjtUtQFYCcwKPkFV/6qqR1tuvgMMiW8zffzB0onMloHPtz87HLivKwZGbfDVGJMsTgJ6EbAn6HZFy33RfBsojXRARG4VkTIRKauqqnLeyhb+YBmNf/LL6MI8nr/9C4wqyKPyRH1IHttpEa7ODGpaoS9jTDLEzKGLyNeAy1X1Oy23bwEmqer3I5w7HfhP4FJVPRx+PFhHc+i3/a6MvCwPf/64kqOnGkOOfemcQj7ce5yqE/VEuqr25LEXrdrC7zeU24pSY0y30lYO3Um1xQpgaNDtIcC+CE9yAfBr4IpYwbwzHrulhEWrtrQK5gB//rgSgGsvLKLRq/xp6wHqGr1kZ7i4fNxAFl51bszHH72wlIbmrtm9yBhj4slJQN8IjBKRkcBe4AZgbvAJIjIMeA64RVU/jXsrWzip4wLw7Ht7A9+3N4999QWDeG7TXtwuodmr7XozMMaYZIoZ0FW1SUTuAF7GN23xSVXdKiLzWo4vA/4Z6Af8p4gANEX7SNAZ6+6ezuI1fwv0vKPJ8vhWjl42bgDfmzYqsItRW8LfLJpbVi/VNdqgpjEmNThaWKSqa1R1tKqerar3tty3rCWYo6rfUdU+qlrc8hX3YA6nB0XbCubgW1zU2Kz0ycmMuotRuPDphm6BaWMKuPaiorgNasZ79aitRjXGBEurlaLhwuu5tCV8uqEXGNI7h19+rTjmm4FT8V49aqtRjTHBUm4LutnFRTy3aW/sE/GlXmaedzr/HWuzaP90Q3/t9FhpGqfasxVeMh7PGJMeUiagOx0QDdbQHJr/Du7R3jljVKvgnqh9RcNz/50daI334xlj0kPKpFz8Oe4sj7Mmjx3UM1BRccyiUkbMX82K9eWB0rqTlqxlw05fcE90Ljreq0dtNaoxJpKUCej+INbQ7MUtsc/fcehkoKJi+IBnsPDgnijxXj1qq1GNMeFSags6//6gn1Wd4HBNA/nZGZTtPhr7B/FtVadAhktoaNbAPPNIMj0uPo2Qi46Vg+/oucYY41RnV4p2G5Fmm5z1k9V4lTYDtEtg6qj+DOnbg7mThvGDlZvYVllDpsdFQ4S8/FfOHxTxcYJz8LHKAbTnXGOMiYeU6qEH8/eAczPdDOmTyx82ltPYRmnd4Bkg/p5+rJ/JdAuf3ntl1AHZTLeL4mG9Q3rhVg/dnInsE2nXSZtNov0qq+u4+uE32bDzCH1zM/n04Ale+N4XGNgrK+rPaNDPHj3VyJ0zPsdb93yJgb2yWuXk/b+Ur4z3leoNz8H7t7f7ygWDWs0Dt3ro5kxkayK6h5TroUfrAQtErLDot2HhDAp7Zreqorhw1Rae3lCOC4jWWc/yuLjuoiE8vaEcgVZ7mgaf98niKwKPmel20dDstYqNJm3ZJ9KulzY59LbmorcVzAf1ymbK/a9FXIzjErhp8nA+qzrBgeP1nKhr5OipxlaFuS5eshbVyM8TPg+8rQVK9tHUpBNbE9G9pFRA9794Vn+wP+oAaCQHq+t45yczor7wggPrj/6wmec27SXTLSHzu8N/3j8IG34etL1AyQZLTTqxNRHdS0oFdP+Lp9mruCR66iOcV2HKL17juouGxHzhvbPDV8p9yqgCBvXOCfSuI71wRxXm8dANEzpUzdGW65t0kaiSGab9Uiqgg+/Fc/PFvhfP15b9lZMNzTF/ZkDPLGrqm9hx6GTUF154wF3bsllG8MrUSC/c/nm+QdlH5k5osw2p8tHUUkKmvRJVMsO0X8oF9MduKaGyuo7JS9a2mTcPdvCEbxXl258d5qEbiinsmd3qhRdtbDj47kgv3EWrtjhKocTzo2kig66lhIxJXSk3ywXgrj9u5tn3fHluVSVSefRsj1DXFP3adt13Vchtfz2XXYdPBe7L8rhY9b1LGDuoV6uf78jovn/+e3APvyOleROx36nNVjAmNbQ1yyWlAnpHKi5GEylQXXr/n6k4WhsyBfLmyZGDZmV1naNB1nhKZNBNxvUYY9ovbRYWxfPNp77J22rji3GD833PE3RftE0ykjG6n8hFSzZbwZjUl1I59Dfv+RJzfvVX9h6t7dTjuAWuHj84ZEBy9MI1NERZWTSreHDEwcuuHt0vzM/GI0Jdo5fMBARdm61gTGpzFNBFZCbwEL5Non+tqveFHT8H+A1wIbBQVf8j3g0F39TDeKRcmpVWgfAr4wfz7Ht7ccvpFaP+Co3RgmYyRvc37joCwN+dU0ifvKy4Bl2brWBMaosZ0EXEDTwK/B1QAWwUkRdV9aOg044AdwKzE9LKFuvuns6kJWs79Rj9emSiqlQc9Q1+huelgzvpV4wbGPeg2VHh7Vz94QEAxxt+GGPSn5NoMAnYrqo7VLUBWAnMCj5BVStVdSPQmIA2Bkz5xWudfozDJxs4cqqRj/afoPJEHevuns6Ifrkh5/TMdnPV+YNoUmXx7PMczUSJtetRZ3dFsqJfxphYnAT0ImBP0O2KlvvaTURuFZEyESmrqqpq98+vu3t6mxUV26PyRD2T7l3LpCVrQ6YqApyoaybb4woE8o/2Hef8f3mZj/Yfj/p4sarNdbYaXUcGLRO9tV5XSIdrMKarOMmhR9rwrUPTTVT1ceBx8E1bbO/PxyuH7sSzm/by7Ka9ZLgEt9s3EPmDZzbzyo++CJxe3LO5/GjIYGr4kv54Lvlv76BlOiwSSodrMKarxJyHLiKfB36qqpe33P4JgKr+PMK5PwVqnAyKdmQeemV1Hdc8+hZV1XUhue7ggcyucvPkYfx+QzlfnVBEo1cD87dF4LKxA/jZ7PMo7Jnd7vnd8VgF6nS+erTn6g7L/22hkzGRdXYe+kZglIiMFJFM4AbgxXg20KnC/GxmnFOIl9DBQH8wd7B3dNysWF+OKjz73l5e3LyPukavb1aMwqcHTnDH05uoPFEXMVXiFgkcDxePjQKc5tujPVd32KzAxgy6B0t5pZaYKRdVbRKRO4CX8U1bfFJVt4rIvJbjy0RkIFAG5ANeEfkhMFZVq0T3pSoAABD1SURBVOPd4EM19agSsffW0U56bqabUw6KfEXiD9JwuvrjzsOn2Hn4FBffu5aSkX3JzXCFpEpe/6SSimO13F/6MXuO1vLI3AlR67WLwPoFMwI9ZSe951j59mhpoHDJrAhpC526B0t5pZaUWvrvN2rBGhrbUQ89UYJL+A7tk8OeKAue3AKf/fwqRi8spaG59RuRW+DtCPXaB+Zns+vwKQp7ZvHSnZdG3HEpmtt+V0aPLA+fHjjBmIH51NQ3BgZ5w9NA/pTVlecPpNmrvPLRQbxK0pf/x6v2jWk/S3l1X2mz9N/vhTu+kOwmAKH12KMFc/AFyxHzV9MYIZj7j09aspaX3t8X+Ceqa/QGZt/4Z+SMmL86kOqJVpIATu+bKsDW/dXkZLhCAqG/91vXUtXMn7Jas+UAL2/1BXO3kJBecXs+wj92SwmLZ58XKFH8s2602Cn8OtItNWEpr8RI9OskJQP61Q+/mewmdEi0zxT+f5apowu4afJwVnx7UszHyvK46Ncjk1Xfu6TVsc//fC0bdh7h2ff2Rg3+h2rq+eqFRUwbXYDb1Xr0oVl94wFPb2idiumMjuTnu0NOP1x4m7pjGzvDUl6JkejXSUqlXOJZbbG78JcXCE+h+LfC83O7hKF9cth1+FRI3v6rE4qoOBY5Dx/MX48m/B/Sv6F1httFQ9Pp9IsA+dkehvbL5clvTnT8j/zRvuP8/WPv8Id5F4eUHe7IR/hEfOzv7Awep6/BdEhNJCrl1R1mUXV1O+L5Wk6blEuy3nzaq0emm8KeWbiDOr7hneDcDDfXXjiEl74/hZsmD6eqpj7k+MmGJkYV5iG0TMv0amDrveAXxnOb9rJh5xEm3buWVbdfEvIx2f+8IpHr0VRW17Fmy36+emERz9/+BUYV5tGsp99kjtc18eHe6nb1Jn6wcjMn6pv4wTObQ+7vyEf48FW8bf2M04+yne0hhV9HlsdFUe8csjwSs42pxp/yGjs43/GKaSeS9Wkm/DXSle3oqhRWylVb/Npjb7M7bGVnd3OyobnV1njhY7j98jL55fXjgciFsB67pYTbflfG5LP6MXPcAH70x/c5uzCPZ//xEhav+Rsvbt7X6meuXPpmYKDWX9O9Z5aHS0f1Z/WW/dz55VEhQX3p2m0cq20kx+Nm7OB8PquqidhW/2yXDBdsW+LbGCS8dzNi/uqQn9lWWRO4b9d9V7X7I3ykHk1do5f/eX8fD93Qeru/WLMx4rXAK/w6Gpq95Ga6aWhWS01EEViEt+cYDUncV9f/Gvn8krUh61a6oh1dlcJKqR56YX42zS3RpivnnCfCnqO1jFqwOtBreHNbVavyAv4e0v9+eIDKE/V8tK8aWnrb0QRm3fTNAXy97E8P1nCstjHQExmzqDTiAKvHJVw+bkCrTxN+Q/r2CHwf3rtZc+elFPXOCT2/dw5rfnBp4Br3HqvlpsnDWXX7FyJ+Kgnm79H425LpEUb0y2Xq6IKQ86JdS/hgcTx7SP4Vu/7rOF7b6Pi60lm0T0n+18rV5w9KykBr+GskfBFiV7Uj/HWTiNdJSuXQ4XReb+a4gXznqY2BmRqpKMMFuVkejtc20SPTxckGL6MK83johmL+/rF3qG1oItIuei6BuZOH8+ePD7LvWPtHyzPcgksk0GMNXt360Kvb+H2EOemxZLqF4f16sK2yJnDfyP49KOiZxdA+OTy3aW+7t8zz5/cz3b6ecPjPV1bXcevv3mVAfhavf1oVcyVurMcLftzukONNNeFTatsab/B/uonnNorRRFqtPTA/m91HToW8Fu6cMSol/u5pk0MHX6/1zi99jh+s3JTSwRyg0QvHa5sAONngu5ZtlTVcufRNTtQ3RS1n4FVY8c7uDgVzgMZmDflHU4U/bT1IYc9sDtXUM7hXdtReOsCgXtkheXrw1ZM/XtvI6AF5PHLjBEYPyGPnoZMxZ9v4RerdRerRBJ+3dO023q84xo6qk1E/ygaf77SHlIozVpI5bTLapyRVbdUjH9Qrm2svKupUL7W9lU0jpTuavdrqtdDRv3vw8yV7+mrK9dDTcaZLew3und3hYJ5o/pIM0f5Gl487XecGTs+KmTK6P6UfHojYC7/jmU389Ctj+en/fMTQ3jk8GzT7J1iGW7hm/GD+/HElL//T1KgLsaL1wLtiMU2iev/x2Di8o21rq17RQ69uc/SpqD1iXWuk4+EzdiqOnOJUY3Obs8Oc/t39m9Zfe2ERORnuuG/gHs42iTaOzCoezG1Tz2LZGzsiDroGi1QQbdqYAn5x3QWghPyDBxtVmBeoWAkwcv7qiPPzMz0uPl18ReCfs62XaXaGiy+OLqDyRD1n9e/Bc5v2IrQe3AVfaqh3biZVNfWtgnxb6RuUkGAXHvycBsN4BN5gTt6EuqJt0dJZ8Zz6GO1aM90uiof1bjXoevq48Om9V4bc5w/CV54/kP3H6ijIz+INB2k7J+0Jf+7iYX3i+gaeNgG9srqORS98yJ+2HkxQq85cIqfnwofPgXdqZP8e9M7JQIGzCnqwatPeNgNxPGR6XL4VuNp2LZ/sDN88+0hBPsvj4msXDeH3G8r5XEEe26tqWgWm8GAXfju4l3bPzHNaBfvJS9ZGbF9ne/8f7TvOzU9s4GR9E/VNkYNRrEAdj08mXVGmIdonAY8Iz23ey5ziIpr0dOVT/4yvGecUcqK+KeZaDRHa9Uli1ILVRMv6Btrmkg6NH7UlbQI6+HoCHRm0M9FluoWe2RmMHphHsxcqjpxi3/HOpXSG9slh4si+vLfrCLuP1EYMZjkZLmo7MA7in5KZ6YbG5uiB3F9fJ3ghllMet3DDxGE8vX53xDcBJ26eHBr8h/TOYe+x2nbVyYnVu160akugsFrwQOOdM0Y5fhNpb4nnWO3qSOomOLW2cNWHKPD41y9qc2C7vX9Tt/gW6EXbDB58abu/nzjM0RuS/008XKY7+nP4P3l2RtoMio5ZVGrBPAEampXDJxt4+7MjbNh5pNPBHHzTMp97by+7ogRzoEPBHE4H8IaWYD6iX27IIi6/wyd95Q1GFeYxrG9O6xOA3jkZ9M7JCOT+/WUQZl0wmMWzz+Ol718aUqo5yyMtC4li/+v4Bwf9g4V7jtYG3hzqGr288UkV0//jL7y5vYo5j77F7EffChlY+2jfca5++M2IA3XBA5F+9U1eBAIDfAjkZbkDx32BegBjB+eHPI9/Kmx75kgvXbuNDTuP8IX7/sxH+4+3GqyONbgYbZHPD1ZuZtOeY2zecyzizwcPbBf19rXP/6fP8ggZkV4ILZrV91qPdEaWxzd18a35X2q1iCq8rf7ffaRgnpvp5vnvXcpXLywKeU36X1dfOX9Q1PbFQ0r10P09iVj5XWO6gscFbXUSszyCx+VqtcgsXI9Md+CcmycPA4hYzhh8Pbw3757eKt+f5XGRl+XheG1Dm23yu+r8gWzcdTQwllBVUx9ImTzx5o6QgWW/yuq6qJu0B1ceDRcpdeP/1BJtrCP859fdPT3Q67/0vtciVi31u/bCIsp2H221AFGAy8YN4IOK4+wP67SIQN/cTIb1y+WxWy4KGRcJnnZ754xRrX73wdqquhrrd+KUpVyMSRO9czJwu4Qjpxp8+f7KmpBPQG6gY5X9CdTeX/rqtogzg65++E0qT7RvmuHQPjk8e/slFPbM5qN9x7lyafsK65UM601NQzPbD56gWeGmycOivtk5MbJfD3YfOdnmm8jNk32BO9qbl+9xctnZwRXr0eoqOZU2Ad1muRiTuvy9V//OXsrp8ZBkSNTWlZ8/qy9v7zgS87xd913VocdPmxx6itTmMsZE4E9FeINmJCXzXzpR+xA7CeZASJmPeEmpgP7mPdMZ0ify4JYxxqSSKx96s1VRu85KqYA+5RevUeFgwMEYY1JFPIN6SgX0dXenfo1pY4wJtuYHl8btsRwFdBGZKSKfiMh2EZkf4biIyNKW4x+IyIVxa2GQwvxsvjqhKBEPbYwxSXHlQ/HbUjNmQBcRN/AocAUwFrhRRMaGnXYFMKrl61bgV3FrYZjnN7d/SboxxpwJnPTQJwHbVXWHqjYAK4FZYefMAp5Sn3eA3iKSkCVR7/xkRiIe1hhjkiIjjolvJw9VBOwJul3Rcl97z0FEbhWRMhEpq6qqam9bAUu7GGPSy1tx7KQ6CeiRSh+Ez+B0cg6q+riqlqhqSUFBQYQfcca/gbIT/o0a+uZmRKz3YYwxyZKT4YprXXwnm0RXAEODbg8BwoupODknbvwbKNc1NnPBkN4A/PnjgyjQKzuD7Aw3dU3NZLpdjB2cHyi04y/x+VnVCQ7XNFB1op7q2kYyPS5Khvflr58dQoErzvNli0o/3E9OhhuXS6ipa8LjFjwuobalNGeGW2hqVjxuF4U9sxg7OB+A1z+tAoU+uRkcPFGPAJPO6svm8mO4RDi7sAe7D5/ieG1Ty9Ztuew8dJImLwzIz+JgdfTl1bkZbnIzXRw62Qj46oU0NmvIxtDBbQtePJHh9p0LvnPdLnDJ6cpwLiB4Ha4L33LwXjkZ1Dd5OdnQjMcFPbI8nKxvwuXy9Qe8Xq+j+iHROFkt2JGKiZ15PtN5iVqJmS5cQqs9cjvLSUDfCIwSkZHAXuAGYG7YOS8Cd4jISmAycFxV98e1pWE6Ums53vWZjTGmO4kZ0FW1SUTuAF7GV/vnSVXdKiLzWo4vA9YAVwLbgVPAPySuycYYYyJx0kNHVdfgC9rB9y0L+l6B78W3acYYY9ojpVaKGmOMic4CujHGpAkL6MYYkyYsoBtjTJpI2o5FIlIF7E7Kkydff+BQshuRZGf678Cu366/o9c/XFUjTmBPWkA/k4lIWbQtpM4UZ/rvwK7frj8R128pF2OMSRMW0I0xJk1YQE+Ox5PdgG7gTP8d2PWf2RJy/ZZDN8aYNGE9dGOMSRMW0I0xJk1YQE8gB5trnyMib4tIvYj8OBltTCQH139Ty6biH4jIX0VkfDLamSgOrn9Wy7VvbtnJK37bv3cDsa4/6LyJItIsItd1ZfsSzcHff5qIHG/5+28WkX/u9JOqqn0l4AtfqeHPgLOATOB9YGzYOYXAROBe4MfJbnMSrv8SoE/L91cA65Pd7i6+/jxOj2NdAHyc7HZ35fUHnfdnfNVcr0t2u7v47z8NeCmez2s99MSJubm2qlaq6kagMRkNTDAn1/9XVT3acvMdfDtdpQsn11+jLf/ZQA/SayMlJ5vLA3wfeBao7MrGdQGn1x9XFtATx9HG2Wmsvdf/baA0oS3qWk43Tp8jIh8Dq4FvdVHbukLM6xeRImAOsIz04/T1/3kReV9ESkVkXGef1AJ64jjaODuNOb5+EZmOL6Dfk9AWdS2nG6evUtVzgNnAzxLeqq7j5PofBO5R1eYuaE9Xc3L97+GryzIeeBh4vrNPagE9cbp04+xuyNH1i8gFwK+BWap6uIva1hXa9fdX1TeAs0Wkf6Ib1kWcXH8JsFJEdgHXAf8pIrO7pnkJF/P6VbVaVWtavl8DZHT2728BPXECm2uLSCa+zbVfTHKbulLM6xeRYcBzwC2q+mkS2phITq7/cyIiLd9fiG/wLF3e1GJev6qOVNURqjoC+H/A7ara6V5qN+Hk7z8w6O8/CV887tTf39Geoqb91MHm2iIyECgD8gGviPwQ30h4ddIaHidOrh/4Z6Afvp4ZQJOmSQU+h9d/LfB1EWkEaoG/DxokTWkOrz9tObz+64B/FJEmfH//Gzr797el/8YYkyYs5WKMMWnCAroxxqQJC+jGGJMmLKAbY0yasIBujDFpwgK6McakCQvoxhiTJv4/SVhOS/oolxgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(r'$\\xi$', fontsize = 20)\n",
    "plt.text(0.06,0.4, 'Average='+str(np.round(Av_RE[0].item()*100,2))+'%', fontsize = 12)\n",
    "plt.plot(XI,RE_xi.detach().numpy(), '*')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.title(r'$\\nu$', fontsize = 20)\n",
    "plt.text(1.5,0.1, 'Average='+str(np.round(Av_RE[1].item()*100,2))+'%', fontsize = 12)\n",
    "plt.plot(NU,RE_nu.detach().numpy(), '*')\n",
    "plt.show()\n",
    "\n",
    "plt.title(r'$\\rho$', fontsize = 20)\n",
    "plt.text(-0.7,0.2, 'Average='+str(np.round(Av_RE[2].item()*100,2))+'%', fontsize = 12)\n",
    "plt.plot(RHO,RE_rho.detach().numpy(), '*')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.title('H', fontsize = 20)\n",
    "plt.text(0.2,0.5, 'Average='+str(np.round(Av_RE[3].item()*100,2))+'%', fontsize = 12)\n",
    "plt.plot(H,RE_H.detach().numpy(), '*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative error distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantile 90% =  0.02476830966770649\n",
      "quantile 95% =  0.03967531770467758\n",
      "quantile 99% =  0.11209042370319366\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "RE_ = torch.reshape(RE_, (6000*4,1))\n",
    "sorted_re_ = sorted(RE_)\n",
    "\n",
    "q_90 = sorted_re_[int(0.90*len(RE_.detach().numpy()))]\n",
    "q_95 = sorted_re_[int(0.95*len(RE_.detach().numpy()))]\n",
    "q_99 = sorted_re_[int(0.99*len(RE_.detach().numpy()))]\n",
    "\n",
    "print('quantile 90% = ', q_90.item())\n",
    "print('quantile 95% = ', q_95.item())\n",
    "print('quantile 99% = ', q_99.item())\n",
    "\n",
    "RE_xi = torch.reshape(RE_xi, (6000,1))\n",
    "sorted_re_xi = sorted(RE_xi)\n",
    "\n",
    "RE_nu = torch.reshape(RE_nu, (6000,1))\n",
    "sorted_re_nu = sorted(RE_nu)\n",
    "\n",
    "RE_rho = torch.reshape(RE_rho, (6000,1))\n",
    "sorted_re_rho = sorted(RE_rho)\n",
    "\n",
    "RE_H = torch.reshape(RE_H, (6000,1))\n",
    "sorted_re_h = sorted(RE_H)\n",
    "\n",
    "plt.hist(RE_nu.detach().numpy(), bins=np.linspace(0, 0.15, 200),histtype='step', density=True,label =r'$\\nu$', rwidth=0.5, color='orange')\n",
    "plt.hist(RE_rho.detach().numpy(), bins=np.linspace(0, 0.15, 200),histtype='step', density=True,label =r'$\\rho$', rwidth=0.5, color='green')\n",
    "plt.hist(RE_xi.detach().numpy(), bins=np.linspace(0, 0.15, 200),histtype='step', density=True,label =r'$\\xi$', rwidth=0.5, color= 'blue')\n",
    "plt.hist(RE_H.detach().numpy(), bins=np.linspace(0, 0.15, 200),histtype='step', density=True,label ='H', rwidth=0.5, color='red')\n",
    "plt.xlim((0, 0.15))\n",
    "\n",
    "plt.axvline(x=q_95, ls='-.', color='purple')\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.xlabel(\"relative error\")\n",
    "plt.ylabel(\"frequency density\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
